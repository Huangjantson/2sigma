{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "from mochi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../../kaggleData/2sigma/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#basic feature engineering\n",
    "#basic feature generation\n",
    "#some transfromed features\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#time-related\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "train_df[\"created_weekday\"] = train_df[\"created\"].dt.dayofweek\n",
    "\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "\n",
    "train_df[\"sine_hour\"] = np.sin(2*np.pi*train_df[\"created_hour\"]/24)\n",
    "train_df[\"cos_hour\"] = np.cos(2*np.pi*train_df[\"created_hour\"]/24)\n",
    "\n",
    "train_df[\"sine_weekday\"] = np.sin(2*np.pi*train_df[\"created_weekday\"]/7)\n",
    "train_df[\"cos_weekday\"] = np.cos(2*np.pi*train_df[\"created_weekday\"]/7)\n",
    "\n",
    "train_df[\"sine_day\"] = np.sin(2*np.pi*train_df[\"created_day\"]/7)\n",
    "train_df[\"cos_day\"] = np.cos(2*np.pi*train_df[\"created_day\"]/7)\n",
    "\n",
    "#some new numerical features related to the price\n",
    "train_df[\"price_per_bath\"] =  (train_df[\"price\"]*1.0/(train_df[\"bathrooms\"]+0.01))\n",
    "train_df[\"price_per_bed\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"])+0.01)\n",
    "train_df[\"bath_per_bed\"] = (train_df[\"bathrooms\"]*1.0/(train_df[\"bedrooms\"]+1))\n",
    "train_df[\"price_per_room\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"]+train_df[\"bathrooms\"]+0.01))\n",
    "\n",
    "train_df['house_type']=map(lambda x,y:(x,y),train_df['bedrooms'],train_df['bathrooms'])\n",
    "train_df['house_type'] = train_df['house_type'].apply(str)\n",
    "\n",
    "ny_lat = 40.785091\n",
    "ny_lon = -73.968285\n",
    "train_df['central_distance']= np.sqrt((train_df['latitude']-ny_lat)**2 + (train_df['longitude']-ny_lon)**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from non-structured features\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "#time-related\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "test_df[\"created_weekday\"] = test_df[\"created\"].dt.dayofweek\n",
    "\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "\n",
    "test_df[\"sine_hour\"] = np.sin(2*np.pi*test_df[\"created_hour\"]/24)\n",
    "test_df[\"cos_hour\"] = np.cos(2*np.pi*test_df[\"created_hour\"]/24)\n",
    "\n",
    "test_df[\"sine_weekday\"] = np.sin(2*np.pi*test_df[\"created_weekday\"]/7)\n",
    "test_df[\"cos_weekday\"] = np.cos(2*np.pi*test_df[\"created_weekday\"]/7)\n",
    "\n",
    "test_df[\"sine_day\"] = np.sin(2*np.pi*test_df[\"created_day\"]/7)\n",
    "test_df[\"cos_day\"] = np.cos(2*np.pi*test_df[\"created_day\"]/7)\n",
    "\n",
    "#some new numerical features related to the price\n",
    "\n",
    "test_df[\"price_per_bath\"] =  (test_df[\"price\"]*1.0/(test_df[\"bathrooms\"]+0.01))\n",
    "test_df[\"price_per_bed\"] = (test_df[\"price\"]*1.0/(test_df[\"bedrooms\"]+0.01))\n",
    "test_df[\"bath_per_bed\"] = (test_df[\"bathrooms\"]*1.0/(test_df[\"bedrooms\"]+1))\n",
    "test_df[\"price_per_room\"] = (test_df[\"price\"]*1.0/(test_df[\"bedrooms\"]+test_df[\"bathrooms\"]+0.01))\n",
    "\n",
    "test_df['house_type']=map(lambda x,y:(x,y),test_df['bedrooms'],test_df['bathrooms'])\n",
    "test_df['house_type'] = test_df['house_type'].apply(str)\n",
    "\n",
    "test_df['central_distance']= np.sqrt((test_df['latitude']-ny_lat)**2 + (test_df['longitude']-ny_lon)**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "new categorical data generated from the old ones\n",
    "\"\"\"\n",
    "#new feature for the street_address, use them instead of the original one\n",
    "train_df[\"street_name\"] = train_df[\"street_address\"].apply(proecessStreet)\n",
    "test_df[\"street_name\"] = test_df[\"street_address\"].apply(proecessStreet)\n",
    "\n",
    "train_df['building0']=map(lambda x:1 if x== '0' else 0,train_df['building_id'])\n",
    "test_df['building0']=map(lambda x:1 if x== '0' else 0,test_df['building_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dealing with features\n",
    "#preprocessing for features\n",
    "train_df[\"features\"] = train_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_') \\\n",
    "                                                            for i in x])\n",
    "test_df[\"features\"] = test_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_')\\\n",
    "                                                          for i in x])\n",
    "#create the accept list\n",
    "accept_list = list(featureList(train_df,test_df,limit = 0.001))\n",
    "\n",
    "#map the feature to dummy slots\n",
    "featureMapping(train_df,test_df,accept_list)\n",
    "#features_to_use.extend(map(lambda x : 'with_'+x,accept_list))\n",
    "#map(lambda x : 'with_'+x,accept_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processMap(train_df)\n",
    "processMap(test_df)\n",
    "train_df['latitude']=train_df['latitude'].fillna(-1)\n",
    "train_df['longitude']=train_df['longitude'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['latitude']=test_df['latitude'].fillna(-1)\n",
    "test_df['longitude']=test_df['longitude'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features from clustering\n",
    "getCluster(train_df,test_df,30)\n",
    "getCluster(train_df,test_df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store the basic transformed train and test\n",
    "train_df.to_json(data_path+'basic_train_df.json')\n",
    "test_df.to_json(data_path+'basic_test_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KF=StratifiedKFold(train_df['interest_level'],5,shuffle=True,random_state = 1983)\n",
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:]\n",
    "    dev_set.to_json(data_path+'b_dev_set_'+str(i)+'.json')\n",
    "    val_set.to_json(data_path+'b_val_set_'+str(i)+'.json')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processing_features = ['bathrooms',\n",
    " 'bedrooms',\n",
    " 'price',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'num_photos',\n",
    " 'num_features',\n",
    " 'num_description_words',\n",
    " 'dayofyear',\n",
    " 'listing_id',\n",
    " 'price_per_bed',\n",
    " 'bath_per_bed',\n",
    " 'price_per_room',\n",
    " 'price_per_bath',\n",
    " 'central_distance',\n",
    "'cluster_id_30_d','cluster_id_10_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skF=StratifiedKFold(train_df['interest_level'],5,shuffle=True,random_state = 1983)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf_l = []\n",
    "skf_l=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dev,val in KF:\n",
    "    kf_l.append(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dev,val in skF:\n",
    "    skf_l.append(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print sum(skf_l[i]-kf_l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10        7211212\n",
       "10000     7150865\n",
       "100004    6887163\n",
       "100007    6888711\n",
       "100013    6934781\n",
       "100014    6894514\n",
       "100016    6930771\n",
       "100020    6867392\n",
       "100026    6898799\n",
       "100027    6814332\n",
       "100030    6869199\n",
       "10004     7102986\n",
       "100044    6895442\n",
       "100048    6846213\n",
       "10005     7089402\n",
       "100051    6889043\n",
       "100052    6913348\n",
       "100053    6894111\n",
       "100055    6900220\n",
       "100058    6848536\n",
       "100062    6858062\n",
       "100063    6836760\n",
       "100065    6866830\n",
       "100066    6885927\n",
       "10007     7120132\n",
       "100071    6933499\n",
       "100075    6921632\n",
       "100076    6913084\n",
       "100079    6907079\n",
       "100081    6925264\n",
       "           ...   \n",
       "99915     6921019\n",
       "99917     6926146\n",
       "99919     6844805\n",
       "99921     6943991\n",
       "99923     6822618\n",
       "99924     6918969\n",
       "99931     6921162\n",
       "99933     6819357\n",
       "99935     6893263\n",
       "99937     6873182\n",
       "9994      7114685\n",
       "99953     6924210\n",
       "99956     6884807\n",
       "99960     6825168\n",
       "99961     6911061\n",
       "99964     6942494\n",
       "99965     6819478\n",
       "99966     6878391\n",
       "99979     6871559\n",
       "99980     6933865\n",
       "99982     6837242\n",
       "99984     6815109\n",
       "99986     6871681\n",
       "99987     6856001\n",
       "99988     6913833\n",
       "9999      7098690\n",
       "99991     6822449\n",
       "99992     6881461\n",
       "99993     6841891\n",
       "99994     6858245\n",
       "Name: listing_id, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(train_df.drop('interest_level',axis=1))\n",
    "train_test = pd.concat([train_df.drop('interest_level',axis=1),test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalized features\n",
    "normalized_train = train_df.copy()\n",
    "normalized_test = test_df.copy()\n",
    "\n",
    "for f in processing_features:\n",
    "    normalized_train[f]=normalized_train[f].fillna(train_test[f].median())\n",
    "    normalized_test[f]=normalized_test[f].fillna(train_test[f].median())\n",
    "\n",
    "for f in processing_features:\n",
    "    normalized_train[f]=(normalized_train[f]-train_test[f].mean())/train_test[f].std()\n",
    "    normalized_test[f]=(normalized_test[f]-train_test[f].mean())/train_test[f].std()\n",
    "\n",
    "#store the basic transformed train and test\n",
    "normalized_train.to_json(data_path+'normal_train_df.json')\n",
    "normalized_test.to_json(data_path+'normal_test_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    dev_set, val_set = normalized_train.iloc[dev_index,:] , normalized_train.iloc[val_index,:]\n",
    "    dev_set.to_json(data_path+'n_dev_set_'+str(i)+'.json')\n",
    "    val_set.to_json(data_path+'n_val_set_'+str(i)+'.json')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: invalid value encountered in log\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "#log transformed features\n",
    "#normalized features\n",
    "log_train = train_df.copy()\n",
    "log_test = test_df.copy()\n",
    "\n",
    "for f in processing_features:\n",
    "    log_train[f]=np.log(log_train[f]+1)\n",
    "    log_test[f]=np.log(log_test[f]+1)\n",
    "\n",
    "\n",
    "#store the basic transformed train and test\n",
    "log_train.to_json(data_path+'log_train_df.json')\n",
    "log_test.to_json(data_path+'log_test_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    dev_set, val_set = log_train.iloc[dev_index,:] , log_train.iloc[val_index,:]\n",
    "    dev_set.to_json(data_path+'l_dev_set_'+str(i)+'.json')\n",
    "    val_set.to_json(data_path+'l_val_set_'+str(i)+'.json')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_train_test = pd.concat([log_train.drop('interest_level',axis=1),log_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log transformed and normalized features\n",
    "#normalized features\n",
    "normalized_log_train = log_train.copy()\n",
    "normalized_log_test = log_test.copy()\n",
    "\n",
    "for f in processing_features:\n",
    "    normalized_log_train[f]=normalized_log_train[f].fillna(log_train_test[f].median())\n",
    "    normalized_log_test[f]=normalized_log_test[f].fillna(log_train_test[f].median())\n",
    "\n",
    "for f in processing_features:\n",
    "    normalized_log_train[f]=(normalized_log_train[f]-log_train_test[f].mean())/log_train_test[f].std()\n",
    "    normalized_log_test[f]=(normalized_log_test[f]-log_train_test[f].mean())/log_train_test[f].std()\n",
    "\n",
    "#store the basic transformed train and test\n",
    "normalized_log_train.to_json(data_path+'lognor_train_df.json')\n",
    "normalized_log_test.to_json(data_path+'lognor_test_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    dev_set, val_set = normalized_log_train.iloc[dev_index,:] , normalized_log_train.iloc[val_index,:]\n",
    "    dev_set.to_json(data_path+'ln_dev_set_'+str(i)+'.json')\n",
    "    val_set.to_json(data_path+'ln_val_set_'+str(i)+'.json')\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
