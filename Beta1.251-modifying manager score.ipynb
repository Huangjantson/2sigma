{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import  preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try xgboost\n",
    "#fucntion from SRK\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, \\\n",
    "     seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1, max_depth = 6):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = eta\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,\\\n",
    "        early_stopping_rounds=early_stop)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature processing functions\n",
    "#define punctutaion filter\n",
    "def removePunctuation(x):\n",
    "    #filter the head or tail blanks\n",
    "    x = re.sub(r'^\\s+',r' ',x)\n",
    "    x = re.sub(r'\\s+$',r' ',x)\n",
    "    \n",
    "    # Lowercasing all words\n",
    "    x = x.lower()\n",
    "    # Removing non ASCII chars, warning if you are dealing with other languages!!!!!!!!!!!!!!!\n",
    "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n",
    "    #change all the blank to space\n",
    "    x = re.sub(r'\\s',r' ',x)\n",
    "    # Removing (replacing with empty spaces actually) all the punctuations\n",
    "    removing = string.punctuation#.replace('-','')# except '-'\n",
    "    removed = re.sub(\"[\"+removing+\"]\", \"\", x)\n",
    "    #removing the line-changing\n",
    "    #removed = re.sub('\\\\n',\" \",removed)    \n",
    "    return removed\n",
    "\n",
    "#feature processing functions\n",
    "def proecessStreet(address):\n",
    "    #remove the building number\n",
    "    pattern = re.compile('^[\\d-]*[\\s]+')\n",
    "    street = removePunctuation(pattern.sub('',address))\n",
    "    \n",
    "    #sub the st to street\n",
    "    pattern = re.compile('( st)$')\n",
    "    street = pattern.sub(' street',street)\n",
    "    \n",
    "    #sub the ave to avenue\n",
    "    pattern = re.compile('( ave)$')\n",
    "    street = pattern.sub(' avenue',street)\n",
    "    \n",
    "    pattern = re.compile('(\\d+)((th)|(st)|(rd)|(nd))')\n",
    "    street = pattern.sub('\\g<1>',street)\n",
    "    \n",
    "    #deal with the w 14 street => west 14 street\n",
    "    pattern = re.compile('(w)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('west \\g<3>',street)\n",
    "    \n",
    "    #deal with the e....\n",
    "    pattern = re.compile('(e)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('east \\g<3>',street)\n",
    "    \n",
    "    return street\n",
    "    \n",
    "#from \"this is a lit\"s python version by rakhlin\n",
    "def singleValueConvert(df1,df2,column,minimum_size=5):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= minimum_size, column] = -1\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= minimum_size, column] = -1\n",
    "    return df1, df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"the old manager skil \"\"\"\n",
    "#def manager_skill_eval(train_df,test_df,unrank_threshold = 10):\n",
    "def manager_skill_eval(train_df,test_df,update_df =None):\n",
    "    \n",
    "    target_num_map = {'High':2, 'Medium':1, 'Low':0}\n",
    "    temp=pd.concat([train_df.manager_id,pd.get_dummies(train_df.interest_level)], axis = 1).groupby('manager_id').mean()\n",
    "     \n",
    "    temp.columns = ['ManHigh','ManLow', 'ManMedium']\n",
    "    \n",
    "    #temp['count'] = train_df.groupby('manager_id').count().iloc[:,1]\n",
    "    \n",
    "    #this may be modified - the rate of high and medium\n",
    "    temp['manager_skill'] = temp['ManHigh']*2 + temp['ManMedium']\n",
    "    \n",
    "    \"\"\"unrank part is done by the singleValueConvert\"\"\"\n",
    "    #ixes of the managers with to few sample\n",
    "    #unranked_managers_ixes = temp['count']<unrank_threshold\n",
    "    #ranked_managers_ixes = ~unranked_managers_ixes\n",
    "    \n",
    "    #test for using rank or unrank part for the filling values\n",
    "    #mean_values = temp.loc[unranked_managers_ixes, ['ManHigh','ManLow', 'ManMedium','manager_skill']].mean()\n",
    "    #mean_values_total = temp.loc[:, ['ManHigh','ManLow', 'ManMedium','manager_skill']].mean()\n",
    "    mean_values = temp.loc[:, ['manager_skill']].mean()\n",
    "    \n",
    "    #reset their values to their average\n",
    "    #temp.loc[unranked_managers_ixes,['ManHigh','ManLow', 'ManMedium','manager_skill']] = mean_values.values\n",
    "    \n",
    "    #assign the features for the train set\n",
    "    #new_train_df = train_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n",
    "    \n",
    "    #assign the features for the test/val set\n",
    "    #new_test_df = test_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n",
    "    #new_manager_ixes = new_test_df['ManHigh'].isnull()\n",
    "    #new_test_df.loc[new_manager_ixes,['ManHigh','ManLow', 'ManMedium','manager_skill']] = mean_values_total.values           \n",
    "    \n",
    "    value = test_df[['manager_id']].join(temp, on='manager_id', how=\"left\")['manager_id'].fillna(mean_values)\n",
    "    \n",
    "    if update_df is None: update_df = test_df\n",
    "    if 'manager_skill' not in update_df.columns: update_df['manager_skill'] = np.nan\n",
    "    update_df.update(value)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manager_skill_eval_old(train_df,test_df,unrank_threshold = 10):\n",
    "\n",
    "    target_num_map = {'High':2, 'Medium':1, 'Low':0}\n",
    "    temp=pd.concat([train_df.manager_id,pd.get_dummies(train_df.interest_level)], axis = 1).groupby('manager_id').mean()\n",
    "     \n",
    "    temp.columns = ['ManHigh','ManLow', 'ManMedium']\n",
    "    \n",
    "    print temp.columns\n",
    "    temp['count'] = train_df.groupby('manager_id').count().iloc[:,1]\n",
    "    \n",
    "    temp['manager_skill'] = temp['ManHigh']*2 + temp['ManMedium']\n",
    "    \n",
    "    #ixes of the managers with to few sample\n",
    "    unranked_managers_ixes = temp['count']<unrank_threshold\n",
    "    ranked_managers_ixes = ~unranked_managers_ixes\n",
    "    \n",
    "    #test for using rank or unrank part for the filling values\n",
    "    mean_values = temp.loc[unranked_managers_ixes, ['ManHigh','ManLow', 'ManMedium','manager_skill']].mean()\n",
    "    mean_values_total = temp.loc[:, ['ManHigh','ManLow', 'ManMedium','manager_skill']].mean()\n",
    "    \n",
    "    #reset their values to their average\n",
    "    temp.loc[unranked_managers_ixes,['ManHigh','ManLow', 'ManMedium','manager_skill']] = mean_values.values\n",
    "    \n",
    "    #assign the features for the train set\n",
    "    new_train_df = train_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n",
    "    \n",
    "    #assign the features for the test/val set\n",
    "    new_test_df = test_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n",
    "    new_manager_ixes = new_test_df['ManHigh'].isnull()\n",
    "    new_test_df.loc[new_manager_ixes,['ManHigh','ManLow', 'ManMedium','manager_skill']] = mean_values_total.values           \n",
    "    \n",
    "    return new_train_df,new_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions for features\n",
    "def featureList(train_df,test_df,limit = 0.001):\n",
    "    #acquiring the feature lists\n",
    "    features_in_train = train_df[\"features\"].apply(pd.Series).unstack().reset_index(drop = True).dropna().value_counts()\n",
    "    features_in_test = test_df[\"features\"].apply(pd.Series).unstack().reset_index(drop = True).dropna().value_counts()\n",
    "    \n",
    "    filtered_features_in_train = features_in_train[features_in_train > limit*len(train_df)]\n",
    "    filtered_features_in_test = features_in_test[features_in_test > limit*len(test_df)]\n",
    "    accept_list = set(filtered_features_in_train.index).union(set(filtered_features_in_test.index))\n",
    "    return accept_list\n",
    "\n",
    "def featureMapping(train_df,test_df,feature_list):\n",
    "    for feature in feature_list:\n",
    "        #add the feature column for both\n",
    "        #if feature in the row, then set the value for (row,feature) to 1\n",
    "        train_df['with_'+feature]=train_df['features'].apply(lambda x : 1 if feature in x else 0)\n",
    "        test_df['with_'+feature]=test_df['features'].apply(lambda x : 1 if feature in x else 0)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"../../kaggleData/2sigma/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic numerical features\n",
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some new numerical features related to the price\n",
    "train_df[\"price_per_bath\"] =  (train_df[\"price\"]*1.0/train_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_bed\"] = (train_df[\"price\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"bath_per_bed\"] = (train_df[\"bathrooms\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_room\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"]+train_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "test_df[\"price_per_bath\"] =  (test_df[\"price\"]*1.0/test_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_bed\"] = (test_df[\"price\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"bath_per_bed\"] = (test_df[\"bathrooms\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_room\"] = (test_df[\"price\"]*1.0/(test_df[\"bedrooms\"]+test_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "features_to_use.extend([\"price_per_bed\",\"bath_per_bed\",\"price_per_room\"])\n",
    "#features_to_use.append('price_per_bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some transfromed features\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# adding all these new features to use list # \"listing_id\",\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\",\"listing_id\", \"created_month\", \"created_day\", \"created_hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "new categorical data append and converting label dummies for future use\n",
    "\"\"\"\n",
    "#new feature for the street_address, use them instead of the original one\n",
    "train_df[\"street_name\"] = train_df[\"street_address\"].apply(proecessStreet)\n",
    "test_df[\"street_name\"] = test_df[\"street_address\"].apply(proecessStreet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dealing with features\n",
    "\n",
    "#preprocessing for features\n",
    "train_df[\"features\"] = train_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_') \\\n",
    "                                                            for i in x])\n",
    "test_df[\"features\"] = test_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_')\\\n",
    "                                                          for i in x])\n",
    "#create the accept list\n",
    "accept_list = list(featureList(train_df,test_df,limit = 0.001))\n",
    "\n",
    "#map the feature to dummy slots\n",
    "featureMapping(train_df,test_df,accept_list)\n",
    "features_to_use.extend(map(lambda x : 'with_'+x,accept_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=StratifiedKFold(train_y,5,shuffle=True,random_state = 42)\n",
    "\n",
    "train_df = train_df.fillna(-1)\n",
    "test_df = test_df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.append('manager_skill')\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\",\"street_name\"]\n",
    "features_to_use.extend(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.04194\ttest-mlogloss:1.04236\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:0.992839\ttest-mlogloss:0.99363\n",
      "[2]\ttrain-mlogloss:0.951279\ttest-mlogloss:0.95233\n",
      "[3]\ttrain-mlogloss:0.916156\ttest-mlogloss:0.917514\n",
      "[4]\ttrain-mlogloss:0.884269\ttest-mlogloss:0.885863\n",
      "[5]\ttrain-mlogloss:0.857093\ttest-mlogloss:0.85881\n",
      "[6]\ttrain-mlogloss:0.833671\ttest-mlogloss:0.835631\n",
      "[7]\ttrain-mlogloss:0.812394\ttest-mlogloss:0.814716\n",
      "[8]\ttrain-mlogloss:0.79334\ttest-mlogloss:0.79578\n",
      "[9]\ttrain-mlogloss:0.776881\ttest-mlogloss:0.779636\n",
      "[10]\ttrain-mlogloss:0.762277\ttest-mlogloss:0.765194\n",
      "[11]\ttrain-mlogloss:0.748984\ttest-mlogloss:0.75206\n",
      "[12]\ttrain-mlogloss:0.737754\ttest-mlogloss:0.741126\n",
      "[13]\ttrain-mlogloss:0.726699\ttest-mlogloss:0.730243\n",
      "[14]\ttrain-mlogloss:0.716795\ttest-mlogloss:0.720408\n",
      "[15]\ttrain-mlogloss:0.707897\ttest-mlogloss:0.71164\n",
      "[16]\ttrain-mlogloss:0.699914\ttest-mlogloss:0.703805\n",
      "[17]\ttrain-mlogloss:0.693265\ttest-mlogloss:0.697311\n",
      "[18]\ttrain-mlogloss:0.686825\ttest-mlogloss:0.691066\n",
      "[19]\ttrain-mlogloss:0.680758\ttest-mlogloss:0.68519\n",
      "[20]\ttrain-mlogloss:0.675174\ttest-mlogloss:0.679539\n",
      "[21]\ttrain-mlogloss:0.670011\ttest-mlogloss:0.674447\n",
      "[22]\ttrain-mlogloss:0.665081\ttest-mlogloss:0.669713\n",
      "[23]\ttrain-mlogloss:0.660725\ttest-mlogloss:0.665534\n",
      "[24]\ttrain-mlogloss:0.656137\ttest-mlogloss:0.661106\n",
      "[25]\ttrain-mlogloss:0.652644\ttest-mlogloss:0.657785\n",
      "[26]\ttrain-mlogloss:0.648944\ttest-mlogloss:0.654271\n",
      "[27]\ttrain-mlogloss:0.645723\ttest-mlogloss:0.651287\n",
      "[28]\ttrain-mlogloss:0.642407\ttest-mlogloss:0.648107\n",
      "[29]\ttrain-mlogloss:0.639607\ttest-mlogloss:0.645317\n",
      "[30]\ttrain-mlogloss:0.636738\ttest-mlogloss:0.642662\n",
      "[31]\ttrain-mlogloss:0.633899\ttest-mlogloss:0.640014\n",
      "[32]\ttrain-mlogloss:0.631669\ttest-mlogloss:0.637973\n",
      "[33]\ttrain-mlogloss:0.629452\ttest-mlogloss:0.635825\n",
      "[34]\ttrain-mlogloss:0.627224\ttest-mlogloss:0.633805\n",
      "[35]\ttrain-mlogloss:0.625414\ttest-mlogloss:0.632039\n",
      "[36]\ttrain-mlogloss:0.623361\ttest-mlogloss:0.630126\n",
      "[37]\ttrain-mlogloss:0.62145\ttest-mlogloss:0.628364\n",
      "[38]\ttrain-mlogloss:0.619784\ttest-mlogloss:0.626782\n",
      "[39]\ttrain-mlogloss:0.618157\ttest-mlogloss:0.625305\n",
      "[40]\ttrain-mlogloss:0.616277\ttest-mlogloss:0.623753\n",
      "[41]\ttrain-mlogloss:0.61471\ttest-mlogloss:0.622442\n",
      "[42]\ttrain-mlogloss:0.613062\ttest-mlogloss:0.620956\n",
      "[43]\ttrain-mlogloss:0.611585\ttest-mlogloss:0.619755\n",
      "[44]\ttrain-mlogloss:0.610012\ttest-mlogloss:0.61835\n",
      "[45]\ttrain-mlogloss:0.608517\ttest-mlogloss:0.617118\n",
      "[46]\ttrain-mlogloss:0.607354\ttest-mlogloss:0.616262\n",
      "[47]\ttrain-mlogloss:0.605882\ttest-mlogloss:0.615124\n",
      "[48]\ttrain-mlogloss:0.604764\ttest-mlogloss:0.61424\n",
      "[49]\ttrain-mlogloss:0.603473\ttest-mlogloss:0.613106\n",
      "[50]\ttrain-mlogloss:0.60192\ttest-mlogloss:0.611705\n",
      "[51]\ttrain-mlogloss:0.600605\ttest-mlogloss:0.610559\n",
      "[52]\ttrain-mlogloss:0.599417\ttest-mlogloss:0.609614\n",
      "[53]\ttrain-mlogloss:0.598107\ttest-mlogloss:0.608525\n",
      "[54]\ttrain-mlogloss:0.596893\ttest-mlogloss:0.60738\n",
      "[55]\ttrain-mlogloss:0.595856\ttest-mlogloss:0.606556\n",
      "[56]\ttrain-mlogloss:0.594598\ttest-mlogloss:0.60549\n",
      "[57]\ttrain-mlogloss:0.593511\ttest-mlogloss:0.6047\n",
      "[58]\ttrain-mlogloss:0.5924\ttest-mlogloss:0.603967\n",
      "[59]\ttrain-mlogloss:0.591444\ttest-mlogloss:0.603211\n",
      "[60]\ttrain-mlogloss:0.590384\ttest-mlogloss:0.602345\n",
      "[61]\ttrain-mlogloss:0.589394\ttest-mlogloss:0.601483\n",
      "[62]\ttrain-mlogloss:0.588292\ttest-mlogloss:0.600482\n",
      "[63]\ttrain-mlogloss:0.587481\ttest-mlogloss:0.599817\n",
      "[64]\ttrain-mlogloss:0.586594\ttest-mlogloss:0.599173\n",
      "[65]\ttrain-mlogloss:0.585802\ttest-mlogloss:0.598528\n",
      "[66]\ttrain-mlogloss:0.584952\ttest-mlogloss:0.597826\n",
      "[67]\ttrain-mlogloss:0.584198\ttest-mlogloss:0.597232\n",
      "[68]\ttrain-mlogloss:0.583629\ttest-mlogloss:0.596719\n",
      "[69]\ttrain-mlogloss:0.582614\ttest-mlogloss:0.595883\n",
      "[70]\ttrain-mlogloss:0.581926\ttest-mlogloss:0.595348\n",
      "[71]\ttrain-mlogloss:0.58126\ttest-mlogloss:0.594784\n",
      "[72]\ttrain-mlogloss:0.580591\ttest-mlogloss:0.594273\n",
      "[73]\ttrain-mlogloss:0.579886\ttest-mlogloss:0.593685\n",
      "[74]\ttrain-mlogloss:0.579204\ttest-mlogloss:0.593147\n",
      "[75]\ttrain-mlogloss:0.578251\ttest-mlogloss:0.592498\n",
      "[76]\ttrain-mlogloss:0.577397\ttest-mlogloss:0.591823\n",
      "[77]\ttrain-mlogloss:0.576585\ttest-mlogloss:0.591352\n",
      "[78]\ttrain-mlogloss:0.575979\ttest-mlogloss:0.59085\n",
      "[79]\ttrain-mlogloss:0.575086\ttest-mlogloss:0.590311\n",
      "[80]\ttrain-mlogloss:0.574297\ttest-mlogloss:0.589765\n",
      "[81]\ttrain-mlogloss:0.573641\ttest-mlogloss:0.589312\n",
      "[82]\ttrain-mlogloss:0.573117\ttest-mlogloss:0.588916\n",
      "[83]\ttrain-mlogloss:0.572532\ttest-mlogloss:0.588553\n",
      "[84]\ttrain-mlogloss:0.571962\ttest-mlogloss:0.588091\n",
      "[85]\ttrain-mlogloss:0.571286\ttest-mlogloss:0.587625\n",
      "[86]\ttrain-mlogloss:0.570557\ttest-mlogloss:0.586965\n",
      "[87]\ttrain-mlogloss:0.56984\ttest-mlogloss:0.586474\n",
      "[88]\ttrain-mlogloss:0.569046\ttest-mlogloss:0.585877\n",
      "[89]\ttrain-mlogloss:0.568155\ttest-mlogloss:0.585241\n",
      "[90]\ttrain-mlogloss:0.567407\ttest-mlogloss:0.584762\n",
      "[91]\ttrain-mlogloss:0.566653\ttest-mlogloss:0.584338\n",
      "[92]\ttrain-mlogloss:0.56582\ttest-mlogloss:0.583738\n",
      "[93]\ttrain-mlogloss:0.56523\ttest-mlogloss:0.583293\n",
      "[94]\ttrain-mlogloss:0.564771\ttest-mlogloss:0.58304\n",
      "[95]\ttrain-mlogloss:0.564283\ttest-mlogloss:0.582574\n",
      "[96]\ttrain-mlogloss:0.56371\ttest-mlogloss:0.582137\n",
      "[97]\ttrain-mlogloss:0.563028\ttest-mlogloss:0.581614\n",
      "[98]\ttrain-mlogloss:0.562355\ttest-mlogloss:0.581214\n",
      "[99]\ttrain-mlogloss:0.561794\ttest-mlogloss:0.580825\n",
      "[100]\ttrain-mlogloss:0.561245\ttest-mlogloss:0.580492\n",
      "[101]\ttrain-mlogloss:0.560773\ttest-mlogloss:0.58018\n",
      "[102]\ttrain-mlogloss:0.560219\ttest-mlogloss:0.579743\n",
      "[103]\ttrain-mlogloss:0.559602\ttest-mlogloss:0.579338\n",
      "[104]\ttrain-mlogloss:0.559104\ttest-mlogloss:0.57902\n",
      "[105]\ttrain-mlogloss:0.558551\ttest-mlogloss:0.578651\n",
      "[106]\ttrain-mlogloss:0.5581\ttest-mlogloss:0.578436\n",
      "[107]\ttrain-mlogloss:0.557434\ttest-mlogloss:0.57802\n",
      "[108]\ttrain-mlogloss:0.556857\ttest-mlogloss:0.577724\n",
      "[109]\ttrain-mlogloss:0.556517\ttest-mlogloss:0.577557\n",
      "[110]\ttrain-mlogloss:0.555902\ttest-mlogloss:0.57708\n",
      "[111]\ttrain-mlogloss:0.555301\ttest-mlogloss:0.576685\n",
      "[112]\ttrain-mlogloss:0.554723\ttest-mlogloss:0.576376\n",
      "[113]\ttrain-mlogloss:0.554275\ttest-mlogloss:0.576188\n",
      "[114]\ttrain-mlogloss:0.553653\ttest-mlogloss:0.57579\n",
      "[115]\ttrain-mlogloss:0.553208\ttest-mlogloss:0.575506\n",
      "[116]\ttrain-mlogloss:0.552748\ttest-mlogloss:0.575159\n",
      "[117]\ttrain-mlogloss:0.552223\ttest-mlogloss:0.574785\n",
      "[118]\ttrain-mlogloss:0.55169\ttest-mlogloss:0.574513\n",
      "[119]\ttrain-mlogloss:0.551275\ttest-mlogloss:0.57427\n",
      "[120]\ttrain-mlogloss:0.550794\ttest-mlogloss:0.573936\n",
      "[121]\ttrain-mlogloss:0.550229\ttest-mlogloss:0.573694\n",
      "[122]\ttrain-mlogloss:0.549756\ttest-mlogloss:0.573361\n",
      "[123]\ttrain-mlogloss:0.549386\ttest-mlogloss:0.573088\n",
      "[124]\ttrain-mlogloss:0.548828\ttest-mlogloss:0.572743\n",
      "[125]\ttrain-mlogloss:0.548438\ttest-mlogloss:0.572443\n",
      "[126]\ttrain-mlogloss:0.548008\ttest-mlogloss:0.572192\n",
      "[127]\ttrain-mlogloss:0.547582\ttest-mlogloss:0.57194\n",
      "[128]\ttrain-mlogloss:0.547317\ttest-mlogloss:0.571717\n",
      "[129]\ttrain-mlogloss:0.546784\ttest-mlogloss:0.571377\n",
      "[130]\ttrain-mlogloss:0.546175\ttest-mlogloss:0.571044\n",
      "[131]\ttrain-mlogloss:0.545706\ttest-mlogloss:0.570798\n",
      "[132]\ttrain-mlogloss:0.545342\ttest-mlogloss:0.570601\n",
      "[133]\ttrain-mlogloss:0.54481\ttest-mlogloss:0.570265\n",
      "[134]\ttrain-mlogloss:0.544401\ttest-mlogloss:0.570071\n",
      "[135]\ttrain-mlogloss:0.543953\ttest-mlogloss:0.569811\n",
      "[136]\ttrain-mlogloss:0.543484\ttest-mlogloss:0.569619\n",
      "[137]\ttrain-mlogloss:0.542845\ttest-mlogloss:0.569129\n",
      "[138]\ttrain-mlogloss:0.54235\ttest-mlogloss:0.56887\n",
      "[139]\ttrain-mlogloss:0.541849\ttest-mlogloss:0.56854\n",
      "[140]\ttrain-mlogloss:0.541496\ttest-mlogloss:0.568306\n",
      "[141]\ttrain-mlogloss:0.541166\ttest-mlogloss:0.568174\n",
      "[142]\ttrain-mlogloss:0.540833\ttest-mlogloss:0.568047\n",
      "[143]\ttrain-mlogloss:0.540415\ttest-mlogloss:0.567779\n",
      "[144]\ttrain-mlogloss:0.539941\ttest-mlogloss:0.567432\n",
      "[145]\ttrain-mlogloss:0.539457\ttest-mlogloss:0.567208\n",
      "[146]\ttrain-mlogloss:0.539046\ttest-mlogloss:0.566927\n",
      "[147]\ttrain-mlogloss:0.538643\ttest-mlogloss:0.566755\n",
      "[148]\ttrain-mlogloss:0.538322\ttest-mlogloss:0.566566\n",
      "[149]\ttrain-mlogloss:0.537905\ttest-mlogloss:0.56627\n",
      "[150]\ttrain-mlogloss:0.537535\ttest-mlogloss:0.56608\n",
      "[151]\ttrain-mlogloss:0.537134\ttest-mlogloss:0.565942\n",
      "[152]\ttrain-mlogloss:0.536758\ttest-mlogloss:0.565764\n",
      "[153]\ttrain-mlogloss:0.536387\ttest-mlogloss:0.565546\n",
      "[154]\ttrain-mlogloss:0.536002\ttest-mlogloss:0.565347\n",
      "[155]\ttrain-mlogloss:0.535698\ttest-mlogloss:0.565168\n",
      "[156]\ttrain-mlogloss:0.535162\ttest-mlogloss:0.564793\n",
      "[157]\ttrain-mlogloss:0.534704\ttest-mlogloss:0.564602\n",
      "[158]\ttrain-mlogloss:0.534334\ttest-mlogloss:0.564443\n",
      "[159]\ttrain-mlogloss:0.534066\ttest-mlogloss:0.56428\n",
      "[160]\ttrain-mlogloss:0.533567\ttest-mlogloss:0.564062\n",
      "[161]\ttrain-mlogloss:0.53315\ttest-mlogloss:0.563776\n",
      "[162]\ttrain-mlogloss:0.532745\ttest-mlogloss:0.56356\n",
      "[163]\ttrain-mlogloss:0.532495\ttest-mlogloss:0.563449\n",
      "[164]\ttrain-mlogloss:0.532156\ttest-mlogloss:0.563272\n",
      "[165]\ttrain-mlogloss:0.531849\ttest-mlogloss:0.563107\n",
      "[166]\ttrain-mlogloss:0.531499\ttest-mlogloss:0.562957\n",
      "[167]\ttrain-mlogloss:0.531196\ttest-mlogloss:0.562947\n",
      "[168]\ttrain-mlogloss:0.530815\ttest-mlogloss:0.562778\n",
      "[169]\ttrain-mlogloss:0.530405\ttest-mlogloss:0.562562\n",
      "[170]\ttrain-mlogloss:0.53012\ttest-mlogloss:0.562478\n",
      "[171]\ttrain-mlogloss:0.529715\ttest-mlogloss:0.562306\n",
      "[172]\ttrain-mlogloss:0.529304\ttest-mlogloss:0.562056\n",
      "[173]\ttrain-mlogloss:0.529101\ttest-mlogloss:0.561914\n",
      "[174]\ttrain-mlogloss:0.528777\ttest-mlogloss:0.561872\n",
      "[175]\ttrain-mlogloss:0.52841\ttest-mlogloss:0.561658\n",
      "[176]\ttrain-mlogloss:0.528087\ttest-mlogloss:0.561564\n",
      "[177]\ttrain-mlogloss:0.527825\ttest-mlogloss:0.56143\n",
      "[178]\ttrain-mlogloss:0.527482\ttest-mlogloss:0.5613\n",
      "[179]\ttrain-mlogloss:0.527097\ttest-mlogloss:0.56117\n",
      "[180]\ttrain-mlogloss:0.526787\ttest-mlogloss:0.560992\n",
      "[181]\ttrain-mlogloss:0.526343\ttest-mlogloss:0.560775\n",
      "[182]\ttrain-mlogloss:0.525957\ttest-mlogloss:0.560712\n",
      "[183]\ttrain-mlogloss:0.5257\ttest-mlogloss:0.560569\n",
      "[184]\ttrain-mlogloss:0.525431\ttest-mlogloss:0.56049\n",
      "[185]\ttrain-mlogloss:0.525201\ttest-mlogloss:0.560361\n",
      "[186]\ttrain-mlogloss:0.524916\ttest-mlogloss:0.560208\n",
      "[187]\ttrain-mlogloss:0.524594\ttest-mlogloss:0.560112\n",
      "[188]\ttrain-mlogloss:0.524303\ttest-mlogloss:0.559943\n",
      "[189]\ttrain-mlogloss:0.523855\ttest-mlogloss:0.559543\n",
      "[190]\ttrain-mlogloss:0.523567\ttest-mlogloss:0.559387\n",
      "[191]\ttrain-mlogloss:0.523211\ttest-mlogloss:0.559358\n",
      "[192]\ttrain-mlogloss:0.522867\ttest-mlogloss:0.559254\n",
      "[193]\ttrain-mlogloss:0.52252\ttest-mlogloss:0.559104\n",
      "[194]\ttrain-mlogloss:0.522193\ttest-mlogloss:0.558982\n",
      "[195]\ttrain-mlogloss:0.521884\ttest-mlogloss:0.558909\n",
      "[196]\ttrain-mlogloss:0.521612\ttest-mlogloss:0.55876\n",
      "[197]\ttrain-mlogloss:0.521258\ttest-mlogloss:0.558578\n",
      "[198]\ttrain-mlogloss:0.520892\ttest-mlogloss:0.558362\n",
      "[199]\ttrain-mlogloss:0.5206\ttest-mlogloss:0.558237\n",
      "[200]\ttrain-mlogloss:0.52029\ttest-mlogloss:0.558145\n",
      "[201]\ttrain-mlogloss:0.519893\ttest-mlogloss:0.558006\n",
      "[202]\ttrain-mlogloss:0.519626\ttest-mlogloss:0.557949\n",
      "[203]\ttrain-mlogloss:0.519279\ttest-mlogloss:0.557784\n",
      "[204]\ttrain-mlogloss:0.518943\ttest-mlogloss:0.557654\n",
      "[205]\ttrain-mlogloss:0.51872\ttest-mlogloss:0.557564\n",
      "[206]\ttrain-mlogloss:0.518386\ttest-mlogloss:0.557451\n",
      "[207]\ttrain-mlogloss:0.518035\ttest-mlogloss:0.55729\n",
      "[208]\ttrain-mlogloss:0.517688\ttest-mlogloss:0.557276\n",
      "[209]\ttrain-mlogloss:0.517404\ttest-mlogloss:0.557158\n",
      "[210]\ttrain-mlogloss:0.517061\ttest-mlogloss:0.557012\n",
      "[211]\ttrain-mlogloss:0.516756\ttest-mlogloss:0.55695\n",
      "[212]\ttrain-mlogloss:0.516449\ttest-mlogloss:0.55683\n",
      "[213]\ttrain-mlogloss:0.516182\ttest-mlogloss:0.556754\n",
      "[214]\ttrain-mlogloss:0.515784\ttest-mlogloss:0.556568\n",
      "[215]\ttrain-mlogloss:0.515469\ttest-mlogloss:0.556439\n",
      "[216]\ttrain-mlogloss:0.5151\ttest-mlogloss:0.556312\n",
      "[217]\ttrain-mlogloss:0.514828\ttest-mlogloss:0.556225\n",
      "[218]\ttrain-mlogloss:0.514626\ttest-mlogloss:0.556204\n",
      "[219]\ttrain-mlogloss:0.514263\ttest-mlogloss:0.556176\n",
      "[220]\ttrain-mlogloss:0.513967\ttest-mlogloss:0.556139\n",
      "[221]\ttrain-mlogloss:0.513654\ttest-mlogloss:0.555957\n",
      "[222]\ttrain-mlogloss:0.513323\ttest-mlogloss:0.555799\n",
      "[223]\ttrain-mlogloss:0.513054\ttest-mlogloss:0.555688\n",
      "[224]\ttrain-mlogloss:0.512816\ttest-mlogloss:0.555539\n",
      "[225]\ttrain-mlogloss:0.512528\ttest-mlogloss:0.555367\n",
      "[226]\ttrain-mlogloss:0.512177\ttest-mlogloss:0.555174\n",
      "[227]\ttrain-mlogloss:0.511857\ttest-mlogloss:0.555081\n",
      "[228]\ttrain-mlogloss:0.511617\ttest-mlogloss:0.555002\n",
      "[229]\ttrain-mlogloss:0.511307\ttest-mlogloss:0.554851\n",
      "[230]\ttrain-mlogloss:0.51096\ttest-mlogloss:0.554693\n",
      "[231]\ttrain-mlogloss:0.51066\ttest-mlogloss:0.554621\n",
      "[232]\ttrain-mlogloss:0.51047\ttest-mlogloss:0.554594\n",
      "[233]\ttrain-mlogloss:0.510263\ttest-mlogloss:0.554521\n",
      "[234]\ttrain-mlogloss:0.509992\ttest-mlogloss:0.554372\n",
      "[235]\ttrain-mlogloss:0.509784\ttest-mlogloss:0.554276\n",
      "[236]\ttrain-mlogloss:0.509441\ttest-mlogloss:0.55412\n",
      "[237]\ttrain-mlogloss:0.509175\ttest-mlogloss:0.554085\n",
      "[238]\ttrain-mlogloss:0.508945\ttest-mlogloss:0.553982\n",
      "[239]\ttrain-mlogloss:0.508698\ttest-mlogloss:0.553931\n",
      "[240]\ttrain-mlogloss:0.508325\ttest-mlogloss:0.553642\n",
      "[241]\ttrain-mlogloss:0.507952\ttest-mlogloss:0.553631\n",
      "[242]\ttrain-mlogloss:0.5077\ttest-mlogloss:0.55361\n",
      "[243]\ttrain-mlogloss:0.507515\ttest-mlogloss:0.553544\n",
      "[244]\ttrain-mlogloss:0.50718\ttest-mlogloss:0.553343\n",
      "[245]\ttrain-mlogloss:0.506846\ttest-mlogloss:0.553156\n",
      "[246]\ttrain-mlogloss:0.506599\ttest-mlogloss:0.553077\n",
      "[247]\ttrain-mlogloss:0.506319\ttest-mlogloss:0.552901\n",
      "[248]\ttrain-mlogloss:0.506123\ttest-mlogloss:0.552886\n",
      "[249]\ttrain-mlogloss:0.505726\ttest-mlogloss:0.55272\n",
      "[250]\ttrain-mlogloss:0.505402\ttest-mlogloss:0.552571\n",
      "[251]\ttrain-mlogloss:0.505139\ttest-mlogloss:0.552455\n",
      "[252]\ttrain-mlogloss:0.504804\ttest-mlogloss:0.55233\n",
      "[253]\ttrain-mlogloss:0.504564\ttest-mlogloss:0.552325\n",
      "[254]\ttrain-mlogloss:0.504366\ttest-mlogloss:0.552181\n",
      "[255]\ttrain-mlogloss:0.504089\ttest-mlogloss:0.552112\n",
      "[256]\ttrain-mlogloss:0.503742\ttest-mlogloss:0.552\n",
      "[257]\ttrain-mlogloss:0.503445\ttest-mlogloss:0.551949\n",
      "[258]\ttrain-mlogloss:0.503196\ttest-mlogloss:0.551933\n",
      "[259]\ttrain-mlogloss:0.502844\ttest-mlogloss:0.551825\n",
      "[260]\ttrain-mlogloss:0.502623\ttest-mlogloss:0.551759\n",
      "[261]\ttrain-mlogloss:0.502384\ttest-mlogloss:0.551637\n",
      "[262]\ttrain-mlogloss:0.502062\ttest-mlogloss:0.551446\n",
      "[263]\ttrain-mlogloss:0.501846\ttest-mlogloss:0.551498\n",
      "[264]\ttrain-mlogloss:0.501565\ttest-mlogloss:0.551427\n",
      "[265]\ttrain-mlogloss:0.50126\ttest-mlogloss:0.551404\n",
      "[266]\ttrain-mlogloss:0.500981\ttest-mlogloss:0.551303\n",
      "[267]\ttrain-mlogloss:0.500794\ttest-mlogloss:0.551299\n",
      "[268]\ttrain-mlogloss:0.500482\ttest-mlogloss:0.551173\n",
      "[269]\ttrain-mlogloss:0.500227\ttest-mlogloss:0.551082\n",
      "[270]\ttrain-mlogloss:0.499987\ttest-mlogloss:0.551087\n",
      "[271]\ttrain-mlogloss:0.49972\ttest-mlogloss:0.551049\n",
      "[272]\ttrain-mlogloss:0.499422\ttest-mlogloss:0.550958\n",
      "[273]\ttrain-mlogloss:0.499145\ttest-mlogloss:0.550824\n",
      "[274]\ttrain-mlogloss:0.498855\ttest-mlogloss:0.55072\n",
      "[275]\ttrain-mlogloss:0.498673\ttest-mlogloss:0.550625\n",
      "[276]\ttrain-mlogloss:0.498421\ttest-mlogloss:0.550616\n",
      "[277]\ttrain-mlogloss:0.498202\ttest-mlogloss:0.550568\n",
      "[278]\ttrain-mlogloss:0.497932\ttest-mlogloss:0.550437\n",
      "[279]\ttrain-mlogloss:0.497664\ttest-mlogloss:0.550427\n",
      "[280]\ttrain-mlogloss:0.497379\ttest-mlogloss:0.550413\n",
      "[281]\ttrain-mlogloss:0.497261\ttest-mlogloss:0.550384\n",
      "[282]\ttrain-mlogloss:0.497024\ttest-mlogloss:0.550314\n",
      "[283]\ttrain-mlogloss:0.496796\ttest-mlogloss:0.5502\n",
      "[284]\ttrain-mlogloss:0.49652\ttest-mlogloss:0.550022\n",
      "[285]\ttrain-mlogloss:0.496354\ttest-mlogloss:0.549992\n",
      "[286]\ttrain-mlogloss:0.496059\ttest-mlogloss:0.549932\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "\n",
    "#mini_ranking = 15\n",
    "\n",
    "for dev_index, val_index in KF:\n",
    "        #split the orginal train set into dev_set and val_set\n",
    "        dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "        \n",
    "#====================================================================        \n",
    "        \"\"\"feature engineerings for the categorical features\"\"\"\n",
    "        #fill substitute the small size values by their mean\n",
    "        for f in ['display_address','manager_id','building_id','street_name']:\n",
    "            dev_set,val_set  = singleValueConvert(dev_set,val_set,f,1)\n",
    "        \n",
    "        \n",
    "        #K-FOLD evaluation for the manager skill\n",
    "        \n",
    "        skf=StratifiedKFold(dev_set['interest_level'],5,shuffle=True,random_state = 42)\n",
    "        #dev set adding manager skill\n",
    "        for train,test in skf:\n",
    "            manager_skill_eval(dev_set.iloc[train,:],dev_set.iloc[test,:],update_df = dev_set)\n",
    "            \n",
    "        #val set adding manager skill\n",
    "        manager_skill_eval(dev_set,val_set)\n",
    "        \n",
    "        #dev_set,val_set = manager_skill_eval_old(dev_set,val_set,15)\n",
    "\n",
    "        for f in categorical:\n",
    "\n",
    "            if dev_set[f].dtype=='object':\n",
    "                #print(f)\n",
    "                lbl = preprocessing.LabelEncoder()\n",
    "                lbl.fit(list(dev_set[f])+list(val_set[f]))\n",
    "                dev_set[f] = lbl.transform(list(dev_set[f].values))\n",
    "                val_set[f] = lbl.transform(list(val_set[f].values))\n",
    "        \n",
    "#===================================================================\n",
    "                \n",
    "        #filter the features\n",
    "        dev_X, val_X = dev_set[features_to_use].as_matrix(), val_set[features_to_use].as_matrix()\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        \n",
    "        \"\"\"\n",
    "        runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, \\\n",
    "        seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1, max_depth = 6)\n",
    "        \"\"\"        \n",
    "        \n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y,\\\n",
    "        feature_names=features_to_use,early_stop=20,\n",
    "        num_rounds = 20000, eta = 0.1,max_depth = 4)\n",
    "    \n",
    "        #using rf for feature choosing\n",
    "        #model = ensemble.RandomForestClassifier(500,random_state = 42,class_weight='balanced')\n",
    "        #model.fit(dev_X,dev_y)\n",
    "        #pred_prob = model.predict_proba(val_X)\n",
    "        #pred = model.predict(val_X)\n",
    "            \n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "print cv_scores\n",
    "#print accuracy_score(val_y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     3,     4, ..., 49347, 49349, 49350])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'ManHigh', u'ManLow', u'ManMedium'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:123: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#features_to_use.append('manager_skill')\n",
    "#categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\",\"street_name\"]\n",
    "#features_to_use.extend(categorical)\n",
    "#features_to_use.extend(['diff_price','diff_price_per_bed','diff_price_per_bath','diff_price_per_room'])\n",
    "\n",
    "#====================================================================        \n",
    "\"\"\"feature engineerings for the categorical features\"\"\"\n",
    "\n",
    "train_set, test_set =manager_skill_eval(train_df,test_df,\\\n",
    "unrank_threshold = mini_ranking)\n",
    "\n",
    "\n",
    "#fill substitute the small size values by their mean\n",
    "for f in categorical:\n",
    "    train_set,test_set  = singleValueConvert(train_set,test_set,f,mini_ranking)\n",
    "\n",
    "    if train_set[f].dtype=='object':\n",
    "        #print(f)\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train_df[f])+list(test_df[f]))\n",
    "        train_set[f] = lbl.transform(list(train_set[f].values))\n",
    "        test_set[f] = lbl.transform(list(test_set[f].values))\n",
    "\n",
    "addAvgDiff(train_set,test_set,nn=15)\n",
    "\n",
    "#===================================================================\n",
    "\n",
    "train_X = train_set[features_to_use]\n",
    "test_X = test_set[features_to_use]\n",
    "\n",
    "train_X_m = train_X.as_matrix()\n",
    "test_X_m = test_X.as_matrix()\n",
    "\n",
    "preds, model = runXGB(train_X_m, train_y, test_X_m, num_rounds=243)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_beta1point251-nndiff.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([            u'bathrooms',              u'bedrooms',\n",
       "                 u'building_id',               u'created',\n",
       "                 u'description',       u'display_address',\n",
       "                    u'features',        u'interest_level',\n",
       "                    u'latitude',            u'listing_id',\n",
       "                   u'longitude',            u'manager_id',\n",
       "                      u'photos',                 u'price',\n",
       "              u'street_address',            u'num_photos',\n",
       "                u'num_features', u'num_description_words',\n",
       "                u'created_year',         u'created_month',\n",
       "                 u'created_day',          u'created_hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bathrooms': 0.010285346346753424,\n",
       " 'bedrooms': 0.030466314219872576,\n",
       " 'building_id': 0.06857462383082553,\n",
       " 'created_day': 0.046309475396502646,\n",
       " 'created_hour': 0.04346278975193168,\n",
       " 'created_month': 0.006015317879896977,\n",
       " 'display_address': 0.0806899823776603,\n",
       " 'latitude': 0.09202589128371967,\n",
       " 'listing_id': 0.09887149247661652,\n",
       " 'longitude': 0.07911413853870138,\n",
       " 'manager_id': 0.09904093805069812,\n",
       " 'num_description_words': 0.0829605530703538,\n",
       " 'num_features': 0.04493696624644164,\n",
       " 'num_photos': 0.04109055171478921,\n",
       " 'price': 0.0970753693913515,\n",
       " 'street_address': 0.07908024942388504}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ananlysis by the feature importance by weight\n",
    "weight = model.get_score()\n",
    "total = sum(weight.values())\n",
    "for key in weight:\n",
    "    weight[key] = weight[key]*1.0/total\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ananlysis by the feature importance by gain\n",
    "gain = model.get_score(importance_type='gain')\n",
    "total = sum(gain.values())\n",
    "#for key in gain:\n",
    "#    gain[key] = gain[key]*1.0/total\n",
    "gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bathrooms': 0.15003324661763429,\n",
       " 'bedrooms': 0.11847222747849985,\n",
       " 'building_id': 0.05966144646752775,\n",
       " 'created_day': 0.027908091350767217,\n",
       " 'created_hour': 0.04913703475375256,\n",
       " 'created_month': 0.015463921187964249,\n",
       " 'display_address': 0.051917534421511584,\n",
       " 'latitude': 0.062329192852910546,\n",
       " 'listing_id': 0.05823796559748455,\n",
       " 'longitude': 0.05796867229011468,\n",
       " 'manager_id': 0.0658834209429622,\n",
       " 'num_description_words': 0.04385875263322271,\n",
       " 'num_features': 0.05493240649113651,\n",
       " 'num_photos': 0.053803480057786596,\n",
       " 'price': 0.07955324745771991,\n",
       " 'street_address': 0.050839359399004566}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ananlysis by the feature importance by coverage\n",
    "cover = model.get_score(importance_type='cover')\n",
    "total = sum(cover.values())\n",
    "for key in cover:\n",
    "    cover[key] = cover[key]*1.0/total\n",
    "cover"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
