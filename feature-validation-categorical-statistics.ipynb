{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import  preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try xgboost\n",
    "#fucntion from SRK\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, \\\n",
    "     seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1,\\\n",
    "     max_depth = 6,cv_dict = None,verbose_eval=True):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = eta\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,\\\n",
    "        early_stopping_rounds=early_stop,evals_result = cv_dict,verbose_eval = verbose_eval)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "class CVstatistics(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    self.result : the result dataframe storing the cv results\n",
    "    self.endpoint : the first ending point for the validations\n",
    "    self.turns: the turns for each validation\n",
    "    \n",
    "    validCurve : plot the validation curve,stop at the first endpoint\n",
    "    \"\"\"\n",
    "    def __init__(self,result_dict,metric,k=5):\n",
    "        self.metric = metric\n",
    "        if type(result_dict) == pd.DataFrame:\n",
    "            self.result = result_dict\n",
    "        else:\n",
    "            tempDict = {}\n",
    "            for phase in ['train','test']:\n",
    "                for turn in range(k):\n",
    "                    tempDict[phase+str(turn)]=cv_result[turn][phase][metric]\n",
    "                    self.result=pd.DataFrame(dict([ (key,pd.Series(v)) for key,v in tempDict.iteritems()]))    \n",
    "        \n",
    "        self.endpoint =len(self.result.filter(like = 'train').dropna())\n",
    "        \n",
    "        self.turns = self.result.filter(like = 'test').\\\n",
    "            apply(lambda x : ~np.isnan(x)).cumsum(axis=0).iloc[len(self.result)-1,:]\n",
    "\n",
    "    def validCurve(self,start=0,stop_at_first = True):\n",
    "        if stop_at_first:\n",
    "            eout = self.result.filter(like = 'test').dropna().mean(axis=1)\n",
    "            ein =  self.result.filter(like = 'train').dropna().mean(axis=1)\n",
    "        else:\n",
    "            eout = self.result.filter(like = 'test').mean(axis=1)\n",
    "            ein =  self.result.filter(like = 'train').mean(axis=1)\n",
    "        plt.plot(range(len(eout)), eout,\n",
    "        range(len(ein)), ein)\n",
    "        plt.xlabel(\"turn\")\n",
    "        plt.ylabel(self.metric)\n",
    "        plt.title('Validation Curve')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def errorsAt(self,turn):\n",
    "        eout = self.result.filter(like = 'test').loc[turn].mean()\n",
    "        ein = self.result.filter(like = 'train').loc[turn].mean()\n",
    "        return eout,ein\n",
    "    \n",
    "\n",
    "def showImportance(model,factor_name):\n",
    "    factors = model.get_score(importance_type=factor_name)\n",
    "    factor_list = []\n",
    "    total = sum(factors.values())\n",
    "    for key in factors:\n",
    "        factors[key] = factors[key]*1.0/total\n",
    "        factor_list.append((key,factors[key]))\n",
    "    return sorted(factor_list,key=lambda x : x[1],reverse=True)\n",
    "    \n",
    "#feature processing functions\n",
    "def proecessStreet(address):\n",
    "    #remove the building number\n",
    "    pattern = re.compile('^[\\d-]*[\\s]+')\n",
    "    street = removePunctuation(pattern.sub('',address))\n",
    "    \n",
    "    #sub the st to street\n",
    "    pattern = re.compile('( st)$')\n",
    "    street = pattern.sub(' street',street)\n",
    "    \n",
    "    #sub the ave to avenue\n",
    "    pattern = re.compile('( ave)$')\n",
    "    street = pattern.sub(' avenue',street)\n",
    "    \n",
    "    pattern = re.compile('(\\d+)((th)|(st)|(rd)|(nd))')\n",
    "    street = pattern.sub('\\g<1>',street)\n",
    "    \n",
    "    #deal with the w 14 street => west 14 street\n",
    "    pattern = re.compile('(w)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('west \\g<3>',street)\n",
    "    \n",
    "    #deal with the e....\n",
    "    pattern = re.compile('(e)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('east \\g<3>',street)\n",
    "    \n",
    "    return street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processMap(df):\n",
    "    for i in ['latitude', 'longitude']:\n",
    "        Q1 = df[i].quantile(0.005)\n",
    "        Q3 = df[i].quantile(0.995)\n",
    "        IQR = Q3 - Q1\n",
    "        upper = Q3\n",
    "        lower = Q1\n",
    "        df.ix[(df[i]>upper)|(df[i]<lower),i] = np.nan\n",
    "        #df.ix[:,i] =  df[i].round(3) \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCluster(train_df,test_df,k):\n",
    "    cluster = KMeans(k,random_state = 2333)\n",
    "    cluster.fit(train_df[['latitude', 'longitude']].dropna())\n",
    "    train_df['cluster_id_'+str(k)]=map(lambda x,y: cluster.predict(np.array([x,y]).reshape(1,-1))[0] \\\n",
    "                           if ~(np.isnan(x)|np.isnan(y)) else -1,\\\n",
    "                           train_df['latitude'],train_df['longitude'])\n",
    "    test_df['cluster_id_'+str(k)]=map(lambda x,y: cluster.predict(np.array([x,y]).reshape(1,-1))[0] \\\n",
    "                           if ~(np.isnan(x)|np.isnan(y)) else -1,\\\n",
    "                           test_df['latitude'],test_df['longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorical_statistics(train_df,test_df,cf,nf,update_df = None,\\\n",
    "                           get_mean=True,get_std=True,get_median=True,get_min = True,get_max = True,\\\n",
    "                           get_size = True,get_normalized_in_group = True):\n",
    "    statistics ={}\n",
    "    if get_mean:\n",
    "        statistics['mean']='mean'\n",
    "    if get_max:\n",
    "        statistics['max']='max'\n",
    "    if get_min:\n",
    "        statistics['min']='min'\n",
    "    if get_std:\n",
    "        statistics['std']='std'\n",
    "    if get_median:\n",
    "        statistics['median']='median'\n",
    "    if get_size:\n",
    "        statistics['size']='size'\n",
    "        \n",
    "    values = train_df.groupby(cf)[nf].agg(statistics)\n",
    "    values = values.add_prefix(cf+'_'+nf+'_')\n",
    "    \n",
    "    new_feature = list(values.columns)\n",
    "    \n",
    "    #consider using -1 for others\n",
    "    updateM = test_df[[cf]].join(values, on = cf, how=\"left\")[new_feature]#.fillna(-1)\n",
    "    \n",
    "    if update_df is None: update_df = test_df\n",
    "    \n",
    "    for f in new_feature:\n",
    "        if f not in update_df.columns: \n",
    "            update_df[f] = np.nan\n",
    "    #update the statistics excluding the normalized value\n",
    "    update_df.update(updateM)\n",
    "    \n",
    "    #update the normalized value \n",
    "    if get_normalized_in_group:\n",
    "        if not (get_mean and get_std):\n",
    "            print 'Can\\' get normailized score without gettting mean and std'\n",
    "            return\n",
    "        normal_feature = cf+'_'+nf+'_normalized'\n",
    "        update_df[normal_feature] = (update_df[nf]-update_df[cf+'_'+nf+'_mean'])/update_df[cf+'_'+nf+'_std']\n",
    "        update_df[normal_feature] = update_df[normal_feature].fillna(0)\n",
    "    \n",
    "    for f in new_feature:\n",
    "        update_df[f] = update_df[f].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"../../kaggleData/2sigma/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic numerical features\n",
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some transfromed features\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "\n",
    "#some new numerical features related to the price\n",
    "train_df[\"price_per_bath\"] =  (train_df[\"price\"]*1.0/train_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_bed\"] = (train_df[\"price\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"bath_per_bed\"] = (train_df[\"bathrooms\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_room\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"]+train_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "\n",
    "# adding all these new features to use list # \"listing_id\",\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\\\n",
    "                        \"created_year\",\"listing_id\", \"created_month\", \"created_day\", \"created_hour\"])\n",
    "#price new features\n",
    "features_to_use.extend([\"price_per_bed\",\"bath_per_bed\",\"price_per_room\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter the outliers to be nan\n",
    "processMap(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding the house type\n",
    "train_df['house_type']=pd.Series(map(lambda x,y:(x,y),train_df['bedrooms'],train_df['bathrooms'])).apply(str)\n",
    "#features_to_use.append('house_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(features_to_use)\n",
    "#categorical = [\"display_address\", \"street_address\",'building_id','manager_id','house_type']\n",
    "categorical = [\"display_address\", \"street_address\",'building_id','manager_id']\n",
    "features.extend(categorical)\n",
    "#features.extend(['cluster_id_10','cluster_id_30'])\n",
    "\n",
    "main_st_nf = [\"bathrooms\", \"bedrooms\",\"price_per_bed\",\"bath_per_bed\",\"price_per_room\",\\\n",
    "                  \"num_photos\", \"num_features\", \"num_description_words\",'price']\n",
    "\n",
    "main_statistics = ['mean','max','min','median','size','normalized']\n",
    "\n",
    "for st in main_statistics:\n",
    "    features.extend(map(lambda x : 'cluster_id_10_'+x+'_'+st,main_st_nf))\n",
    "    features.extend(map(lambda x : 'manager_id_'+x+'_'+st,main_st_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py:3847: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  raise_on_error=True)\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.0361\ttest-mlogloss:1.03776\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.469553\ttest-mlogloss:0.568486\n",
      "[200]\ttrain-mlogloss:0.383721\ttest-mlogloss:0.555695\n",
      "Stopping. Best iteration:\n",
      "[236]\ttrain-mlogloss:0.358023\ttest-mlogloss:0.553224\n",
      "\n",
      "loss for the turn 1 is 0.553443164011\n",
      "[0]\ttrain-mlogloss:1.03629\ttest-mlogloss:1.03723\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.470298\ttest-mlogloss:0.567631\n",
      "[200]\ttrain-mlogloss:0.381315\ttest-mlogloss:0.555534\n",
      "Stopping. Best iteration:\n",
      "[262]\ttrain-mlogloss:0.33832\ttest-mlogloss:0.553754\n",
      "\n",
      "loss for the turn 2 is 0.554277973045\n",
      "[0]\ttrain-mlogloss:1.03741\ttest-mlogloss:1.03821\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.469066\ttest-mlogloss:0.567941\n",
      "[200]\ttrain-mlogloss:0.381905\ttest-mlogloss:0.55661\n",
      "Stopping. Best iteration:\n",
      "[277]\ttrain-mlogloss:0.330702\ttest-mlogloss:0.554888\n",
      "\n",
      "loss for the turn 3 is 0.555666892942\n",
      "[0]\ttrain-mlogloss:1.03786\ttest-mlogloss:1.03868\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.467865\ttest-mlogloss:0.565133\n",
      "[200]\ttrain-mlogloss:0.380064\ttest-mlogloss:0.552478\n",
      "Stopping. Best iteration:\n",
      "[277]\ttrain-mlogloss:0.330178\ttest-mlogloss:0.550121\n",
      "\n",
      "loss for the turn 4 is 0.55021497532\n",
      "[0]\ttrain-mlogloss:1.03773\ttest-mlogloss:1.03862\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.469595\ttest-mlogloss:0.565765\n",
      "[200]\ttrain-mlogloss:0.383254\ttest-mlogloss:0.555065\n",
      "[300]\ttrain-mlogloss:0.319827\ttest-mlogloss:0.553933\n",
      "Stopping. Best iteration:\n",
      "[287]\ttrain-mlogloss:0.327417\ttest-mlogloss:0.553532\n",
      "\n",
      "loss for the turn 5 is 0.553938424953\n"
     ]
    }
   ],
   "source": [
    "#running and getting the cv from xgboost\n",
    "cv_scores = []\n",
    "cv_result = []\n",
    "\n",
    "#K-FOLD already defined.If not ,use\n",
    "#KF=KFold(len(train_X),5,shuffle=True,random_state = 42)\n",
    "i=0\n",
    "for dev_index, val_index in KF:\n",
    "        result_dict = {}\n",
    "        \n",
    "        \"\"\"some preprocessing like feature constructed in cv manners\"\"\"\n",
    "        #split the orginal train set into dev_set and val_set\n",
    "        dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "        \n",
    "        getCluster(dev_set,val_set,30)\n",
    "        getCluster(dev_set,val_set,10)\n",
    "        \n",
    "        skf=KFold(len(dev_set),5,shuffle=True,random_state = 42)\n",
    "        \"\"\"\n",
    "        categorical_statistics(train_df,test_df,cf,nf,update_df = None)\n",
    "        \"\"\"\n",
    "        #statitstic based on cid ,cv-manner statistics\n",
    "        for f in main_st_nf:\n",
    "            for train,test in skf:\n",
    "                categorical_statistics(dev_set.iloc[train,:],dev_set.iloc[test,:],'cluster_id_10',f\\\n",
    "                              ,update_df = dev_set)\n",
    "                categorical_statistics(dev_set.iloc[train,:],dev_set.iloc[test,:],'manager_id',f\\\n",
    "                              ,update_df = dev_set)\n",
    "            \n",
    "            categorical_statistics(dev_set,val_set,'cluster_id_10',f)\n",
    "            categorical_statistics(dev_set,val_set,'manager_id',f)\n",
    "\n",
    "        \"\"\" \n",
    "         runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, \\\n",
    "         seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1,\\\n",
    "         max_depth = 6,cv_dict = None):\n",
    "         \"\"\"\n",
    "        for f in categorical:\n",
    "    \n",
    "            if dev_set[f].dtype=='object':\n",
    "            #print(f)\n",
    "                lbl = preprocessing.LabelEncoder()\n",
    "                lbl.fit(list(dev_set[f])+list(val_set[f]))\n",
    "                dev_set[f] = lbl.transform(list(dev_set[f].values))\n",
    "                val_set[f] = lbl.transform(list(val_set[f].values))\n",
    "        \n",
    "        dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "       \n",
    "        \n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y,early_stop  = 20,\\\n",
    "                              feature_names = features,cv_dict = result_dict,verbose_eval=100)\n",
    "       \n",
    "        loss = log_loss(val_y, preds)\n",
    "        cv_scores.append(loss)\n",
    "        cv_result.append(result_dict)\n",
    "        i+=1\n",
    "        print 'loss for the turn '+str(i)+' is '+str(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test0    256\n",
       "test1    282\n",
       "test2    297\n",
       "test3    297\n",
       "test4    307\n",
       "Name: 306, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot the validation curv\n",
    "cvResult = CVstatistics(cv_result,'mlogloss')\n",
    "cvResult.turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55350828605432667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cvResult.validCurve(stop=False)\n",
    "#some errors at certain turn to see the descending\n",
    "cv_scores\n",
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5532725000000001, 0.32902400000000004)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cvResult.validCurve(stop=False)\n",
    "#some errors at certain turn to see the descending\n",
    "cvResult.errorsAt(280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('price', 0.023782450247223502),\n",
       " ('price_per_bed', 0.023174606153476572),\n",
       " ('bathrooms', 0.022687856678926213),\n",
       " ('building_id', 0.022269437603283142),\n",
       " ('price_per_room', 0.021253756875588163),\n",
       " ('num_photos', 0.019529492068654185),\n",
       " ('created_hour', 0.01617431340699021),\n",
       " ('cluster_id_10_price_normalized', 0.015263110459077226),\n",
       " ('cluster_id_10_price_per_room_normalized', 0.015053809394470407),\n",
       " ('bedrooms', 0.014528987617314184),\n",
       " ('cluster_id_10_price_median', 0.013417242997926859),\n",
       " ('manager_id_num_features_median', 0.012698344382386017),\n",
       " ('manager_id_num_photos_min', 0.012506321948649552),\n",
       " ('manager_id_price_per_room_mean', 0.012345966464373566),\n",
       " ('cluster_id_10_price_per_bed_normalized', 0.011831685194025846),\n",
       " ('manager_id_num_features_max', 0.011036274087553257),\n",
       " ('cluster_id_10_price_per_room_mean', 0.010729684478840167),\n",
       " ('manager_id_num_features_mean', 0.010710552801504692),\n",
       " ('cluster_id_10_price_per_room_median', 0.010634669522229906),\n",
       " ('bath_per_bed', 0.010507306712827897),\n",
       " ('manager_id_price_per_bed_median', 0.01050578293026965),\n",
       " ('cluster_id_10_num_features_normalized', 0.01034136415933659),\n",
       " ('manager_id_num_description_words_mean', 0.010250286511933649),\n",
       " ('cluster_id_10_price_per_room_max', 0.010126768364037652),\n",
       " ('manager_id_price_per_bed_size', 0.009883460103293976),\n",
       " ('cluster_id_10_num_photos_normalized', 0.009851813646447941),\n",
       " ('cluster_id_10_price_per_bed_median', 0.009826901511307207),\n",
       " ('longitude', 0.009770181084105113),\n",
       " ('manager_id_price_per_bed_mean', 0.009731155654610421),\n",
       " ('cluster_id_10_price_max', 0.009708491571941688),\n",
       " ('cluster_id_10_bath_per_bed_median', 0.009560003813550759),\n",
       " ('manager_id_num_photos_median', 0.009424133644650626),\n",
       " ('cluster_id_10_bath_per_bed_size', 0.00920183455106116),\n",
       " ('latitude', 0.009132174654910294),\n",
       " ('manager_id_price_per_room_median', 0.009076728355838537),\n",
       " ('manager_id_num_description_words_median', 0.009069425925623662),\n",
       " ('manager_id_price_per_room_min', 0.009061459529708337),\n",
       " ('manager_id_bathrooms_size', 0.009040860144023307),\n",
       " ('cluster_id_10_bathrooms_mean', 0.009008365063370844),\n",
       " ('num_features', 0.00886178618684721),\n",
       " ('manager_id_bath_per_bed_max', 0.008757765223245296),\n",
       " ('manager_id_num_features_min', 0.008673234157545864),\n",
       " ('manager_id_num_photos_max', 0.008573541030705989),\n",
       " ('manager_id_bedrooms_size', 0.008419288691729466),\n",
       " ('manager_id_price_per_room_max', 0.008337227186764601),\n",
       " ('manager_id_price_per_bed_max', 0.008314660261815389),\n",
       " ('cluster_id_10_num_photos_max', 0.008202693218290695),\n",
       " ('cluster_id_10_bedrooms_normalized', 0.008119300854558384),\n",
       " ('manager_id_bathrooms_normalized', 0.008088760991108954),\n",
       " ('manager_id_price_mean', 0.008067136853361988),\n",
       " ('manager_id', 0.008019378688814601),\n",
       " ('cluster_id_10_price_min', 0.00788051432826687),\n",
       " ('cluster_id_10_num_photos_mean', 0.007844585022547375),\n",
       " ('manager_id_price_max', 0.007828896745094903),\n",
       " ('manager_id_num_description_words_max', 0.007820205749653008),\n",
       " ('manager_id_num_description_words_min', 0.007798746178237784),\n",
       " ('manager_id_bath_per_bed_mean', 0.007782317136007095),\n",
       " ('listing_id', 0.007669458710694702),\n",
       " ('manager_id_price_median', 0.007663647180809007),\n",
       " ('cluster_id_10_price_per_bed_mean', 0.007660420347048695),\n",
       " ('street_address', 0.0076260191672775585),\n",
       " ('cluster_id_10_num_description_words_max', 0.007610848055659019),\n",
       " ('cluster_id_10_bathrooms_normalized', 0.007601988598299678),\n",
       " ('cluster_id_10_price_per_bed_max', 0.007556930561332377),\n",
       " ('cluster_id_10_num_description_words_normalized', 0.007533499493897351),\n",
       " ('manager_id_price_normalized', 0.007501455305719823),\n",
       " ('manager_id_num_photos_mean', 0.007469339351441761),\n",
       " ('manager_id_bathrooms_max', 0.007446462091439734),\n",
       " ('display_address', 0.0074146790061521),\n",
       " ('cluster_id_10_num_features_median', 0.007306089061472399),\n",
       " ('manager_id_price_min', 0.007282767971780349),\n",
       " ('cluster_id_10_bedrooms_size', 0.007253064688722966),\n",
       " ('manager_id_price_per_room_normalized', 0.0072278667232092845),\n",
       " ('cluster_id_10_num_features_mean', 0.007151268295865262),\n",
       " ('manager_id_bathrooms_mean', 0.007123546772310807),\n",
       " ('cluster_id_10_num_description_words_median', 0.00705655736434556),\n",
       " ('manager_id_bath_per_bed_median', 0.0069767241456177975),\n",
       " ('manager_id_price_per_bed_min', 0.00692588964982899),\n",
       " ('manager_id_bedrooms_mean', 0.006909577674402584),\n",
       " ('cluster_id_10_bath_per_bed_normalized', 0.006907702571632116),\n",
       " ('cluster_id_10_num_description_words_mean', 0.006745999101225537),\n",
       " ('num_description_words', 0.006732355460140196),\n",
       " ('manager_id_bath_per_bed_min', 0.006725205061158762),\n",
       " ('manager_id_bedrooms_max', 0.006698804442298296),\n",
       " ('cluster_id_10_bathrooms_size', 0.0066908308742550885),\n",
       " ('cluster_id_10_bedrooms_mean', 0.006653262258718733),\n",
       " ('manager_id_num_photos_normalized', 0.00661948940246042),\n",
       " ('manager_id_bath_per_bed_normalized', 0.006607192449592404),\n",
       " ('manager_id_bathrooms_min', 0.0065726302210987525),\n",
       " ('cluster_id_10_price_mean', 0.006569111755761804),\n",
       " ('manager_id_bedrooms_normalized', 0.006535710675784639),\n",
       " ('manager_id_num_features_normalized', 0.006430366851779489),\n",
       " ('manager_id_num_description_words_normalized', 0.006423345551119508),\n",
       " ('cluster_id_10_bath_per_bed_mean', 0.006391040597416007),\n",
       " ('created_month', 0.00638066195533285),\n",
       " ('manager_id_bathrooms_median', 0.006343557159267118),\n",
       " ('manager_id_bedrooms_median', 0.006308232502877225),\n",
       " ('created_day', 0.006208417244521408),\n",
       " ('cluster_id_10_bathrooms_max', 0.0061921712093262835),\n",
       " ('manager_id_price_per_bed_normalized', 0.0061537249850011026),\n",
       " ('cluster_id_10_price_per_bed_size', 0.006069625483364785),\n",
       " ('cluster_id_10_num_photos_median', 0.006051217986893783),\n",
       " ('cluster_id_10_num_features_max', 0.005947494421123114),\n",
       " ('cluster_id_10_bathrooms_min', 0.005883597243254257),\n",
       " ('manager_id_price_per_room_size', 0.00581189146984605),\n",
       " ('manager_id_bath_per_bed_size', 0.005535233705836467),\n",
       " ('cluster_id_10_bath_per_bed_max', 0.005371492876375092),\n",
       " ('manager_id_bedrooms_min', 0.005278921186049845),\n",
       " ('cluster_id_10_bedrooms_max', 0.00523678123389105),\n",
       " ('cluster_id_10_price_per_room_size', 0.004103419738094135),\n",
       " ('manager_id_num_photos_size', 0.0024768960683984854),\n",
       " ('cluster_id_10_price_per_room_min', 0.0016693316839360834),\n",
       " ('cluster_id_10_num_photos_size', 0.0016090855349025425)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the importance of the features\n",
    "showImportance(model,'gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cid in range(30):\n",
    "    plt.scatter(dev_set.ix[dev_set['cluster_id_30']==cid,'latitude'],\\\n",
    "                dev_set.ix[dev_set['cluster_id_30']==cid,'longitude'],c=np.random.rand(3,))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
