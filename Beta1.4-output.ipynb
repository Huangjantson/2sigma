{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import  preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try xgboost\n",
    "#original fucntion from SRK\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, \\\n",
    "     seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1,\\\n",
    "     max_depth = 6,cv_dict = None,verbose_eval=True):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = eta\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.3\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,\\\n",
    "        early_stopping_rounds=early_stop,evals_result = cv_dict,verbose_eval = verbose_eval)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "class CVstatistics(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    self.result : the result dataframe storing the cv results\n",
    "    self.endpoint : the first ending point for the validations\n",
    "    self.turns: the turns for each validation\n",
    "    \n",
    "    validCurve : plot the validation curve,stop at the first endpoint\n",
    "    errorsAt: return the average errors at a certain turn\n",
    "    \"\"\"\n",
    "    def __init__(self,result_dict,metric,k=5):\n",
    "        self.metric = metric\n",
    "        if type(result_dict) == pd.DataFrame:\n",
    "            self.result = result_dict\n",
    "        else:\n",
    "            temp_dict = {}\n",
    "            for phase in ['train','test']:\n",
    "                for turn in range(k):\n",
    "                    temp_dict[phase+str(turn)]=cv_result[turn][phase][metric]\n",
    "                    self.result=pd.DataFrame(dict([ (key,pd.Series(v)) for key,v in temp_dict.iteritems()]))    \n",
    "        \n",
    "        self.endpoint =len(self.result.filter(like = 'train').dropna())\n",
    "        \n",
    "        self.turns = self.result.filter(like = 'test').\\\n",
    "            apply(lambda x : ~np.isnan(x)).cumsum(axis=0).iloc[len(self.result)-1,:]\n",
    "\n",
    "    def validCurve(self,start = 0, stop_at_first = True):\n",
    "        if stop_at_first:\n",
    "            eout = self.result.iloc[start:,:].filter(like = 'test').dropna().mean(axis=1)\n",
    "            ein =  self.result.iloc[start:,:].filter(like = 'train').dropna().mean(axis=1)\n",
    "        else:\n",
    "            eout = self.result.iloc[start:,:].filter(like = 'test').mean(axis=1)\n",
    "            ein =  self.result.iloc[start:,:].filter(like = 'train').mean(axis=1)\n",
    "        plt.plot(map(lambda x :x+start,range(len(eout))), eout,\n",
    "        map(lambda x :x+start,range(len(ein))), ein)\n",
    "        plt.xlabel(\"turn\")\n",
    "        plt.ylabel(self.metric)\n",
    "        plt.title('Validation Curve')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def eoutCurve(self,stop_at_first = True):\n",
    "        if stop_at_first:\n",
    "            eout = self.result.iloc[start:,:].filter(like = 'test').dropna().mean(axis=1)\n",
    "        else:\n",
    "            eout = self.result.iloc[start:,:].filter(like = 'test').mean(axis=1)\n",
    "        plt.plot(map(lambda x :x+start,range(len(eout))), eout)\n",
    "        plt.xlabel(\"turn\")\n",
    "        plt.ylabel(self.metric)\n",
    "        plt.title('Eout Curve')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def minAvgEout(self):\n",
    "        meanTestError = cvResult.result.filter(like='test').mean(axis=1)\n",
    "        return meanTestError[meanTestError==np.min(meanTestError)]\n",
    "    \n",
    "    def errorsAt(self,turn):\n",
    "        eout = self.result.filter(like = 'test').loc[turn].mean()\n",
    "        ein = self.result.filter(like = 'train').loc[turn].mean()\n",
    "        return eout,ein\n",
    "    \n",
    "def xgbImportance(model,factor_name):\n",
    "    factors = model.get_score(importance_type=factor_name)\n",
    "    factor_list = []\n",
    "    total = sum(factors.values())\n",
    "    for key in factors:\n",
    "        factors[key] = factors[key]*1.0/total\n",
    "        factor_list.append((key,factors[key]))\n",
    "    return sorted(factor_list,key=lambda x : x[1],reverse=True)\n",
    "    \n",
    "def showFscore(model,normalize = True):\n",
    "    factors = model.get_fscore()\n",
    "    factor_list = []\n",
    "    total = sum(factors.values())\n",
    "    for key in factors:\n",
    "        if normalize:\n",
    "            factors[key] = factors[key]*1.0/total\n",
    "        else:\n",
    "            factors[key] = factors[key]\n",
    "        factor_list.append((key,factors[key]))\n",
    "    return sorted(factor_list,key=lambda x : x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature processing functions\n",
    "#define punctutaion filter\n",
    "def removePunctuation(x):\n",
    "    #filter the head or tail blanks\n",
    "    x = re.sub(r'^\\s+',r' ',x)\n",
    "    x = re.sub(r'\\s+$',r' ',x)\n",
    "    \n",
    "    # Lowercasing all words\n",
    "    x = x.lower()\n",
    "    # Removing non ASCII chars, warning if you are dealing with other languages!!!!!!!!!!!!!!!\n",
    "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n",
    "    #change all the blank to space\n",
    "    x = re.sub(r'\\s',r' ',x)\n",
    "    # Removing (replacing with empty spaces actually) all the punctuations\n",
    "    removing = string.punctuation#.replace('-','')# except '-'\n",
    "    removed = re.sub(\"[\"+removing+\"]\", \"\", x)\n",
    "    #removing the line-changing\n",
    "    #removed = re.sub('\\\\n',\" \",removed)    \n",
    "    return removed\n",
    "\n",
    "#feature processing functions\n",
    "def proecessStreet(address):\n",
    "    #remove the building number\n",
    "    pattern = re.compile('^[\\d-]*[\\s]+')\n",
    "    street = removePunctuation(pattern.sub('',address))\n",
    "    \n",
    "    #sub the st to street\n",
    "    pattern = re.compile('( st)$')\n",
    "    street = pattern.sub(' street',street)\n",
    "    \n",
    "    #sub the ave to avenue\n",
    "    pattern = re.compile('( ave)$')\n",
    "    street = pattern.sub(' avenue',street)\n",
    "    \n",
    "    pattern = re.compile('(\\d+)((th)|(st)|(rd)|(nd))')\n",
    "    street = pattern.sub('\\g<1>',street)\n",
    "    \n",
    "    #deal with the w 14 street => west 14 street\n",
    "    pattern = re.compile('(w)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('west \\g<3>',street)\n",
    "    \n",
    "    #deal with the e....\n",
    "    pattern = re.compile('(e)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('east \\g<3>',street)\n",
    "    \n",
    "    return street\n",
    "    \n",
    "#from \"this is a lit\"s python version by rakhlin\n",
    "def singleValueConvert(df1,df2,column,minimum_size=5):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= minimum_size, column] = -1\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= minimum_size, column] = -1\n",
    "    return df1, df2\n",
    "\n",
    "#add ranking for this function\n",
    "def performance_eval(train_df,test_df,feature,k,smoothing=True,g=1,f=1,update_df =None,random = None):\n",
    "    target_num_map = {'High':2, 'Medium':1, 'Low':0}\n",
    "    temp=pd.concat([train_df[feature],pd.get_dummies(train_df.interest_level)], axis = 1)\\\n",
    "         .groupby(feature).mean()\n",
    "     \n",
    "    new_feature = feature+'_perf'\n",
    "    new_rank = feature+'_rank'\n",
    "    new_nrank = feature+'_nrank'\n",
    "    \n",
    "    temp.columns = ['tempHigh','tempLow', 'tempMed']\n",
    "    \n",
    "    temp[feature+'_origin'] = temp['tempHigh']*2 + temp['tempMed']\n",
    "    mean_values = temp.loc[:, feature+'_origin'].mean()\n",
    "\n",
    "    temp['count'] = train_df.groupby(feature).count().iloc[:,1]\n",
    "    if smoothing:\n",
    "        temp[\"lambda\"] = g / (g + np.exp((k - temp[\"count\"] )/f))\n",
    "        temp[new_feature] = temp[\"lambda\"]*temp[feature+'_origin']+(1-temp[\"lambda\"])*mean_values\n",
    "    else:\n",
    "        temp[new_feature] = temp[feature+'_origin']\n",
    "        \n",
    "    temp[new_rank]=temp[new_feature].rank()\n",
    "    temp[new_nrank]=temp[new_rank]/temp['count']\n",
    "    \n",
    "    # Add uniform noise. Not mentioned in original paper.adding to each manager\n",
    "    if random:\n",
    "        temp[new_feature] *= np.random.uniform(1 - random, 1 + random, len(temp))     \n",
    "\n",
    "    value = test_df[[feature]].join(temp, on=feature, how=\"left\")[[new_feature,new_rank,new_nrank]].fillna(mean_values)\n",
    "    \n",
    "    if update_df is None: update_df = test_df\n",
    "    if new_feature not in update_df.columns: update_df[new_feature] = np.nan\n",
    "    if new_rank not in update_df.columns: update_df[new_rank] = np.nan\n",
    "    if new_nrank not in update_df.columns: update_df[new_nrank] = np.nan\n",
    "\n",
    "    update_df.update(value)\n",
    "    \n",
    "#functions for features\n",
    "def featureList(train_df,test_df,limit = 0.001):\n",
    "    #acquiring the feature lists\n",
    "    features_in_train = train_df[\"features\"].apply(pd.Series).unstack().reset_index(drop = True).dropna().value_counts()\n",
    "    features_in_test = test_df[\"features\"].apply(pd.Series).unstack().reset_index(drop = True).dropna().value_counts()\n",
    "    \n",
    "    filtered_features_in_train = features_in_train[features_in_train > limit*len(train_df)]\n",
    "    filtered_features_in_test = features_in_test[features_in_test > limit*len(test_df)]\n",
    "    accept_list = set(filtered_features_in_train.index).union(set(filtered_features_in_test.index))\n",
    "    return accept_list\n",
    "\n",
    "def featureMapping(train_df,test_df,feature_list):\n",
    "    for feature in feature_list:\n",
    "        #add the feature column for both\n",
    "        #if feature in the row, then set the value for (row,feature) to 1\n",
    "        train_df['with_'+feature]=train_df['features'].apply(lambda x : 1 if feature in x else 0)\n",
    "        test_df['with_'+feature]=test_df['features'].apply(lambda x : 1 if feature in x else 0)\n",
    "    return\n",
    "\n",
    "#new function for clustering\n",
    "def getCluster(train_df,test_df,k):\n",
    "    cluster = KMeans(k,random_state = 2333)\n",
    "    cluster.fit(train_df[['latitude', 'longitude']].dropna())\n",
    "    train_df['cluster_id_'+str(k)]=map(lambda x,y: cluster.predict(np.array([x,y]).reshape(1,-1))[0] \\\n",
    "                           if ~(np.isnan(x)|np.isnan(y)) else -1,\\\n",
    "                           train_df['latitude'],train_df['longitude'])\n",
    "    test_df['cluster_id_'+str(k)]=map(lambda x,y: cluster.predict(np.array([x,y]).reshape(1,-1))[0] \\\n",
    "                           if ~(np.isnan(x)|np.isnan(y)) else -1,\\\n",
    "                           test_df['latitude'],test_df['longitude'])\n",
    "    \n",
    "#setting the outliers to be nan. to be test\n",
    "def processMap(df):\n",
    "    for i in ['latitude', 'longitude']:\n",
    "        Q1 = df[i].quantile(0.005)\n",
    "        Q3 = df[i].quantile(0.995)\n",
    "        IQR = Q3 - Q1\n",
    "        upper = Q3\n",
    "        lower = Q1\n",
    "        df.ix[(df[i]>upper)|(df[i]<lower),i] = np.nan\n",
    "        #df.ix[:,i] =  df[i].round(3) \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manager_lon_lat(train_df,test_df):\n",
    "    \n",
    "    #adding the features about distance and location\n",
    "    temp=train_df[['manager_id',\"latitude\", \"longitude\"]].dropna()\n",
    "    mean_value = temp.groupby('manager_id')[[\"latitude\", \"longitude\"]].mean().round(4)\n",
    "    mean_value.columns = ['mlat','mlon']\n",
    "    std_value = train_df.groupby('manager_id')[[\"latitude\", \"longitude\"]].std()\n",
    "    mstd = std_value[[\"latitude\", \"longitude\"]].mean()\n",
    "    std_value['latitude']=std_value['latitude'].fillna(mstd['latitude'])\n",
    "    std_value['longitude']=std_value['longitude'].fillna(mstd['longitude'])\n",
    "    #manager mean distance\n",
    "    std_value['m_m_distance'] = map(lambda x,y:np.sqrt(x**2+y**2).round(4),\\\n",
    "                                    std_value['latitude'],std_value['longitude'])\n",
    "    \n",
    "    value = pd.concat([mean_value,std_value])\n",
    "\n",
    "    updateMTest = test_df[['manager_id']].join(mean_value, on = 'manager_id', how=\"left\")[['mlat','mlon']].fillna(-1)\n",
    "    updateDTest = test_df[['manager_id']].join(std_value, on='manager_id', how=\"left\")['m_m_distance'].fillna(-1)\n",
    "    updateMTrain = train_df[['manager_id']].join(mean_value, on = 'manager_id', how=\"left\")[['mlat','mlon']].fillna(-1)\n",
    "    updateDTrain = train_df[['manager_id']].join(std_value, on='manager_id', how=\"left\")['m_m_distance'].fillna(-1)\n",
    "    \n",
    "    for f in ['mlat','mlon','m_m_distance']:\n",
    "        if f not in test_df.columns: \n",
    "            test_df[f] = np.nan\n",
    "        if f not in train_df.columns: \n",
    "            train_df[f] = np.nan\n",
    "    \n",
    "    test_df.update(updateDTest)\n",
    "    test_df.update(updateMTest)\n",
    "    \n",
    "    train_df.update(updateDTrain)\n",
    "    train_df.update(updateMTrain)\n",
    "    \n",
    "def categorical_size(train_df,test_df,cf):\n",
    "    values =train_df.groupby(cf)['interest_level'].agg({'size':'size'})\n",
    "    values = values.add_prefix(cf+'_')\n",
    "    new_feature = list(values.columns)\n",
    "    updateTest = test_df[[cf]].join(values, on = cf, how=\"left\")[new_feature].fillna(-1)\n",
    "    updateTrain = train_df[[cf]].join(values, on = cf, how=\"left\")[new_feature]#.fillna(-1)\n",
    "    \n",
    "    for f in new_feature:\n",
    "        if f not in test_df.columns: \n",
    "            test_df[f] = np.nan\n",
    "        if f not in train_df.columns:\n",
    "            train_df[f] = np.nan\n",
    "    #update the statistics excluding the normalized value\n",
    "    test_df.update(updateTest)\n",
    "    train_df.update(updateTrain)\n",
    "    \n",
    "#the new one not using cv-manner for the statistics\n",
    "def categorical_statistics(train_df,test_df,cf,nf,\\\n",
    "                           get_median=True,get_min = True,get_max = True,\\\n",
    "                           get_normalized_in_group = True,mini_size = 20):\n",
    "    statistics ={}\n",
    "    statistics['mean']='mean'\n",
    "    statistics['std']='std'\n",
    "    statistics['size']='size'\n",
    "\n",
    "    if get_max:\n",
    "        statistics['max']='max'\n",
    "    if get_min:\n",
    "        statistics['min']='min'\n",
    "    if get_median:\n",
    "        statistics['median']='median'\n",
    "        \n",
    "    values = train_df.groupby(cf)[nf].agg(statistics)\n",
    "    values = values.add_prefix(cf+'_'+nf+'_')\n",
    "    \n",
    "    new_feature = list(values.columns)\n",
    "    \n",
    "    #consider using -1 for others\n",
    "    updateTest = test_df[[cf]].join(values, on = cf, how=\"left\")[new_feature]#.fillna(-1)\n",
    "    updateTrain = train_df[[cf]].join(values, on = cf, how=\"left\")[new_feature]#.fillna(-1)\n",
    "        \n",
    "    for f in new_feature:\n",
    "        if f not in test_df.columns: \n",
    "            test_df[f] = np.nan\n",
    "        if f not in train_df.columns:\n",
    "            train_df[f] = np.nan\n",
    "    #update the statistics excluding the normalized value\n",
    "    test_df.update(updateTest)\n",
    "    train_df.update(updateTrain)\n",
    "    \n",
    "#try performance instead of high&medium\n",
    "def temporalManagerPerf(train_df,test_df,update_df =None):\n",
    "    temp=pd.concat([train_df,pd.get_dummies(train_df.interest_level)], axis = 1)\n",
    "    tempTrain = temp[['manager_id','dayofyear','high','low','medium']].set_index('manager_id')\n",
    "    tempTest = test_df[['manager_id','dayofyear']]\n",
    "    tempJoin = tempTest.join(tempTrain,on='manager_id',how='left', rsuffix='_toSum')\n",
    "    \n",
    "    #3 day performance\n",
    "    performance_3 = tempJoin[tempJoin['dayofyear'] - tempJoin['dayofyear_toSum']<4]\n",
    "    performance_3 = performance_3.groupby(performance_3.index).sum()[['high','low','medium']]\n",
    "    performance_3['total'] = performance_3['high']+performance_3['low']+performance_3['medium']\n",
    "    performance_3['m3perf'] = (2*performance_3['high']+performance_3['medium'])*1.0/performance_3['total']\n",
    "\n",
    "    \n",
    "    performance_7 = tempJoin[tempJoin['dayofyear'] - tempJoin['dayofyear_toSum']<8]\n",
    "    performance_7 = performance_7.groupby(performance_7.index).sum()[['high','low','medium']]\n",
    "    performance_7['total'] = performance_7['high']+performance_7['low']+performance_7['medium']\n",
    "    performance_7['m7perf'] = (2*performance_7['high']+performance_7['medium'])*1.0/performance_7['total']\n",
    "    \n",
    "    performance_14 = tempJoin[tempJoin['dayofyear'] - tempJoin['dayofyear_toSum']<15]\n",
    "    performance_14 = performance_14.groupby(performance_14.index).sum()[['high','low','medium']]\n",
    "    performance_14['total'] = performance_14['high']+performance_14['low']+performance_14['medium']\n",
    "    performance_14['m14perf'] = (2*performance_14['high']+performance_14['medium'])*1.0/performance_14['total']\n",
    "\n",
    "    \n",
    "    performance_30 = tempJoin[tempJoin['dayofyear'] - tempJoin['dayofyear_toSum']<31]\n",
    "    performance_30 = performance_30.groupby(performance_30.index).sum()[['high','low','medium']]\n",
    "    performance_30['total'] = performance_30['high']+performance_30['low']+performance_30['medium']\n",
    "    performance_30['m30perf'] = (2*performance_30['high']+performance_30['medium'])*1.0/performance_30['total']\n",
    "\n",
    "    update = pd.concat([performance_3[['m3perf']],performance_7[['m7perf']],\\\n",
    "                        performance_14[['m14perf']],performance_30[['m30perf']]],axis=1).fillna(-1)\n",
    "\n",
    "    if update_df is None: update_df = test_df\n",
    "    \n",
    "    new_features = ['m3perf','m7perf','m14perf','m30perf']\n",
    "    \n",
    "    for f in new_features:\n",
    "        if f not in update_df.columns: \n",
    "             update_df[f] = np.nan\n",
    "    \n",
    "    update_df.update(update)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#another verision for statistics including some leakage\n",
    "def categorical_statistics(train_df,test_df,cf,nf,\\\n",
    "                           get_median=True,get_min = True,get_max = True,\\\n",
    "                           get_normalized_in_group = True,mini_size = 20):\n",
    "    statistics ={}\n",
    "    statistics['mean']='mean'\n",
    "    statistics['std']='std'\n",
    "    statistics['size']='size'\n",
    "\n",
    "    if get_max:\n",
    "        statistics['max']='max'\n",
    "    if get_min:\n",
    "        statistics['min']='min'\n",
    "    if get_median:\n",
    "        statistics['median']='median'\n",
    "        \n",
    "    values = pd.concat([train_df,test_df]).groupby(cf)[nf].agg(statistics)\n",
    "    values = values.add_prefix(cf+'_'+nf+'_')\n",
    "    \n",
    "    new_feature = list(values.columns)\n",
    "    \n",
    "    #consider using -1 for others\n",
    "    updateTest = test_df[[cf]].join(values, on = cf, how=\"left\")[new_feature]#.fillna(-1)\n",
    "    updateTrain = train_df[[cf]].join(values, on = cf, how=\"left\")[new_feature]#.fillna(-1)\n",
    "        \n",
    "    for f in new_feature:\n",
    "        if f not in test_df.columns: \n",
    "            test_df[f] = np.nan\n",
    "        if f not in train_df.columns:\n",
    "            train_df[f] = np.nan\n",
    "    #update the statistics excluding the normalized value\n",
    "    test_df.update(updateTest)\n",
    "    train_df.update(updateTrain)\n",
    "    \n",
    "#another version for man lat lon including some data leakage\n",
    "def manager_lon_lat(train_df,test_df):\n",
    "    \n",
    "    #adding the features about distance and location\n",
    "    temp=pd.concat([train_df,test_df])[['manager_id',\"latitude\", \"longitude\"]].dropna()\n",
    "    mean_value = temp.groupby('manager_id')[[\"latitude\", \"longitude\"]].mean().round(4)\n",
    "    mean_value.columns = ['mlat','mlon']\n",
    "    std_value = train_df.groupby('manager_id')[[\"latitude\", \"longitude\"]].std()\n",
    "    mstd = std_value[[\"latitude\", \"longitude\"]].mean()\n",
    "    std_value['latitude']=std_value['latitude'].fillna(mstd['latitude'])\n",
    "    std_value['longitude']=std_value['longitude'].fillna(mstd['longitude'])\n",
    "    #manager mean distance\n",
    "    std_value['m_m_distance'] = map(lambda x,y:np.sqrt(x**2+y**2).round(4),\\\n",
    "                                    std_value['latitude'],std_value['longitude'])\n",
    "    \n",
    "    value = pd.concat([mean_value,std_value])\n",
    "\n",
    "    updateMTest = test_df[['manager_id']].join(mean_value, on = 'manager_id', how=\"left\")[['mlat','mlon']].fillna(-1)\n",
    "    updateDTest = test_df[['manager_id']].join(std_value, on='manager_id', how=\"left\")['m_m_distance'].fillna(-1)\n",
    "    updateMTrain = train_df[['manager_id']].join(mean_value, on = 'manager_id', how=\"left\")[['mlat','mlon']].fillna(-1)\n",
    "    updateDTrain = train_df[['manager_id']].join(std_value, on='manager_id', how=\"left\")['m_m_distance'].fillna(-1)\n",
    "    \n",
    "    for f in ['mlat','mlon','m_m_distance']:\n",
    "        if f not in test_df.columns: \n",
    "            test_df[f] = np.nan\n",
    "        if f not in train_df.columns: \n",
    "            train_df[f] = np.nan\n",
    "    \n",
    "    test_df.update(updateDTest)\n",
    "    test_df.update(updateMTest)\n",
    "    \n",
    "    train_df.update(updateDTrain)\n",
    "    train_df.update(updateMTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_on_categorical(train_df,test_df,cf,nf,mini_size=20,random=None):\n",
    "    base = train_df.groupby(cf)[nf].agg({'rank':'rank','size':'size'})\n",
    "    base['nrank'] = base['rank']/base['size']\n",
    "    \n",
    "    if mini_size:\n",
    "        base.ix[base['size']<mini_size,:] = -1\n",
    "    \n",
    "    updateTrain = train_df[[cf]].join(base, on = cf, how=\"left\").fillna(-1)\n",
    "    updateTest = test_df[[cf]].join(base,on=cf,how = 'left').fillna(-1)\n",
    "\n",
    "    n_feature = cf+'_'+nf+'_nrank'\n",
    "    r_feature = cf+'_'+nf+'_rank'\n",
    "    \n",
    "    train_df[n_feature] =  updateTrain['rank']\n",
    "    train_df[r_feature] =  updateTrain['nrank']\n",
    "    \n",
    "    test_df[n_feature] =  updateTest['rank']\n",
    "    test_df[r_feature] =  updateTest['nrank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"../../kaggleData/2sigma/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic numerical features\n",
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some transfromed features\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "#some new numerical features related to the price\n",
    "train_df[\"price_per_bath\"] =  (train_df[\"price\"]*1.0/train_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_bed\"] = (train_df[\"price\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"bath_per_bed\"] = (train_df[\"bathrooms\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_room\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"]+train_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "test_df[\"price_per_bath\"] =  (test_df[\"price\"]*1.0/test_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_bed\"] = (test_df[\"price\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"bath_per_bed\"] = (test_df[\"bathrooms\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_room\"] = (test_df[\"price\"]*1.0/(test_df[\"bedrooms\"]+test_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "\n",
    "# adding all these new features to use list # \"listing_id\",\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\\\n",
    "                        \"created_year\",\"listing_id\", \"created_month\", \"created_day\", \"created_hour\"])\n",
    "#price new features\n",
    "features_to_use.extend([\"price_per_bed\",\"bath_per_bed\",\"price_per_room\"])\n",
    "\n",
    "#for latter use\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processMap(train_df)\n",
    "processMap(test_df)\n",
    "\n",
    "#adding the house type\n",
    "train_df['house_type']=map(lambda x,y:(x,y),train_df['bedrooms'],train_df['bathrooms'])\n",
    "train_df['house_type'] = train_df['house_type'].apply(str)\n",
    "#filling outliers with nan\n",
    "test_df['house_type']=map(lambda x,y:(x,y),test_df['bedrooms'],test_df['bathrooms'])\n",
    "test_df['house_type'] = test_df['house_type'].apply(str)\n",
    "\n",
    "\"\"\"\n",
    "new categorical data generated from the old ones\n",
    "\"\"\"\n",
    "#new feature for the street_address, use them instead of the original one\n",
    "train_df[\"street_name\"] = train_df[\"street_address\"].apply(proecessStreet)\n",
    "test_df[\"street_name\"] = test_df[\"street_address\"].apply(proecessStreet)\n",
    "\n",
    "train_df['building0']=map(lambda x:1 if x== '0' else 0,train_df['building_id'])\n",
    "test_df['building0']=map(lambda x:1 if x== '0' else 0,test_df['building_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dealing with features\n",
    "\n",
    "#preprocessing for features\n",
    "train_df[\"features\"] = train_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_') \\\n",
    "                                                            for i in x])\n",
    "test_df[\"features\"] = test_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_')\\\n",
    "                                                          for i in x])\n",
    "#create the accept list\n",
    "accept_list = list(featureList(train_df,test_df,limit = 0.001))\n",
    "\n",
    "#map the feature to dummy slots\n",
    "featureMapping(train_df,test_df,accept_list)\n",
    "features_to_use.extend(map(lambda x : 'with_'+x,accept_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the basic features from preprocessing \n",
    "features = list(features_to_use)\n",
    "\n",
    "#features to be added during cv by cv-manner statistics\n",
    "features.extend(['manager_id_perf'])\n",
    "features.extend(['m3perf','m7perf','m14perf','m30perf'])\n",
    "features.extend(['manager_id_nrank'])\n",
    "\n",
    "\n",
    "#categorical features to be added\n",
    "categorical = [\"display_address\", \"street_address\",\"street_name\",'building_id',\\\n",
    "'manager_id','building0','house_type']\n",
    "features.extend(categorical)\n",
    "features.extend(['cluster_id_10','cluster_id_30'])\n",
    "\n",
    "#statistical features\n",
    "features.extend(['m_m_distance','mlon','mlat'])\n",
    "\n",
    "main_st_nf = [\"bathrooms\", \"bedrooms\",\"price_per_bed\",\"bath_per_bed\",\\\n",
    "\"price_per_room\",\"num_photos\", \"num_features\", \"num_description_words\",'price']\n",
    "main_statistics =['mean','max','min','median']\n",
    "\n",
    "for st in main_statistics:\n",
    "    features.extend(map(lambda x : 'manager_id_'+x+'_'+st,main_st_nf))\n",
    "    features.extend(map(lambda x : 'house_type_'+x+'_'+st,main_st_nf)) \n",
    "\n",
    "features.extend(map(lambda x : 'cluster_id_10_'+x+'_'+'mean',main_st_nf))\n",
    "features.extend(map(lambda x : 'cluster_id_30_'+x+'_'+'mean',main_st_nf))\n",
    "\n",
    "price_related = ['price_per_bed','price_per_room','price']\n",
    "\n",
    "features.extend(['manager_id_size','house_type_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    #=============================================================        \n",
    "    \"\"\"feature engineerings for the categorical features\"\"\"\n",
    "    #fill substitute the small size values by their mean\n",
    "    for f in categorical:    \n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f])+list(test_df[f]))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "    \n",
    "    for f in categorical:\n",
    "        train_df,test_df  = singleValueConvert(train_df,test_df,f,1)\n",
    "    \n",
    "    #kmeans grouping\n",
    "    getCluster(train_df,test_df,30)\n",
    "    getCluster(train_df,test_df,10)\n",
    "    \n",
    "    #K-FOLD evaluation for the statistic features\n",
    "    skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "    #dev set adding manager skill\n",
    "    for train,test in skf:\n",
    "            performance_eval(train_df.iloc[train,:],train_df.iloc[test,:],feature='manager_id',k=5,g=10,\n",
    "                           update_df = train_df,smoothing=False)\n",
    "            temporalManagerPerf(train_df.iloc[train,:],train_df.iloc[test,:],update_df = train_df)\n",
    "\n",
    "    performance_eval(train_df,test_df,feature='manager_id',k=5,g=10,smoothing=False)\n",
    "    temporalManagerPerf(train_df,test_df)\n",
    "        \n",
    "        \n",
    "    #statitstic\n",
    "    for f in main_st_nf:\n",
    "        #print f\n",
    "        categorical_statistics(train_df,test_df,'manager_id',f)\n",
    "        categorical_statistics(train_df,test_df,'cluster_id_10',f)\n",
    "        categorical_statistics(train_df,test_df,'cluster_id_30',f)\n",
    "        categorical_statistics(train_df,test_df,'house_type',f)\n",
    "        categorical_size(train_df,test_df,'manager_id')\n",
    "        categorical_size(train_df,test_df,'house_type')\n",
    "    \n",
    "    #for f in price_related:\n",
    "    #    rank_on_categorical(train_df,test_df,'house_type_30',f,random =None)\n",
    "\n",
    "    \n",
    "    #manager main location\n",
    "    manager_lon_lat(train_df,test_df)\n",
    "    \n",
    "    for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f])+list(test_df[f]))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"output the processed dataframes\"\"\"\n",
    "#shorten reprocessing time: save the preprocessed train_df and test_df with some basic features\n",
    "#train_df.to_json(data_path+'processed_train_df.json')\n",
    "#test_df.to_json(data_path+'processed_test_df.json')\n",
    "#print features_to_use\n",
    "#shorten reprocessing time: load the preprocessed train_df and test_df with some basic features\n",
    "\n",
    "train_df=pd.read_json(data_path+'processed_train_df.json')\n",
    "test_df=pd.read_json(data_path+'processed_test_df.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output the feature list\n",
    "\"\"\"\n",
    "xgb14featureFile = 'xgb14.feat'\n",
    "fileObject = open(xgb14featureFile,'wb') \n",
    "pickle.dump(features,fileObject)   \n",
    "fileObject.close()\n",
    "\"\"\"\n",
    "xgb14featureFile = 'xgb14.feat'\n",
    "fileObject = open(xgb14featureFile,'r')\n",
    "features = pickle.load(fileObject)\n",
    "fileObject.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare for validation\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=StratifiedKFold(train_y,5,shuffle=True,random_state = 42)\n",
    "\n",
    "train_df = train_df.fillna(-1)\n",
    "#test_df = test_df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the output\n",
    "train_X, test_X = train_df[features].as_matrix(), test_df[features].as_matrix()\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X,\\\n",
    "feature_names=features,\n",
    "num_rounds = 3500, eta = 0.02,max_depth = 4,verbose_eval=100)\n",
    "\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_beta1point4-0.02-2-leakage.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_levels = ['low', 'medium', 'high']\n",
    "\n",
    "tau = {\n",
    "    'low': 0.69195995, \n",
    "    'medium': 0.23108864,\n",
    "    'high': 0.07695141, \n",
    "}\n",
    "\n",
    "def correct(df):\n",
    "    y = df[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print a\n",
    "\n",
    "    def f(p):\n",
    "        for k in range(len(interest_levels)):\n",
    "            p[k] *= a[k]\n",
    "        return p / p.sum()\n",
    "\n",
    "    df_correct = df.copy()\n",
    "    df_correct[interest_levels] = df_correct[interest_levels].apply(f, axis=1)\n",
    "\n",
    "    y = df_correct[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print a\n",
    "\n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99625049318383285, 1.0136787685555868, 0.99326832457494296]\n",
      "[0.99879942216004725, 1.0029246904265237, 1.0019507327558748]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-35016bcb20c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_correct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xgb_beta1point4-0.02-correct.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_correct' is not defined"
     ]
    }
   ],
   "source": [
    "out_df = correct(out_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df.to_csv('xgb_beta1point4-0.02-correct.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the model\n",
    "xgbmodel = 'xgb14.model'\n",
    "modelFileObject = open(xgbmodel,'wb') \n",
    "pickle.dump(model,modelFileObject)   \n",
    "modelFileObject.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
