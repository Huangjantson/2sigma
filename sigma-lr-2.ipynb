{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mochi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 293)\n",
      "(74659, 292)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"/home/raku/kaggleData/2sigma/xgb142/\"\n",
    "train_file = data_path + \"xgb1.42-train.json\"\n",
    "test_file = data_path + \"xgb1.42-test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericals = [u'bath_per_bed',u'bathrooms',u'bedrooms',u'building0',u'cluster_id_10_d',u'cluster_id_30_d',u'dayofyear',\n",
    " u'latitude',u'listing_id',u'longitude',u'm14perf',u'm14perf_f',u'm30perf',u'm30perf_f',u'm3perf',u'm3perf_f',\n",
    " u'm7perf',u'm7perf_f',u'm_c_distance',u'm_m_distance',u'manager_id_nrank',u'manager_id_perf',u'mlat',\n",
    " u'mlon',u'num_description_words',u'num_features', u'num_photos',\n",
    " u'price',u'price_per_bath',u'price_per_bed',u'price_per_room',]\n",
    "\n",
    "numerical_may_processed = [ u'created_day',u'created_hour',u'created_month',u'time_stamp'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_hcc_categoricals=[ u'building_id',u'cluster_id_10',u'cluster_id_30',u'street_name']\n",
    "hcc_categoricals = ['manager_id','house_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#statiscals\n",
    "statistical = []\n",
    "for feature in test_df.columns:\n",
    "    if re.match('((manager_id)|(house_type))\\S+((mean)|(median)|(min)|(max))',feature) !=None:\n",
    "        statistical.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features\n",
    "with_feat = []\n",
    "for feature in test_df.columns:\n",
    "    if re.match('with_\\S+',feature) !=None:\n",
    "        statistical.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_hcc_feature = []\n",
    "\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#hcc encoding for the old hcc features\n",
    "for feature in hcc_categoricals:    \n",
    "    for train,test in skf:\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'high',\\\n",
    "                   update_df = train_df)\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'medium',\\\n",
    "                   update_df = train_df)\n",
    "\n",
    "    hcc_scoring(train_df,test_df,feature,'high')\n",
    "    hcc_scoring(train_df,test_df,feature,'medium')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_high')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_medium')\n",
    "\n",
    "transferred=['another_day','another_hour']\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"],unit='ms')\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"],unit='ms')\n",
    "\n",
    "train_df['another_day']=(train_df['created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "train_df['another_hour']=(train_df['created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "test_df['another_day']=(test_df['created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "test_df['another_hour']=(test_df['created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "\n",
    "train_df['pic_created']=train_df['time_stamp'].apply(datetime.datetime.fromtimestamp)\n",
    "test_df['pic_created']=test_df['time_stamp'].apply(datetime.datetime.fromtimestamp)\n",
    "\n",
    "train_df[\"pic_month\"] = train_df[\"pic_created\"].dt.month\n",
    "test_df[\"pic_month\"] = test_df[\"pic_created\"].dt.month\n",
    "train_df[\"pic_day\"] = train_df[\"pic_created\"].dt.day\n",
    "test_df[\"pic_day\"] = test_df[\"pic_created\"].dt.day\n",
    "train_df[\"pic_hour\"] = train_df[\"pic_created\"].dt.hour\n",
    "test_df[\"pic_hour\"] = test_df[\"pic_created\"].dt.hour\n",
    "\n",
    "train_df['another_pic_day']=(train_df['pic_created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "train_df['another_pic_hour']=(train_df['pic_created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "test_df['another_pic_day']=(test_df['pic_created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "test_df['another_pic_hour']=(test_df['pic_created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "\n",
    "transferred.extend(['pic_month','pic_day','pic_hour','another_pic_day','another_pic_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processing_features = numericals+numerical_may_processed+new_hcc_feature+transferred+statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_train = train_df.copy()\n",
    "normalized_test = test_df.copy()\n",
    "\n",
    "train_test = pd.concat([train_df.drop('interest_level',axis=1),test_df])\n",
    "\n",
    "for f in processing_features:\n",
    "    normalized_train[f]=normalized_train[f].fillna(train_test[f].median())\n",
    "    normalized_test[f]=normalized_test[f].fillna(train_test[f].median())\n",
    "        \n",
    "for f in processing_features:\n",
    "    normalized_train[f]=(normalized_train[f]-train_test[f].mean())/train_test[f].std()\n",
    "    normalized_test[f]=(normalized_test[f]-train_test[f].mean())/train_test[f].std()\n",
    "\n",
    "#store the basic transformed train and test\n",
    "#normalized_train.to_json(data_path+'normal_train_df.json')\n",
    "#normalized_test.to_json(data_path+'normal_test_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=list(processing_features)+with_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58050456098964887]\n",
      "0.58050456099\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = normalized_train.iloc[dev_index,:] , normalized_train.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_new_hcc_features=[]\n",
    "#new cat_gen_features\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#hcc encoding for the old hcc features\n",
    "for feature in new_hcc_categoricals:\n",
    "    for train,test in skf:\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'high',\\\n",
    "                   update_df = train_df)\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'medium',\\\n",
    "                   update_df = train_df)\n",
    "        performance_eval(train_df.iloc[train,:],train_df.iloc[test,:],feature,\\\n",
    "                   update_df = train_df,random=0.01)\n",
    "\n",
    "    hcc_scoring(train_df,test_df,feature,'high')\n",
    "    hcc_scoring(train_df,test_df,feature,'medium')\n",
    "    performance_eval(train_df,test_df,feature,random=0.01)\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_high')\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_medium')\n",
    "    new_new_hcc_features.append(feature+'_nrank_s_r')\n",
    "    new_new_hcc_features.append(feature+'_perf_s_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_train = train_df.copy()\n",
    "normalized_test = test_df.copy()\n",
    "\n",
    "train_test = pd.concat([train_df.drop('interest_level',axis=1),test_df])\n",
    "\n",
    "for f in new_new_hcc_features:\n",
    "    normalized_train[f]=normalized_train[f].fillna(train_test[f].median())\n",
    "    normalized_test[f]=normalized_test[f].fillna(train_test[f].median())\n",
    "        \n",
    "for f in new_new_hcc_features:\n",
    "    normalized_train[f]=(normalized_train[f]-train_test[f].mean())/train_test[f].std()\n",
    "    normalized_test[f]=(normalized_test[f]-train_test[f].mean())/train_test[f].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=list(processing_features)+with_feat+new_new_hcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77667739242905409]\n",
      "0.776677392429\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = normalized_train.iloc[dev_index,:] , normalized_train.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
