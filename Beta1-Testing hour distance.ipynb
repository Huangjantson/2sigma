{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import  preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try xgboost\n",
    "#fucntion from SRK\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature processing functions\n",
    "def getHourDistance(time):\n",
    "    #return a distance from 2016-01-01\n",
    "    year = time.year\n",
    "    month = time.month\n",
    "    day = time.day\n",
    "    hour = time.hour\n",
    "    return hour+day*24+month*24*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"../\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic numerical features\n",
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some transfromed features\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "#add the hourDistance,use it instead of listing id\n",
    "train_df[\"HourDistance\"] = train_df[\"created\"].apply(getHourDistance)\n",
    "test_df[\"HourDistance\"] = test_df[\"created\"].apply(getHourDistance)\n",
    "\n",
    "# adding all these new features to use list excluding # \"listing_id\",or \"HourDistance\",\n",
    "#features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"HourDistance\",\"created_year\", \"created_month\", \"created_day\", \"created_hour\"])\n",
    "\n",
    "#also using the listing id to see if it works better\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"HourDistance\",\"listing_id\",\"created_year\", \"created_month\", \"created_day\", \"created_hour\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dealing feature with categorical features \n",
    "\"\"\"\n",
    "display_address 8826    \n",
    "building_id        7585   =》many zeros in this feature\n",
    "manager_id   3481\n",
    "street_address 15358 =》will be 3800 if no numbers in it \n",
    "\"\"\"\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare for training - for using HDFeature\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_X = train_df[features_to_use]\n",
    "test_X = test_df[features_to_use]\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_X),5,shuffle=True,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.03918\ttest-mlogloss:1.04061\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:0.989109\ttest-mlogloss:0.991955\n",
      "[2]\ttrain-mlogloss:0.947074\ttest-mlogloss:0.95086\n",
      "[3]\ttrain-mlogloss:0.911403\ttest-mlogloss:0.916116\n",
      "[4]\ttrain-mlogloss:0.880169\ttest-mlogloss:0.886116\n",
      "[5]\ttrain-mlogloss:0.854063\ttest-mlogloss:0.860927\n",
      "[6]\ttrain-mlogloss:0.828971\ttest-mlogloss:0.836635\n",
      "[7]\ttrain-mlogloss:0.80714\ttest-mlogloss:0.815615\n",
      "[8]\ttrain-mlogloss:0.786664\ttest-mlogloss:0.796175\n",
      "[9]\ttrain-mlogloss:0.769497\ttest-mlogloss:0.779844\n",
      "[10]\ttrain-mlogloss:0.752352\ttest-mlogloss:0.763503\n",
      "[11]\ttrain-mlogloss:0.73732\ttest-mlogloss:0.749318\n",
      "[12]\ttrain-mlogloss:0.723127\ttest-mlogloss:0.735855\n",
      "[13]\ttrain-mlogloss:0.711223\ttest-mlogloss:0.724666\n",
      "[14]\ttrain-mlogloss:0.701121\ttest-mlogloss:0.715193\n",
      "[15]\ttrain-mlogloss:0.693106\ttest-mlogloss:0.707666\n",
      "[16]\ttrain-mlogloss:0.685471\ttest-mlogloss:0.700879\n",
      "[17]\ttrain-mlogloss:0.677506\ttest-mlogloss:0.693656\n",
      "[18]\ttrain-mlogloss:0.670259\ttest-mlogloss:0.687132\n",
      "[19]\ttrain-mlogloss:0.662945\ttest-mlogloss:0.680655\n",
      "[20]\ttrain-mlogloss:0.657555\ttest-mlogloss:0.675881\n",
      "[21]\ttrain-mlogloss:0.652161\ttest-mlogloss:0.671217\n",
      "[22]\ttrain-mlogloss:0.647492\ttest-mlogloss:0.667336\n",
      "[23]\ttrain-mlogloss:0.642637\ttest-mlogloss:0.663296\n",
      "[24]\ttrain-mlogloss:0.637442\ttest-mlogloss:0.65873\n",
      "[25]\ttrain-mlogloss:0.63395\ttest-mlogloss:0.65585\n",
      "[26]\ttrain-mlogloss:0.629822\ttest-mlogloss:0.652213\n",
      "[27]\ttrain-mlogloss:0.62603\ttest-mlogloss:0.649231\n",
      "[28]\ttrain-mlogloss:0.621573\ttest-mlogloss:0.645403\n",
      "[29]\ttrain-mlogloss:0.618306\ttest-mlogloss:0.642735\n",
      "[30]\ttrain-mlogloss:0.615438\ttest-mlogloss:0.640517\n",
      "[31]\ttrain-mlogloss:0.612301\ttest-mlogloss:0.638089\n",
      "[32]\ttrain-mlogloss:0.608704\ttest-mlogloss:0.63524\n",
      "[33]\ttrain-mlogloss:0.605069\ttest-mlogloss:0.632487\n",
      "[34]\ttrain-mlogloss:0.602525\ttest-mlogloss:0.630621\n",
      "[35]\ttrain-mlogloss:0.600246\ttest-mlogloss:0.629215\n",
      "[36]\ttrain-mlogloss:0.59768\ttest-mlogloss:0.627185\n",
      "[37]\ttrain-mlogloss:0.595551\ttest-mlogloss:0.625746\n",
      "[38]\ttrain-mlogloss:0.592435\ttest-mlogloss:0.623354\n",
      "[39]\ttrain-mlogloss:0.590295\ttest-mlogloss:0.622128\n",
      "[40]\ttrain-mlogloss:0.588016\ttest-mlogloss:0.620356\n",
      "[41]\ttrain-mlogloss:0.585588\ttest-mlogloss:0.618772\n",
      "[42]\ttrain-mlogloss:0.583276\ttest-mlogloss:0.616985\n",
      "[43]\ttrain-mlogloss:0.581597\ttest-mlogloss:0.615947\n",
      "[44]\ttrain-mlogloss:0.579648\ttest-mlogloss:0.614784\n",
      "[45]\ttrain-mlogloss:0.577665\ttest-mlogloss:0.613537\n",
      "[46]\ttrain-mlogloss:0.576316\ttest-mlogloss:0.612698\n",
      "[47]\ttrain-mlogloss:0.574447\ttest-mlogloss:0.611403\n",
      "[48]\ttrain-mlogloss:0.573307\ttest-mlogloss:0.61075\n",
      "[49]\ttrain-mlogloss:0.571652\ttest-mlogloss:0.60978\n",
      "[50]\ttrain-mlogloss:0.57029\ttest-mlogloss:0.608817\n",
      "[51]\ttrain-mlogloss:0.569055\ttest-mlogloss:0.608065\n",
      "[52]\ttrain-mlogloss:0.567941\ttest-mlogloss:0.607538\n",
      "[53]\ttrain-mlogloss:0.566759\ttest-mlogloss:0.60686\n",
      "[54]\ttrain-mlogloss:0.564944\ttest-mlogloss:0.605671\n",
      "[55]\ttrain-mlogloss:0.562912\ttest-mlogloss:0.604576\n",
      "[56]\ttrain-mlogloss:0.561149\ttest-mlogloss:0.603561\n",
      "[57]\ttrain-mlogloss:0.55946\ttest-mlogloss:0.602479\n",
      "[58]\ttrain-mlogloss:0.558028\ttest-mlogloss:0.60191\n",
      "[59]\ttrain-mlogloss:0.556331\ttest-mlogloss:0.601132\n",
      "[60]\ttrain-mlogloss:0.554958\ttest-mlogloss:0.600528\n",
      "[61]\ttrain-mlogloss:0.553205\ttest-mlogloss:0.599459\n",
      "[62]\ttrain-mlogloss:0.551642\ttest-mlogloss:0.598547\n",
      "[63]\ttrain-mlogloss:0.549837\ttest-mlogloss:0.597567\n",
      "[64]\ttrain-mlogloss:0.548559\ttest-mlogloss:0.596711\n",
      "[65]\ttrain-mlogloss:0.5471\ttest-mlogloss:0.595869\n",
      "[66]\ttrain-mlogloss:0.545649\ttest-mlogloss:0.595321\n",
      "[67]\ttrain-mlogloss:0.544378\ttest-mlogloss:0.594693\n",
      "[68]\ttrain-mlogloss:0.542875\ttest-mlogloss:0.593916\n",
      "[69]\ttrain-mlogloss:0.541852\ttest-mlogloss:0.593546\n",
      "[70]\ttrain-mlogloss:0.54061\ttest-mlogloss:0.592992\n",
      "[71]\ttrain-mlogloss:0.539299\ttest-mlogloss:0.592453\n",
      "[72]\ttrain-mlogloss:0.538507\ttest-mlogloss:0.59211\n",
      "[73]\ttrain-mlogloss:0.536906\ttest-mlogloss:0.59133\n",
      "[74]\ttrain-mlogloss:0.536\ttest-mlogloss:0.590875\n",
      "[75]\ttrain-mlogloss:0.535103\ttest-mlogloss:0.590467\n",
      "[76]\ttrain-mlogloss:0.533497\ttest-mlogloss:0.58981\n",
      "[77]\ttrain-mlogloss:0.532671\ttest-mlogloss:0.589438\n",
      "[78]\ttrain-mlogloss:0.531599\ttest-mlogloss:0.589032\n",
      "[79]\ttrain-mlogloss:0.530325\ttest-mlogloss:0.588492\n",
      "[80]\ttrain-mlogloss:0.529248\ttest-mlogloss:0.588119\n",
      "[81]\ttrain-mlogloss:0.527998\ttest-mlogloss:0.587692\n",
      "[82]\ttrain-mlogloss:0.526871\ttest-mlogloss:0.587329\n",
      "[83]\ttrain-mlogloss:0.525759\ttest-mlogloss:0.586819\n",
      "[84]\ttrain-mlogloss:0.524568\ttest-mlogloss:0.586383\n",
      "[85]\ttrain-mlogloss:0.52362\ttest-mlogloss:0.586005\n",
      "[86]\ttrain-mlogloss:0.522393\ttest-mlogloss:0.58561\n",
      "[87]\ttrain-mlogloss:0.521462\ttest-mlogloss:0.585209\n",
      "[88]\ttrain-mlogloss:0.520645\ttest-mlogloss:0.584918\n",
      "[89]\ttrain-mlogloss:0.519315\ttest-mlogloss:0.584446\n",
      "[90]\ttrain-mlogloss:0.518337\ttest-mlogloss:0.584088\n",
      "[91]\ttrain-mlogloss:0.517445\ttest-mlogloss:0.583748\n",
      "[92]\ttrain-mlogloss:0.516504\ttest-mlogloss:0.583442\n",
      "[93]\ttrain-mlogloss:0.515805\ttest-mlogloss:0.583286\n",
      "[94]\ttrain-mlogloss:0.514729\ttest-mlogloss:0.583099\n",
      "[95]\ttrain-mlogloss:0.513708\ttest-mlogloss:0.582705\n",
      "[96]\ttrain-mlogloss:0.512697\ttest-mlogloss:0.582436\n",
      "[97]\ttrain-mlogloss:0.512137\ttest-mlogloss:0.582113\n",
      "[98]\ttrain-mlogloss:0.511409\ttest-mlogloss:0.581874\n",
      "[99]\ttrain-mlogloss:0.510541\ttest-mlogloss:0.581617\n",
      "[100]\ttrain-mlogloss:0.509348\ttest-mlogloss:0.581289\n",
      "[101]\ttrain-mlogloss:0.508325\ttest-mlogloss:0.581025\n",
      "[102]\ttrain-mlogloss:0.507569\ttest-mlogloss:0.580988\n",
      "[103]\ttrain-mlogloss:0.506885\ttest-mlogloss:0.58089\n",
      "[104]\ttrain-mlogloss:0.505725\ttest-mlogloss:0.58058\n",
      "[105]\ttrain-mlogloss:0.505048\ttest-mlogloss:0.580414\n",
      "[106]\ttrain-mlogloss:0.504216\ttest-mlogloss:0.580148\n",
      "[107]\ttrain-mlogloss:0.50337\ttest-mlogloss:0.579889\n",
      "[108]\ttrain-mlogloss:0.502541\ttest-mlogloss:0.579496\n",
      "[109]\ttrain-mlogloss:0.501634\ttest-mlogloss:0.579271\n",
      "[110]\ttrain-mlogloss:0.500817\ttest-mlogloss:0.579059\n",
      "[111]\ttrain-mlogloss:0.499946\ttest-mlogloss:0.57874\n",
      "[112]\ttrain-mlogloss:0.499325\ttest-mlogloss:0.578569\n",
      "[113]\ttrain-mlogloss:0.49844\ttest-mlogloss:0.578348\n",
      "[114]\ttrain-mlogloss:0.497605\ttest-mlogloss:0.578022\n",
      "[115]\ttrain-mlogloss:0.496599\ttest-mlogloss:0.577597\n",
      "[116]\ttrain-mlogloss:0.495892\ttest-mlogloss:0.577462\n",
      "[117]\ttrain-mlogloss:0.494901\ttest-mlogloss:0.577196\n",
      "[118]\ttrain-mlogloss:0.494023\ttest-mlogloss:0.576899\n",
      "[119]\ttrain-mlogloss:0.493213\ttest-mlogloss:0.576791\n",
      "[120]\ttrain-mlogloss:0.492434\ttest-mlogloss:0.576572\n",
      "[121]\ttrain-mlogloss:0.491642\ttest-mlogloss:0.576384\n",
      "[122]\ttrain-mlogloss:0.490805\ttest-mlogloss:0.57618\n",
      "[123]\ttrain-mlogloss:0.490228\ttest-mlogloss:0.57618\n",
      "[124]\ttrain-mlogloss:0.489259\ttest-mlogloss:0.575905\n",
      "[125]\ttrain-mlogloss:0.488478\ttest-mlogloss:0.575695\n",
      "[126]\ttrain-mlogloss:0.487835\ttest-mlogloss:0.575651\n",
      "[127]\ttrain-mlogloss:0.487091\ttest-mlogloss:0.575391\n",
      "[128]\ttrain-mlogloss:0.486278\ttest-mlogloss:0.575137\n",
      "[129]\ttrain-mlogloss:0.485643\ttest-mlogloss:0.574957\n",
      "[130]\ttrain-mlogloss:0.48473\ttest-mlogloss:0.574613\n",
      "[131]\ttrain-mlogloss:0.48432\ttest-mlogloss:0.574451\n",
      "[132]\ttrain-mlogloss:0.483654\ttest-mlogloss:0.574235\n",
      "[133]\ttrain-mlogloss:0.482888\ttest-mlogloss:0.574064\n",
      "[134]\ttrain-mlogloss:0.481949\ttest-mlogloss:0.573851\n",
      "[135]\ttrain-mlogloss:0.48111\ttest-mlogloss:0.573702\n",
      "[136]\ttrain-mlogloss:0.480656\ttest-mlogloss:0.57363\n",
      "[137]\ttrain-mlogloss:0.479788\ttest-mlogloss:0.573461\n",
      "[138]\ttrain-mlogloss:0.479119\ttest-mlogloss:0.573294\n",
      "[139]\ttrain-mlogloss:0.478501\ttest-mlogloss:0.573095\n",
      "[140]\ttrain-mlogloss:0.47752\ttest-mlogloss:0.572986\n",
      "[141]\ttrain-mlogloss:0.476795\ttest-mlogloss:0.572772\n",
      "[142]\ttrain-mlogloss:0.476091\ttest-mlogloss:0.572611\n",
      "[143]\ttrain-mlogloss:0.475373\ttest-mlogloss:0.572444\n",
      "[144]\ttrain-mlogloss:0.474635\ttest-mlogloss:0.572206\n",
      "[145]\ttrain-mlogloss:0.473935\ttest-mlogloss:0.572108\n",
      "[146]\ttrain-mlogloss:0.473095\ttest-mlogloss:0.571907\n",
      "[147]\ttrain-mlogloss:0.472504\ttest-mlogloss:0.571823\n",
      "[148]\ttrain-mlogloss:0.471898\ttest-mlogloss:0.571679\n",
      "[149]\ttrain-mlogloss:0.471113\ttest-mlogloss:0.571413\n",
      "[150]\ttrain-mlogloss:0.470556\ttest-mlogloss:0.571309\n",
      "[151]\ttrain-mlogloss:0.469974\ttest-mlogloss:0.571121\n",
      "[152]\ttrain-mlogloss:0.469395\ttest-mlogloss:0.570999\n",
      "[153]\ttrain-mlogloss:0.468735\ttest-mlogloss:0.570986\n",
      "[154]\ttrain-mlogloss:0.467915\ttest-mlogloss:0.570793\n",
      "[155]\ttrain-mlogloss:0.467185\ttest-mlogloss:0.570679\n",
      "[156]\ttrain-mlogloss:0.466626\ttest-mlogloss:0.570627\n",
      "[157]\ttrain-mlogloss:0.465805\ttest-mlogloss:0.570326\n",
      "[158]\ttrain-mlogloss:0.465081\ttest-mlogloss:0.570048\n",
      "[159]\ttrain-mlogloss:0.464448\ttest-mlogloss:0.569847\n",
      "[160]\ttrain-mlogloss:0.463803\ttest-mlogloss:0.569823\n",
      "[161]\ttrain-mlogloss:0.462911\ttest-mlogloss:0.569691\n",
      "[162]\ttrain-mlogloss:0.462394\ttest-mlogloss:0.569675\n",
      "[163]\ttrain-mlogloss:0.461853\ttest-mlogloss:0.569545\n",
      "[164]\ttrain-mlogloss:0.461114\ttest-mlogloss:0.569264\n",
      "[165]\ttrain-mlogloss:0.460416\ttest-mlogloss:0.56911\n",
      "[166]\ttrain-mlogloss:0.45971\ttest-mlogloss:0.569031\n",
      "[167]\ttrain-mlogloss:0.4591\ttest-mlogloss:0.568917\n",
      "[168]\ttrain-mlogloss:0.458507\ttest-mlogloss:0.568856\n",
      "[169]\ttrain-mlogloss:0.45797\ttest-mlogloss:0.568635\n",
      "[170]\ttrain-mlogloss:0.457258\ttest-mlogloss:0.568536\n",
      "[171]\ttrain-mlogloss:0.456598\ttest-mlogloss:0.56832\n",
      "[172]\ttrain-mlogloss:0.455925\ttest-mlogloss:0.56814\n",
      "[173]\ttrain-mlogloss:0.455353\ttest-mlogloss:0.567995\n",
      "[174]\ttrain-mlogloss:0.454721\ttest-mlogloss:0.567801\n",
      "[175]\ttrain-mlogloss:0.453897\ttest-mlogloss:0.567727\n",
      "[176]\ttrain-mlogloss:0.453452\ttest-mlogloss:0.567598\n",
      "[177]\ttrain-mlogloss:0.452944\ttest-mlogloss:0.567555\n",
      "[178]\ttrain-mlogloss:0.452318\ttest-mlogloss:0.567453\n",
      "[179]\ttrain-mlogloss:0.451802\ttest-mlogloss:0.567351\n",
      "[180]\ttrain-mlogloss:0.451229\ttest-mlogloss:0.567397\n",
      "[181]\ttrain-mlogloss:0.450657\ttest-mlogloss:0.567333\n",
      "[182]\ttrain-mlogloss:0.450127\ttest-mlogloss:0.56713\n",
      "[183]\ttrain-mlogloss:0.449449\ttest-mlogloss:0.567081\n",
      "[184]\ttrain-mlogloss:0.448837\ttest-mlogloss:0.566903\n",
      "[185]\ttrain-mlogloss:0.448159\ttest-mlogloss:0.566679\n",
      "[186]\ttrain-mlogloss:0.447519\ttest-mlogloss:0.566675\n",
      "[187]\ttrain-mlogloss:0.446949\ttest-mlogloss:0.566598\n",
      "[188]\ttrain-mlogloss:0.446362\ttest-mlogloss:0.566554\n",
      "[189]\ttrain-mlogloss:0.445605\ttest-mlogloss:0.566486\n",
      "[190]\ttrain-mlogloss:0.444981\ttest-mlogloss:0.566308\n",
      "[191]\ttrain-mlogloss:0.444476\ttest-mlogloss:0.566125\n",
      "[192]\ttrain-mlogloss:0.443763\ttest-mlogloss:0.565945\n",
      "[193]\ttrain-mlogloss:0.443095\ttest-mlogloss:0.565953\n",
      "[194]\ttrain-mlogloss:0.442493\ttest-mlogloss:0.565871\n",
      "[195]\ttrain-mlogloss:0.441808\ttest-mlogloss:0.56577\n",
      "[196]\ttrain-mlogloss:0.441042\ttest-mlogloss:0.565692\n",
      "[197]\ttrain-mlogloss:0.440513\ttest-mlogloss:0.565508\n",
      "[198]\ttrain-mlogloss:0.440064\ttest-mlogloss:0.565462\n",
      "[199]\ttrain-mlogloss:0.439528\ttest-mlogloss:0.565516\n",
      "[200]\ttrain-mlogloss:0.438955\ttest-mlogloss:0.56561\n",
      "[201]\ttrain-mlogloss:0.438348\ttest-mlogloss:0.565467\n",
      "[202]\ttrain-mlogloss:0.437767\ttest-mlogloss:0.56538\n",
      "[203]\ttrain-mlogloss:0.437265\ttest-mlogloss:0.565343\n",
      "[204]\ttrain-mlogloss:0.436784\ttest-mlogloss:0.565281\n",
      "[205]\ttrain-mlogloss:0.436087\ttest-mlogloss:0.5652\n",
      "[206]\ttrain-mlogloss:0.435685\ttest-mlogloss:0.565064\n",
      "[207]\ttrain-mlogloss:0.435227\ttest-mlogloss:0.565033\n",
      "[208]\ttrain-mlogloss:0.43464\ttest-mlogloss:0.565036\n",
      "[209]\ttrain-mlogloss:0.43408\ttest-mlogloss:0.564969\n",
      "[210]\ttrain-mlogloss:0.433617\ttest-mlogloss:0.564959\n",
      "[211]\ttrain-mlogloss:0.433016\ttest-mlogloss:0.564803\n",
      "[212]\ttrain-mlogloss:0.432553\ttest-mlogloss:0.564804\n",
      "[213]\ttrain-mlogloss:0.432012\ttest-mlogloss:0.564914\n",
      "[214]\ttrain-mlogloss:0.431439\ttest-mlogloss:0.564819\n",
      "[215]\ttrain-mlogloss:0.430761\ttest-mlogloss:0.564624\n",
      "[216]\ttrain-mlogloss:0.430278\ttest-mlogloss:0.564591\n",
      "[217]\ttrain-mlogloss:0.429735\ttest-mlogloss:0.564528\n",
      "[218]\ttrain-mlogloss:0.429147\ttest-mlogloss:0.564457\n",
      "[219]\ttrain-mlogloss:0.428592\ttest-mlogloss:0.564399\n",
      "[220]\ttrain-mlogloss:0.428019\ttest-mlogloss:0.564361\n",
      "[221]\ttrain-mlogloss:0.427545\ttest-mlogloss:0.564349\n",
      "[222]\ttrain-mlogloss:0.427004\ttest-mlogloss:0.564331\n",
      "[223]\ttrain-mlogloss:0.426504\ttest-mlogloss:0.56421\n",
      "[224]\ttrain-mlogloss:0.425926\ttest-mlogloss:0.56417\n",
      "[225]\ttrain-mlogloss:0.425503\ttest-mlogloss:0.56405\n",
      "[226]\ttrain-mlogloss:0.425105\ttest-mlogloss:0.564037\n",
      "[227]\ttrain-mlogloss:0.424609\ttest-mlogloss:0.563939\n",
      "[228]\ttrain-mlogloss:0.424107\ttest-mlogloss:0.563955\n",
      "[229]\ttrain-mlogloss:0.423383\ttest-mlogloss:0.563575\n",
      "[230]\ttrain-mlogloss:0.422786\ttest-mlogloss:0.563571\n",
      "[231]\ttrain-mlogloss:0.422187\ttest-mlogloss:0.563535\n",
      "[232]\ttrain-mlogloss:0.4216\ttest-mlogloss:0.563491\n",
      "[233]\ttrain-mlogloss:0.421019\ttest-mlogloss:0.563507\n",
      "[234]\ttrain-mlogloss:0.420567\ttest-mlogloss:0.563444\n",
      "[235]\ttrain-mlogloss:0.41999\ttest-mlogloss:0.563492\n",
      "[236]\ttrain-mlogloss:0.419196\ttest-mlogloss:0.563483\n",
      "[237]\ttrain-mlogloss:0.418769\ttest-mlogloss:0.563488\n",
      "[238]\ttrain-mlogloss:0.418296\ttest-mlogloss:0.563403\n",
      "[239]\ttrain-mlogloss:0.417822\ttest-mlogloss:0.56345\n",
      "[240]\ttrain-mlogloss:0.417458\ttest-mlogloss:0.563398\n",
      "[241]\ttrain-mlogloss:0.417046\ttest-mlogloss:0.563406\n",
      "[242]\ttrain-mlogloss:0.416595\ttest-mlogloss:0.563356\n",
      "[243]\ttrain-mlogloss:0.41628\ttest-mlogloss:0.563248\n",
      "[244]\ttrain-mlogloss:0.415616\ttest-mlogloss:0.563102\n",
      "[245]\ttrain-mlogloss:0.415217\ttest-mlogloss:0.563107\n",
      "[246]\ttrain-mlogloss:0.41476\ttest-mlogloss:0.563154\n",
      "[247]\ttrain-mlogloss:0.414175\ttest-mlogloss:0.563167\n",
      "[248]\ttrain-mlogloss:0.413556\ttest-mlogloss:0.563198\n",
      "[249]\ttrain-mlogloss:0.413064\ttest-mlogloss:0.563113\n",
      "[250]\ttrain-mlogloss:0.412519\ttest-mlogloss:0.563086\n",
      "[251]\ttrain-mlogloss:0.412012\ttest-mlogloss:0.562992\n",
      "[252]\ttrain-mlogloss:0.411499\ttest-mlogloss:0.562951\n",
      "[253]\ttrain-mlogloss:0.41109\ttest-mlogloss:0.562814\n",
      "[254]\ttrain-mlogloss:0.410809\ttest-mlogloss:0.562741\n",
      "[255]\ttrain-mlogloss:0.41039\ttest-mlogloss:0.562601\n",
      "[256]\ttrain-mlogloss:0.409812\ttest-mlogloss:0.562638\n",
      "[257]\ttrain-mlogloss:0.409264\ttest-mlogloss:0.56264\n",
      "[258]\ttrain-mlogloss:0.408735\ttest-mlogloss:0.562535\n",
      "[259]\ttrain-mlogloss:0.408046\ttest-mlogloss:0.562357\n",
      "[260]\ttrain-mlogloss:0.407482\ttest-mlogloss:0.562479\n",
      "[261]\ttrain-mlogloss:0.407038\ttest-mlogloss:0.562457\n",
      "[262]\ttrain-mlogloss:0.406492\ttest-mlogloss:0.562422\n",
      "[263]\ttrain-mlogloss:0.405926\ttest-mlogloss:0.562315\n",
      "[264]\ttrain-mlogloss:0.4056\ttest-mlogloss:0.562306\n",
      "[265]\ttrain-mlogloss:0.405239\ttest-mlogloss:0.562346\n",
      "[266]\ttrain-mlogloss:0.404929\ttest-mlogloss:0.562343\n",
      "[267]\ttrain-mlogloss:0.40435\ttest-mlogloss:0.562421\n",
      "[268]\ttrain-mlogloss:0.403835\ttest-mlogloss:0.562348\n",
      "[269]\ttrain-mlogloss:0.403393\ttest-mlogloss:0.56228\n",
      "[270]\ttrain-mlogloss:0.403084\ttest-mlogloss:0.562245\n",
      "[271]\ttrain-mlogloss:0.402652\ttest-mlogloss:0.562151\n",
      "[272]\ttrain-mlogloss:0.402217\ttest-mlogloss:0.562118\n",
      "[273]\ttrain-mlogloss:0.40174\ttest-mlogloss:0.562002\n",
      "[274]\ttrain-mlogloss:0.401251\ttest-mlogloss:0.561914\n",
      "[275]\ttrain-mlogloss:0.400737\ttest-mlogloss:0.561766\n",
      "[276]\ttrain-mlogloss:0.400229\ttest-mlogloss:0.561647\n",
      "[277]\ttrain-mlogloss:0.399742\ttest-mlogloss:0.561673\n",
      "[278]\ttrain-mlogloss:0.399266\ttest-mlogloss:0.561623\n",
      "[279]\ttrain-mlogloss:0.398741\ttest-mlogloss:0.561748\n",
      "[280]\ttrain-mlogloss:0.398408\ttest-mlogloss:0.561774\n",
      "[281]\ttrain-mlogloss:0.398077\ttest-mlogloss:0.561821\n",
      "[282]\ttrain-mlogloss:0.397685\ttest-mlogloss:0.561822\n",
      "[283]\ttrain-mlogloss:0.397302\ttest-mlogloss:0.561866\n",
      "[284]\ttrain-mlogloss:0.396813\ttest-mlogloss:0.561928\n",
      "[285]\ttrain-mlogloss:0.396404\ttest-mlogloss:0.561927\n",
      "[286]\ttrain-mlogloss:0.395852\ttest-mlogloss:0.561919\n",
      "[287]\ttrain-mlogloss:0.39518\ttest-mlogloss:0.561854\n",
      "[288]\ttrain-mlogloss:0.394612\ttest-mlogloss:0.561843\n",
      "[289]\ttrain-mlogloss:0.394249\ttest-mlogloss:0.56173\n",
      "[290]\ttrain-mlogloss:0.393982\ttest-mlogloss:0.561737\n",
      "[291]\ttrain-mlogloss:0.393459\ttest-mlogloss:0.561645\n",
      "[292]\ttrain-mlogloss:0.392993\ttest-mlogloss:0.561638\n",
      "[293]\ttrain-mlogloss:0.392518\ttest-mlogloss:0.561721\n",
      "[294]\ttrain-mlogloss:0.392162\ttest-mlogloss:0.5617\n",
      "[295]\ttrain-mlogloss:0.391706\ttest-mlogloss:0.561712\n",
      "[296]\ttrain-mlogloss:0.391139\ttest-mlogloss:0.561609\n",
      "[297]\ttrain-mlogloss:0.390666\ttest-mlogloss:0.561659\n",
      "[298]\ttrain-mlogloss:0.390309\ttest-mlogloss:0.561626\n",
      "[299]\ttrain-mlogloss:0.389805\ttest-mlogloss:0.561558\n",
      "[300]\ttrain-mlogloss:0.389294\ttest-mlogloss:0.561581\n",
      "[301]\ttrain-mlogloss:0.38891\ttest-mlogloss:0.561665\n",
      "[302]\ttrain-mlogloss:0.3884\ttest-mlogloss:0.561682\n",
      "[303]\ttrain-mlogloss:0.388069\ttest-mlogloss:0.561723\n",
      "[304]\ttrain-mlogloss:0.387533\ttest-mlogloss:0.56178\n",
      "[305]\ttrain-mlogloss:0.387008\ttest-mlogloss:0.561679\n",
      "[306]\ttrain-mlogloss:0.386567\ttest-mlogloss:0.561818\n",
      "[307]\ttrain-mlogloss:0.386293\ttest-mlogloss:0.561717\n",
      "[308]\ttrain-mlogloss:0.38567\ttest-mlogloss:0.561604\n",
      "[309]\ttrain-mlogloss:0.385301\ttest-mlogloss:0.561507\n",
      "[310]\ttrain-mlogloss:0.384893\ttest-mlogloss:0.561586\n",
      "[311]\ttrain-mlogloss:0.384473\ttest-mlogloss:0.561545\n",
      "[312]\ttrain-mlogloss:0.384033\ttest-mlogloss:0.561495\n",
      "[313]\ttrain-mlogloss:0.383649\ttest-mlogloss:0.561584\n",
      "[314]\ttrain-mlogloss:0.383171\ttest-mlogloss:0.561466\n",
      "[315]\ttrain-mlogloss:0.382652\ttest-mlogloss:0.561513\n",
      "[316]\ttrain-mlogloss:0.382186\ttest-mlogloss:0.56151\n",
      "[317]\ttrain-mlogloss:0.381814\ttest-mlogloss:0.56157\n",
      "[318]\ttrain-mlogloss:0.381393\ttest-mlogloss:0.56154\n",
      "[319]\ttrain-mlogloss:0.380792\ttest-mlogloss:0.561376\n",
      "[320]\ttrain-mlogloss:0.380466\ttest-mlogloss:0.561467\n",
      "[321]\ttrain-mlogloss:0.380159\ttest-mlogloss:0.561489\n",
      "[322]\ttrain-mlogloss:0.379635\ttest-mlogloss:0.561344\n",
      "[323]\ttrain-mlogloss:0.379147\ttest-mlogloss:0.561296\n",
      "[324]\ttrain-mlogloss:0.378692\ttest-mlogloss:0.561194\n",
      "[325]\ttrain-mlogloss:0.378284\ttest-mlogloss:0.561279\n",
      "[326]\ttrain-mlogloss:0.377848\ttest-mlogloss:0.561272\n",
      "[327]\ttrain-mlogloss:0.377483\ttest-mlogloss:0.561187\n",
      "[328]\ttrain-mlogloss:0.377152\ttest-mlogloss:0.561057\n",
      "[329]\ttrain-mlogloss:0.376679\ttest-mlogloss:0.561033\n",
      "[330]\ttrain-mlogloss:0.376227\ttest-mlogloss:0.561065\n",
      "[331]\ttrain-mlogloss:0.375746\ttest-mlogloss:0.561031\n",
      "[332]\ttrain-mlogloss:0.37531\ttest-mlogloss:0.560989\n",
      "[333]\ttrain-mlogloss:0.374805\ttest-mlogloss:0.560958\n",
      "[334]\ttrain-mlogloss:0.374466\ttest-mlogloss:0.560945\n",
      "[335]\ttrain-mlogloss:0.374105\ttest-mlogloss:0.56102\n",
      "[336]\ttrain-mlogloss:0.373727\ttest-mlogloss:0.561168\n",
      "[337]\ttrain-mlogloss:0.373341\ttest-mlogloss:0.561217\n",
      "[338]\ttrain-mlogloss:0.372971\ttest-mlogloss:0.561186\n",
      "[339]\ttrain-mlogloss:0.372461\ttest-mlogloss:0.561212\n",
      "[340]\ttrain-mlogloss:0.372146\ttest-mlogloss:0.561156\n",
      "[341]\ttrain-mlogloss:0.371655\ttest-mlogloss:0.561039\n",
      "[342]\ttrain-mlogloss:0.37119\ttest-mlogloss:0.561076\n",
      "[343]\ttrain-mlogloss:0.370868\ttest-mlogloss:0.561193\n",
      "[344]\ttrain-mlogloss:0.370399\ttest-mlogloss:0.561223\n",
      "[345]\ttrain-mlogloss:0.369859\ttest-mlogloss:0.561115\n",
      "[346]\ttrain-mlogloss:0.369338\ttest-mlogloss:0.561133\n",
      "[347]\ttrain-mlogloss:0.369119\ttest-mlogloss:0.561159\n",
      "[348]\ttrain-mlogloss:0.368823\ttest-mlogloss:0.561131\n",
      "[349]\ttrain-mlogloss:0.36836\ttest-mlogloss:0.561086\n",
      "[350]\ttrain-mlogloss:0.367988\ttest-mlogloss:0.561107\n",
      "[351]\ttrain-mlogloss:0.367559\ttest-mlogloss:0.561095\n",
      "[352]\ttrain-mlogloss:0.367266\ttest-mlogloss:0.561133\n",
      "[353]\ttrain-mlogloss:0.366858\ttest-mlogloss:0.561147\n",
      "[354]\ttrain-mlogloss:0.366526\ttest-mlogloss:0.561273\n",
      "Stopping. Best iteration:\n",
      "[334]\ttrain-mlogloss:0.374466\ttest-mlogloss:0.560945\n",
      "\n",
      "[0.56127336480626833]\n"
     ]
    }
   ],
   "source": [
    "#running and getting the cv from xgboost\n",
    "cv_scores = []\n",
    "#K-FOLD already defined.If not ,use\n",
    "#KF=KFold(len(train_X),5,shuffle=True,random_state = 42)\n",
    "for dev_index, val_index in KF:\n",
    "        dev_X, val_X = train_X.iloc[dev_index,:].as_matrix(), train_X.iloc[val_index,:].as_matrix()\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y,feature_names=list(list(train_X.columns)))\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print(cv_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ananlysis by the feature importance by weight\n",
    "weight = model.get_score()\n",
    "total = sum(weight.values())\n",
    "for key in weight:\n",
    "    weight[key] = weight[key]*1.0/total\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ananlysis by the feature importance by gain\n",
    "gain = model.get_score(importance_type='gain')\n",
    "total = sum(gain.values())\n",
    "for key in gain:\n",
    "    gain[key] = gain[key]*1.0/total\n",
    "gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ananlysis by the feature importance by coverage\n",
    "cover = model.get_score(importance_type='cover')\n",
    "total = sum(cover.values())\n",
    "for key in cover:\n",
    "    cover[key] = cover[key]*1.0/total\n",
    "cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output the outcome - using xgboost\n",
    "train_X_m = train_X.as_matrix()\n",
    "test_X_m = test_X.as_matrix()\n",
    "\n",
    "preds, model = runXGB(train_X_m, train_y, test_X_m, num_rounds=400)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_beta1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
