{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mochi import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 293)\n",
      "(74659, 292)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"/home/raku/kaggleData/2sigma/xgb142/\"\n",
    "train_file = data_path + \"xgb1.42-train.json\"\n",
    "test_file = data_path + \"xgb1.42-test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericals = [u'bath_per_bed',u'bathrooms',u'bedrooms',u'building0',u'cluster_id_10_d',u'cluster_id_30_d',u'dayofyear',\n",
    " u'latitude',u'listing_id',u'longitude',u'm14perf',u'm14perf_f',u'm30perf',u'm30perf_f',u'm3perf',u'm3perf_f',\n",
    " u'm7perf',u'm7perf_f',u'm_c_distance',u'm_m_distance',u'manager_id_nrank',u'manager_id_perf',u'mlat',\n",
    " u'mlon',u'num_description_words',u'num_features', u'num_photos',\n",
    " u'price',u'price_per_bath',u'price_per_bed',u'price_per_room',]\n",
    "\n",
    "numerical_may_processed = [ u'created_day',u'created_hour',u'created_month',u'time_stamp'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_hcc_categoricals=[ u'building_id',u'cluster_id_10',u'cluster_id_30',u'street_name']\n",
    "hcc_categoricals = ['manager_id','house_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#statiscals\n",
    "statistical = []\n",
    "for feature in test_df.columns:\n",
    "    if re.match('((manager_id)|(house_type))\\S+((mean)|(median)|(min)|(max))',feature) !=None:\n",
    "        statistical.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features\n",
    "with_feat = []\n",
    "for feature in test_df.columns:\n",
    "    if re.match('with_\\S+',feature) !=None:\n",
    "        with_feat.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_hcc_feature = []\n",
    "\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#hcc encoding for the old hcc features\n",
    "for feature in hcc_categoricals:   \n",
    "    new_hcc_feature.append('hcc_'+feature+'_high')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_medium')\n",
    "transferred=['another_day','another_hour']\n",
    "transferred.extend(['pic_month','pic_day','pic_hour','another_pic_day','another_pic_hour'])\n",
    "new_new_hcc_features=[]\n",
    "for feature in new_hcc_categoricals:\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_high')\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_medium')\n",
    "    new_new_hcc_features.append(feature+'_nrank_s_r')\n",
    "    new_new_hcc_features.append(feature+'_perf_s_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_hcc_feature = []\n",
    "\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#hcc encoding for the old hcc features\n",
    "for feature in hcc_categoricals:    \n",
    "    for train,test in skf:\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'high',\\\n",
    "                   update_df = train_df)\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'medium',\\\n",
    "                   update_df = train_df)\n",
    "\n",
    "    hcc_scoring(train_df,test_df,feature,'high')\n",
    "    hcc_scoring(train_df,test_df,feature,'medium')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_high')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_medium')\n",
    "\n",
    "transferred=['another_day','another_hour']\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"],unit='ms')\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"],unit='ms')\n",
    "\n",
    "train_df['another_day']=(train_df['created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "train_df['another_hour']=(train_df['created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "test_df['another_day']=(test_df['created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "test_df['another_hour']=(test_df['created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "\n",
    "train_df['pic_created']=train_df['time_stamp'].apply(datetime.datetime.fromtimestamp)\n",
    "test_df['pic_created']=test_df['time_stamp'].apply(datetime.datetime.fromtimestamp)\n",
    "\n",
    "train_df[\"pic_month\"] = train_df[\"pic_created\"].dt.month\n",
    "test_df[\"pic_month\"] = test_df[\"pic_created\"].dt.month\n",
    "train_df[\"pic_day\"] = train_df[\"pic_created\"].dt.day\n",
    "test_df[\"pic_day\"] = test_df[\"pic_created\"].dt.day\n",
    "train_df[\"pic_hour\"] = train_df[\"pic_created\"].dt.hour\n",
    "test_df[\"pic_hour\"] = test_df[\"pic_created\"].dt.hour\n",
    "\n",
    "train_df['another_pic_day']=(train_df['pic_created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "train_df['another_pic_hour']=(train_df['pic_created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "test_df['another_pic_day']=(test_df['pic_created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "test_df['another_pic_hour']=(test_df['pic_created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "\n",
    "transferred.extend(['pic_month','pic_day','pic_hour','another_pic_day','another_pic_hour'])\n",
    "\n",
    "\n",
    "new_new_hcc_features=[]\n",
    "#new cat_gen_features\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#hcc encoding for the old hcc features\n",
    "for feature in new_hcc_categoricals:\n",
    "    for train,test in skf:\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'high',\\\n",
    "                   update_df = train_df)\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'medium',\\\n",
    "                   update_df = train_df)\n",
    "        performance_eval(train_df.iloc[train,:],train_df.iloc[test,:],feature,\\\n",
    "                   update_df = train_df,random=0.01)\n",
    "\n",
    "    hcc_scoring(train_df,test_df,feature,'high')\n",
    "    hcc_scoring(train_df,test_df,feature,'medium')\n",
    "    performance_eval(train_df,test_df,feature,random=0.01)\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_high')\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_medium')\n",
    "    new_new_hcc_features.append(feature+'_nrank_s_r')\n",
    "    new_new_hcc_features.append(feature+'_perf_s_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processing_features = numericals+numerical_may_processed+new_hcc_feature+transferred+statistical+new_new_hcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in processing_features:\n",
    "    train_df.loc[train_df[f]==-1,f]=np.nan\n",
    "    test_df.loc[test_df[f]==-1,f]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_train = train_df.copy()\n",
    "normalized_test = test_df.copy()\n",
    "\n",
    "train_test = pd.concat([train_df.drop('interest_level',axis=1),test_df])\n",
    "\n",
    "for f in processing_features:\n",
    "    normalized_train[f]=normalized_train[f].fillna(train_test[f].median())\n",
    "    normalized_test[f]=normalized_test[f].fillna(train_test[f].median())\n",
    "        \n",
    "for f in processing_features:\n",
    "    normalized_train[f]=(normalized_train[f]-train_test[f].mean())/train_test[f].std()\n",
    "    normalized_test[f]=(normalized_test[f]-train_test[f].mean())/train_test[f].std()\n",
    "\n",
    "#store the basic transformed train and test\n",
    "#normalized_train.to_json(data_path+'normal_train_df.json')\n",
    "#normalized_test.to_json(data_path+'normal_test_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features=list(processing_features)+with_feat\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57281919267077519]\n",
      "0.572819192671\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = normalized_train.iloc[dev_index,:] , normalized_train.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(-1)\n",
    "test_df = test_df.fillna(-1)\n",
    "\n",
    "store = '/home/raku/kaggleData/2sigma/lr4/'\n",
    "\n",
    "train_df.to_json(store+'lr4-train.json')\n",
    "test_df.to_json(store+'lr4-test.json')\n",
    "\n",
    "pickl_file = store+'lr4features.pickle'\n",
    "fileObject = open(pickl_file,'wb') \n",
    "pickle.dump(features,fileObject)   \n",
    "fileObject.close()\n",
    "print len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(-1)\n",
    "test_df = test_df.fillna(-1)\n",
    "\n",
    "store = '/home/raku/kaggleData/2sigma/lr4/'\n",
    "\n",
    "normalized_train.to_json(store+'lr4-n-train.json')\n",
    "normalized_test.to_json(store+'lr4-n-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try removing the historical and future perfs\n",
    "his_perf=[]\n",
    "for feature in features:\n",
    "    if re.match('m\\d+(perf|perf_f)',feature)!=None:\n",
    "        his_perf.append(feature)\n",
    "        \n",
    "anotherFeatures=[i for i in features if i not in his_perf ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n"
     ]
    }
   ],
   "source": [
    "pickl_file = store+'lr4features.pickle'\n",
    "fileObject = open(pickl_file,'wb') \n",
    "pickle.dump(anotherFeatures,fileObject)   \n",
    "fileObject.close()\n",
    "print len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57272382869917149]\n",
      "0.572723828699\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = normalized_train.iloc[dev_index,:] , normalized_train.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[anotherFeatures].as_matrix(), val_set[anotherFeatures].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trying ica for linear\n",
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
       "    n_components=None, random_state=None, tol=0.0001, w_init=None,\n",
       "    whiten=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying ica\n",
    "fica= FastICA()\n",
    "train_ica_src = train_df[processing_features]\n",
    "fica.fit(train_ica_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "another_train_X = fica.transform(train_ica_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ano_train_X=np.hstack((another_train_X,train_df[with_feat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70538560889641433]\n",
      "0.705385608896\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "for dev_index, val_index in KF:\n",
    "    #dev_set, val_set = normalized_train.iloc[dev_index,:] , normalized_train.iloc[val_index,:] \n",
    "    dev_X, val_X = ano_train_X[dev_index,:], ano_train_X[val_index,:]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
