{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from mochi import CVstatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None,      \n",
    "           seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1,     max_depth = 6,cv_dict = None,verbose_eval=True):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = eta\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.3\n",
    "    param['seed'] = seed_val\n",
    "    param['nthread'] = 4\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,        \n",
    "            early_stopping_rounds=early_stop,evals_result = cv_dict,verbose_eval = verbose_eval)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the data path list\n",
    "data_path = '/home/raku/kaggleData/2sigma/'\n",
    "model_list = ['et2000','knn4','knn8','knn16','knn32','lr4','xgb142','rf2000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_train_temp_list=[]\n",
    "meta_test_temp_list=[]\n",
    "for model in model_list:\n",
    "    dp = os.listdir(data_path+model+'/')\n",
    "    fold_out_file = []\n",
    "    for filename in dp:\n",
    "        if re.match('\\S+-5fold-out-\\d.pickle',filename)!=None:\n",
    "            fold_out_file.append(filename)\n",
    "        elif re.match('\\S+-bulk-out.json',filename)!=None:\n",
    "            bulk_out_file = filename\n",
    "            \n",
    "    fold_out_file=sorted(fold_out_file)\n",
    "    #print fold_out_file\n",
    "    #print bulk_out_file\n",
    "    #load the pickles and combine into meta_train\n",
    "    #load the json bulk out into meta_test\n",
    "    pickle_data = []\n",
    "    json_data = pd.read_json(data_path+model+'/'+bulk_out_file)\n",
    "    temp_meta_test=pd.DataFrame(json_data['high'])\n",
    "    temp_meta_test.columns=[model+'_high']\n",
    "    temp_meta_test[model+'_medium']=json_data['medium']\n",
    "    temp_meta_test[model+'_low']=json_data['low']\n",
    "    test_listing=json_data['listing_id']\n",
    "    \n",
    "    for pickle_file in fold_out_file:\n",
    "        pickl_file = data_path+model+'/'+pickle_file\n",
    "        fileObject = open(pickl_file,'r') \n",
    "        pickle_data.append(pd.DataFrame(pickle.load(fileObject)))   \n",
    "        fileObject.close()\n",
    "    temp_meta_train=pd.concat(pickle_data)\n",
    "    temp_meta_train.columns=[model+'_high',model+'_medium',model+'_low']\n",
    "    temp_meta_test.columns=[model+'_high',model+'_medium',model+'_low']\n",
    "    meta_train_temp_list.append(temp_meta_train)\n",
    "    meta_test_temp_list.append(temp_meta_test)\n",
    "    \n",
    "meta_train=pd.concat(meta_train_temp_list,axis=1)\n",
    "meta_test=pd.concat(meta_test_temp_list,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = []\n",
    "for pickle_file in ['y-5fold-out-0.pickle','y-5fold-out-1.pickle','y-5fold-out-2.pickle','y-5fold-out-3.pickle','y-5fold-out-4.pickle']:\n",
    "    pickl_file = data_path+'/'+pickle_file\n",
    "    fileObject = open(pickl_file,'r') \n",
    "    y_data.append(pd.DataFrame(pickle.load(fileObject)))   \n",
    "    fileObject.close()\n",
    "meta_train_y=np.array(pd.concat(y_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "et2000_high    0.937338\n",
       "knn16_high     0.927173\n",
       "knn32_high     0.927173\n",
       "knn4_high      0.846329\n",
       "knn8_high      0.890040\n",
       "lr4_high       0.800894\n",
       "rf2000_high    0.937338\n",
       "xgb142_high    0.857448\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highs=meta_train.filter(like='_high')\n",
    "coeff_matrix={}\n",
    "for features in highs.columns:\n",
    "    temp_matrix={}\n",
    "    for another_feat in highs.columns:\n",
    "        temp_matrix[another_feat]=pearsonr(highs[features],highs[another_feat])[0]\n",
    "    coeff_matrix[features]=temp_matrix\n",
    "high_coeff_df = pd.DataFrame(coeff_matrix).replace(1.0,-1)\n",
    "high_coeff_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "et2000_medium    0.927758\n",
       "knn16_medium     0.934426\n",
       "knn32_medium     0.934426\n",
       "knn4_medium      0.849902\n",
       "knn8_medium      0.894516\n",
       "lr4_medium       0.861432\n",
       "rf2000_medium    0.927758\n",
       "xgb142_medium    0.862118\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediums=meta_train.filter(like='_medium')\n",
    "coeff_matrix={}\n",
    "for features in mediums.columns:\n",
    "    temp_matrix={}\n",
    "    for another_feat in mediums.columns:\n",
    "        temp_matrix[another_feat]=pearsonr(mediums[features],mediums[another_feat])[0]\n",
    "    coeff_matrix[features]=temp_matrix\n",
    "medium_coeff_df = pd.DataFrame(coeff_matrix).replace(1.0,-1)\n",
    "medium_coeff_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "et2000_low    0.954456\n",
       "knn16_low     0.965450\n",
       "knn32_low     0.965450\n",
       "knn4_low      0.905308\n",
       "knn8_low      0.940407\n",
       "lr4_low       0.903785\n",
       "rf2000_low    0.954456\n",
       "xgb142_low    0.904685\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lows=meta_train.filter(like='_low')\n",
    "coeff_matrix={}\n",
    "for features in lows.columns:\n",
    "    temp_matrix={}\n",
    "    for another_feat in lows.columns:\n",
    "        temp_matrix[another_feat]=pearsonr(lows[features],lows[another_feat])[0]\n",
    "    coeff_matrix[features]=temp_matrix\n",
    "low_coeff_df = pd.DataFrame(coeff_matrix).replace(1.0,-1)\n",
    "low_coeff_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08276\ttest-mlogloss:1.08281\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.566998\ttest-mlogloss:0.567776\n",
      "[200]\ttrain-mlogloss:0.513483\ttest-mlogloss:0.515286\n",
      "[300]\ttrain-mlogloss:0.502975\ttest-mlogloss:0.506824\n",
      "[400]\ttrain-mlogloss:0.497905\ttest-mlogloss:0.504258\n",
      "[500]\ttrain-mlogloss:0.494523\ttest-mlogloss:0.503364\n",
      "[600]\ttrain-mlogloss:0.4917\ttest-mlogloss:0.502913\n",
      "Stopping. Best iteration:\n",
      "[583]\ttrain-mlogloss:0.49214\ttest-mlogloss:0.502901\n",
      "\n",
      "loss for the turn 1 is 0.502910325005\n",
      "[0]\ttrain-mlogloss:1.08281\ttest-mlogloss:1.08278\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.567949\ttest-mlogloss:0.565155\n",
      "[200]\ttrain-mlogloss:0.514432\ttest-mlogloss:0.511628\n",
      "[300]\ttrain-mlogloss:0.504091\ttest-mlogloss:0.502522\n",
      "[400]\ttrain-mlogloss:0.49905\ttest-mlogloss:0.499401\n",
      "[500]\ttrain-mlogloss:0.495719\ttest-mlogloss:0.498123\n",
      "[600]\ttrain-mlogloss:0.492924\ttest-mlogloss:0.49754\n",
      "Stopping. Best iteration:\n",
      "[666]\ttrain-mlogloss:0.491213\ttest-mlogloss:0.49733\n",
      "\n",
      "loss for the turn 2 is 0.497340275773\n",
      "[0]\ttrain-mlogloss:1.08279\ttest-mlogloss:1.08275\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.56735\ttest-mlogloss:0.566286\n",
      "[200]\ttrain-mlogloss:0.513948\ttest-mlogloss:0.513613\n",
      "[300]\ttrain-mlogloss:0.503411\ttest-mlogloss:0.50491\n",
      "[400]\ttrain-mlogloss:0.498392\ttest-mlogloss:0.502071\n",
      "[500]\ttrain-mlogloss:0.494979\ttest-mlogloss:0.500945\n",
      "[600]\ttrain-mlogloss:0.492135\ttest-mlogloss:0.500449\n",
      "[700]\ttrain-mlogloss:0.489526\ttest-mlogloss:0.500283\n",
      "[779]\ttrain-mlogloss:0.487601\ttest-mlogloss:0.500189\n",
      "loss for the turn 3 is 0.500188789914\n",
      "[0]\ttrain-mlogloss:1.0827\ttest-mlogloss:1.08285\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.564926\ttest-mlogloss:0.574099\n",
      "[200]\ttrain-mlogloss:0.511115\ttest-mlogloss:0.523917\n",
      "[300]\ttrain-mlogloss:0.500623\ttest-mlogloss:0.515966\n",
      "[400]\ttrain-mlogloss:0.495621\ttest-mlogloss:0.513452\n",
      "[500]\ttrain-mlogloss:0.492278\ttest-mlogloss:0.512311\n",
      "[600]\ttrain-mlogloss:0.489489\ttest-mlogloss:0.511829\n",
      "Stopping. Best iteration:\n",
      "[620]\ttrain-mlogloss:0.488928\ttest-mlogloss:0.511726\n",
      "\n",
      "loss for the turn 4 is 0.511757121113\n",
      "[0]\ttrain-mlogloss:1.08279\ttest-mlogloss:1.08288\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.566722\ttest-mlogloss:0.574495\n",
      "[200]\ttrain-mlogloss:0.5116\ttest-mlogloss:0.524655\n",
      "[300]\ttrain-mlogloss:0.500203\ttest-mlogloss:0.5169\n",
      "[400]\ttrain-mlogloss:0.494964\ttest-mlogloss:0.514958\n",
      "[500]\ttrain-mlogloss:0.491419\ttest-mlogloss:0.51444\n",
      "Stopping. Best iteration:\n",
      "[522]\ttrain-mlogloss:0.490778\ttest-mlogloss:0.514343\n",
      "\n",
      "loss for the turn 5 is 0.514387547695\n",
      "The mean of the cv_scores is:\n",
      "0.5053168119\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "cv_result = []\n",
    "\n",
    "i=0        \n",
    "for dev_index, val_index in KF.split(meta_train,meta_train_y): \n",
    "    result_dict = {}\n",
    "\n",
    "    dev_X, val_X = meta_train.iloc[dev_index,:].as_matrix(), meta_train.iloc[val_index,:].as_matrix()\n",
    "    dev_y, val_y = meta_train_y[dev_index], meta_train_y[val_index]\n",
    "    \n",
    "    preds,model = runXGB(dev_X, dev_y, val_X, val_y,feature_names=list(meta_train.columns),\\\n",
    "           early_stop = 20,num_rounds=780,eta = 0.02,max_depth=3,cv_dict = result_dict,verbose_eval=100)\n",
    "\n",
    "    loss = log_loss(val_y, preds)\n",
    "    \n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    i+=1\n",
    "    print 'loss for the turn '+str(i)+' is '+str(loss)\n",
    "    \n",
    "print 'The mean of the cv_scores is:'\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676    0.498815\n",
      "677    0.498815\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cvResult = CVstatistics(cv_result,'mlogloss')\n",
    "\n",
    "meanTestError = cvResult.result.filter(like='test').mean(axis=1)\n",
    "\n",
    "print meanTestError[meanTestError==np.min(meanTestError)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X = meta_train.as_matrix(), meta_test.as_matrix()\n",
    "\n",
    "preds, model = runXGB(train_X, meta_train_y, test_X,\n",
    "num_rounds = 670, eta = 0.02,max_depth = 3,verbose_eval=100)\n",
    "\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "#out_df.to_json('stack-.json')\n",
    "out_df[\"listing_id\"] = test_listing.values\n",
    "out_df.to_csv(\"stack-alpah-0.02eta-3md.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high          5.866707e+03\n",
       "medium        1.708426e+04\n",
       "low           5.170623e+04\n",
       "listing_id    5.244067e+11\n",
       "dtype: float32"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(out_df,axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
