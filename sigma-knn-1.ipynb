{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lodaing data\n",
    "data_path = \"/home/raku/kaggleData/2sigma/lr4/\"\n",
    "train_file = data_path + \"lr4-n-train.json\"\n",
    "test_file = data_path + \"lr4-n-test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "pickl_file = data_path+'lr4features.pickle'\n",
    "fileObject = open(pickl_file,'r') \n",
    "lrfeature = pickle.load(fileObject)   \n",
    "fileObject.close()\n",
    "print len(lrfeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statistical = []\n",
    "\n",
    "for feature in lrfeature:\n",
    "    if re.match('((manager_id)|(house_type))\\S+((mean)|(median)|(min)|(max))',feature) !=None:\n",
    "        statistical.append(feature)\n",
    "        \n",
    "weightFeatures=[i for i in lrfeature if i not in statistical ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features\n",
    "with_feat = []\n",
    "for feature in lrfeature:\n",
    "    if re.match('with_\\S+',feature) !=None:\n",
    "        statistical.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightFeatures.remove('another_day')\n",
    "weightFeatures.remove('another_hour')\n",
    "weightFeatures.remove('another_pic_day')\n",
    "weightFeatures.remove('another_pic_hour')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bath_per_bed',\n",
       " u'bathrooms',\n",
       " u'bedrooms',\n",
       " u'building0',\n",
       " u'cluster_id_10_d',\n",
       " u'cluster_id_30_d',\n",
       " u'dayofyear',\n",
       " u'latitude',\n",
       " u'listing_id',\n",
       " u'longitude',\n",
       " u'm_c_distance',\n",
       " u'm_m_distance',\n",
       " u'manager_id_nrank',\n",
       " u'manager_id_perf',\n",
       " u'mlat',\n",
       " u'mlon',\n",
       " u'num_description_words',\n",
       " u'num_features',\n",
       " u'num_photos',\n",
       " u'price',\n",
       " u'price_per_bath',\n",
       " u'price_per_bed',\n",
       " u'price_per_room',\n",
       " u'created_day',\n",
       " u'created_hour',\n",
       " u'created_month',\n",
       " u'time_stamp',\n",
       " 'hcc_manager_id_high',\n",
       " 'hcc_manager_id_medium',\n",
       " 'hcc_house_type_high',\n",
       " 'hcc_house_type_medium',\n",
       " 'pic_month',\n",
       " 'pic_day',\n",
       " 'pic_hour',\n",
       " u'with_actual_apt._photos',\n",
       " u'with_balcony',\n",
       " u'with_bike_room',\n",
       " u'with_cats_allowed',\n",
       " u'with_central_a/c',\n",
       " u'with_childrens_playroom',\n",
       " u'with_common_outdoor_space',\n",
       " u'with_common_parking/garage',\n",
       " u'with_common_roof_deck',\n",
       " u'with_concierge',\n",
       " u'with_dining_room',\n",
       " u'with_dishwasher',\n",
       " u'with_dogs_allowed',\n",
       " u'with_doorman',\n",
       " u'with_dryer_in_unit',\n",
       " u'with_duplex',\n",
       " u'with_eat_in_kitchen',\n",
       " u'with_elevator',\n",
       " u'with_exclusive',\n",
       " u'with_exposed_brick',\n",
       " u'with_fireplace',\n",
       " u'with_fitness_center',\n",
       " u'with_full_time_doorman',\n",
       " u'with_furnished',\n",
       " u'with_garage',\n",
       " u'with_garden',\n",
       " u'with_garden/patio',\n",
       " u'with_granite_kitchen',\n",
       " u'with_green_building',\n",
       " u'with_gym',\n",
       " u'with_gym/fitness',\n",
       " u'with_hardwood',\n",
       " u'with_hardwood_floors',\n",
       " u'with_high_ceiling',\n",
       " u'with_high_ceilings',\n",
       " u'with_high_speed_internet',\n",
       " u'with_highrise',\n",
       " u'with_laundry',\n",
       " u'with_laundry_in_building',\n",
       " u'with_laundry_in_unit',\n",
       " u'with_laundry_room',\n",
       " u'with_light',\n",
       " u'with_live_in_super',\n",
       " u'with_live_in_superintendent',\n",
       " u'with_loft',\n",
       " u'with_lounge',\n",
       " u'with_lowrise',\n",
       " u'with_luxury_building',\n",
       " u'with_marble_bath',\n",
       " u'with_multi_level',\n",
       " u'with_new_construction',\n",
       " u'with_newly_renovated',\n",
       " u'with_no_fee',\n",
       " u'with_no_pets',\n",
       " u'with_on_site_garage',\n",
       " u'with_on_site_laundry',\n",
       " u'with_outdoor_areas',\n",
       " u'with_outdoor_space',\n",
       " u'with_parking',\n",
       " u'with_parking_space',\n",
       " u'with_patio',\n",
       " u'with_pets_on_approval',\n",
       " u'with_pool',\n",
       " u'with_post_war',\n",
       " u'with_pre_war',\n",
       " u'with_prewar',\n",
       " u'with_private_balcony',\n",
       " u'with_private_outdoor_space',\n",
       " u'with_publicoutdoor',\n",
       " u'with_reduced_fee',\n",
       " u'with_renovated',\n",
       " u'with_residents_garden',\n",
       " u'with_residents_lounge',\n",
       " u'with_roof_deck',\n",
       " u'with_roofdeck',\n",
       " u'with_short_term_allowed',\n",
       " u'with_simplex',\n",
       " u'with_stainless_steel_appliances',\n",
       " u'with_storage',\n",
       " u'with_subway',\n",
       " u'with_swimming_pool',\n",
       " u'with_terrace',\n",
       " u'with_valet',\n",
       " u'with_view',\n",
       " u'with_walk_in_closet(s)',\n",
       " u'with_washer/dryer',\n",
       " u'with_washer_in_unit',\n",
       " u'with_wheelchair_access',\n",
       " u'with_wifi_access',\n",
       " u'hcc_building_id_high',\n",
       " u'hcc_building_id_medium',\n",
       " u'building_id_nrank_s_r',\n",
       " u'building_id_perf_s_r',\n",
       " u'hcc_cluster_id_10_high',\n",
       " u'hcc_cluster_id_10_medium',\n",
       " u'cluster_id_10_nrank_s_r',\n",
       " u'cluster_id_10_perf_s_r',\n",
       " u'hcc_cluster_id_30_high',\n",
       " u'hcc_cluster_id_30_medium',\n",
       " u'cluster_id_30_nrank_s_r',\n",
       " u'cluster_id_30_perf_s_r',\n",
       " u'hcc_street_name_high',\n",
       " u'hcc_street_name_medium',\n",
       " u'street_name_nrank_s_r',\n",
       " u'street_name_perf_s_r']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-b415183905cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m#random forest us\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mkn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)\n",
    "\n",
    "#run the logistic algorithm to do \n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[weightFeatures].as_matrix(), val_set[weightFeatures].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = ()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "    models.append(lr)   \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    #print(cv_scores)\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coff = np.abs(models[0].coef_) \n",
    "for i in [1,2,3,4]:\n",
    "    coff+=np.abs(models[i].coef_) \n",
    "coeff = coff.sum(axis=0)\n",
    "\n",
    "total_weight={}\n",
    "total_weight_list=[]\n",
    "for i in range(len(weightFeatures)):\n",
    "    total_weight[weightFeatures[i]]=coeff[i]\n",
    "    total_weight_list.append((weightFeatures[i],coeff[i]))\n",
    "#total_weight_list=sorted(total_weight_list,key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trying knn using the weight from the above regression\n",
    "knn_train_df = train_df.copy()\n",
    "knn_test_df = test_df.copy()\n",
    "\n",
    "for feature_value in total_weight_list:\n",
    "    knn_train_df[feature_value[0]]*=np.sqrt(feature_value[1])\n",
    "    knn_test_df[feature_value[0]]*=np.sqrt(feature_value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = \"/home/raku/kaggleData/2sigma/knn4/\"\n",
    "knn_train_df.to_json('knn-train.json')\n",
    "knn_test_df.to_json('knn-test.json')\n",
    "\n",
    "\n",
    "pickl_file = store+'knn-weigthed-features.pickle'\n",
    "fileObject = open(pickl_file,'wb') \n",
    "pickle.dump(weightFeatures,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9333649797334811]\n",
      "2.93336497973\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "#features=list(processing_features)+with_feat\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = knn_train_df.iloc[dev_index,:] , knn_train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = KN()\n",
    "    kn.fit(dev_X,dev_y)\n",
    "    preds = kn.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71178198764056322"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(kn.predict(val_X),val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bath_per_bed',\n",
       " u'bathrooms',\n",
       " u'bedrooms',\n",
       " u'building0',\n",
       " u'cluster_id_10_d',\n",
       " u'cluster_id_30_d',\n",
       " u'dayofyear',\n",
       " u'latitude',\n",
       " u'listing_id',\n",
       " u'longitude',\n",
       " u'm_c_distance',\n",
       " u'm_m_distance',\n",
       " u'manager_id_nrank',\n",
       " u'manager_id_perf',\n",
       " u'mlat',\n",
       " u'mlon',\n",
       " u'num_description_words',\n",
       " u'num_features',\n",
       " u'num_photos',\n",
       " u'price',\n",
       " u'price_per_bath',\n",
       " u'price_per_bed',\n",
       " u'price_per_room',\n",
       " u'created_day',\n",
       " u'created_hour',\n",
       " u'created_month',\n",
       " u'time_stamp',\n",
       " 'hcc_manager_id_high',\n",
       " 'hcc_manager_id_medium',\n",
       " 'hcc_house_type_high',\n",
       " 'hcc_house_type_medium',\n",
       " 'pic_month',\n",
       " 'pic_day',\n",
       " 'pic_hour',\n",
       " u'with_actual_apt._photos',\n",
       " u'with_balcony',\n",
       " u'with_bike_room',\n",
       " u'with_cats_allowed',\n",
       " u'with_central_a/c',\n",
       " u'with_childrens_playroom',\n",
       " u'with_common_outdoor_space',\n",
       " u'with_common_parking/garage',\n",
       " u'with_common_roof_deck',\n",
       " u'with_concierge',\n",
       " u'with_dining_room',\n",
       " u'with_dishwasher',\n",
       " u'with_dogs_allowed',\n",
       " u'with_doorman',\n",
       " u'with_dryer_in_unit',\n",
       " u'with_duplex',\n",
       " u'with_eat_in_kitchen',\n",
       " u'with_elevator',\n",
       " u'with_exclusive',\n",
       " u'with_exposed_brick',\n",
       " u'with_fireplace',\n",
       " u'with_fitness_center',\n",
       " u'with_full_time_doorman',\n",
       " u'with_furnished',\n",
       " u'with_garage',\n",
       " u'with_garden',\n",
       " u'with_garden/patio',\n",
       " u'with_granite_kitchen',\n",
       " u'with_green_building',\n",
       " u'with_gym',\n",
       " u'with_gym/fitness',\n",
       " u'with_hardwood',\n",
       " u'with_hardwood_floors',\n",
       " u'with_high_ceiling',\n",
       " u'with_high_ceilings',\n",
       " u'with_high_speed_internet',\n",
       " u'with_highrise',\n",
       " u'with_laundry',\n",
       " u'with_laundry_in_building',\n",
       " u'with_laundry_in_unit',\n",
       " u'with_laundry_room',\n",
       " u'with_light',\n",
       " u'with_live_in_super',\n",
       " u'with_live_in_superintendent',\n",
       " u'with_loft',\n",
       " u'with_lounge',\n",
       " u'with_lowrise',\n",
       " u'with_luxury_building',\n",
       " u'with_marble_bath',\n",
       " u'with_multi_level',\n",
       " u'with_new_construction',\n",
       " u'with_newly_renovated',\n",
       " u'with_no_fee',\n",
       " u'with_no_pets',\n",
       " u'with_on_site_garage',\n",
       " u'with_on_site_laundry',\n",
       " u'with_outdoor_areas',\n",
       " u'with_outdoor_space',\n",
       " u'with_parking',\n",
       " u'with_parking_space',\n",
       " u'with_patio',\n",
       " u'with_pets_on_approval',\n",
       " u'with_pool',\n",
       " u'with_post_war',\n",
       " u'with_pre_war',\n",
       " u'with_prewar',\n",
       " u'with_private_balcony',\n",
       " u'with_private_outdoor_space',\n",
       " u'with_publicoutdoor',\n",
       " u'with_reduced_fee',\n",
       " u'with_renovated',\n",
       " u'with_residents_garden',\n",
       " u'with_residents_lounge',\n",
       " u'with_roof_deck',\n",
       " u'with_roofdeck',\n",
       " u'with_short_term_allowed',\n",
       " u'with_simplex',\n",
       " u'with_stainless_steel_appliances',\n",
       " u'with_storage',\n",
       " u'with_subway',\n",
       " u'with_swimming_pool',\n",
       " u'with_terrace',\n",
       " u'with_valet',\n",
       " u'with_view',\n",
       " u'with_walk_in_closet(s)',\n",
       " u'with_washer/dryer',\n",
       " u'with_washer_in_unit',\n",
       " u'with_wheelchair_access',\n",
       " u'with_wifi_access',\n",
       " u'hcc_building_id_high',\n",
       " u'hcc_building_id_medium',\n",
       " u'building_id_nrank_s_r',\n",
       " u'building_id_perf_s_r',\n",
       " u'hcc_cluster_id_10_high',\n",
       " u'hcc_cluster_id_10_medium',\n",
       " u'cluster_id_10_nrank_s_r',\n",
       " u'cluster_id_10_perf_s_r',\n",
       " u'hcc_cluster_id_30_high',\n",
       " u'hcc_cluster_id_30_medium',\n",
       " u'cluster_id_30_nrank_s_r',\n",
       " u'cluster_id_30_perf_s_r',\n",
       " u'hcc_street_name_high',\n",
       " u'hcc_street_name_medium',\n",
       " u'street_name_nrank_s_r',\n",
       " u'street_name_perf_s_r']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_features = ['bathrooms','bedrooms', 'latitude', 'longitude','listing_id','price','price_per_bed','price_per_room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72271469576\n"
     ]
    }
   ],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)\n",
    "\n",
    "#run the logistic algorithm to do \n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = ()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "    models.append(lr)   \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    #print(cv_scores)\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coff = np.abs(models[0].coef_) \n",
    "for i in [1,2,3,4]:\n",
    "    coff+=np.abs(models[i].coef_) \n",
    "coeff = coff.sum(axis=0)\n",
    "\n",
    "total_weight={}\n",
    "total_weight_list=[]\n",
    "for i in range(len(basic_features)):\n",
    "    total_weight[basic_features[i]]=coeff[i]\n",
    "    total_weight_list.append((basic_features[i],coeff[i]))\n",
    "#total_weight_list=sorted(total_weight_list,key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trying knn using the weight from the above regression\n",
    "knn_train_df = train_df.copy()\n",
    "knn_test_df = test_df.copy()\n",
    "\n",
    "for feature_value in total_weight_list:\n",
    "    knn_train_df[feature_value[0]]*=np.sqrt(feature_value[1])\n",
    "    knn_test_df[feature_value[0]]*=np.sqrt(feature_value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71569973846561874, 0.7256302364557482, 0.70545173366351122, 0.73056992785278352, 0.73622184236144717, 3.5521508239977964]\n",
      "1.1942873838\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "#features=list(processing_features)+with_feat\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = knn_train_df.iloc[dev_index,:] , knn_train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = KN()\n",
    "    kn.fit(dev_X,dev_y)\n",
    "    preds = kn.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64745213250937084"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(kn.predict(val_X),val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63070601339\n"
     ]
    }
   ],
   "source": [
    "basic_features.extend(['manager_id_perf','time_stamp'])\n",
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)\n",
    "\n",
    "#run the logistic algorithm to do \n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "    models.append(lr)   \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    #print(cv_scores)\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coff = np.abs(models[0].coef_) \n",
    "for i in [1,2,3,4]:\n",
    "    coff+=np.abs(models[i].coef_) \n",
    "coeff = coff.sum(axis=0)\n",
    "\n",
    "total_weight={}\n",
    "total_weight_list=[]\n",
    "for i in range(len(basic_features)):\n",
    "    total_weight[basic_features[i]]=coeff[i]\n",
    "    total_weight_list.append((basic_features[i],coeff[i]))\n",
    "#total_weight_list=sorted(total_weight_list,key=lambda x:x[1],reverse=True)\n",
    "\n",
    "#trying knn using the weight from the above regression\n",
    "knn_train_df = train_df.copy()\n",
    "knn_test_df = test_df.copy()\n",
    "\n",
    "for feature_value in total_weight_list:\n",
    "    knn_train_df[feature_value[0]]*=np.sqrt(feature_value[1])\n",
    "    knn_test_df[feature_value[0]]*=np.sqrt(feature_value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691621922804\n",
      "0.691621922804\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "#features=list(processing_features)+with_feat\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = knn_train_df.iloc[dev_index,:] , knn_train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = KN()\n",
    "    kn.fit(dev_X,dev_y)\n",
    "    #preds = kn.predict_proba(val_X)\n",
    "    accr=accuracy_score(kn.predict(val_X),val_y)   \n",
    "    #cv_scores.append(log_loss(val_y, preds))\n",
    "    cv_scores.append(accr)  \n",
    "\n",
    "    print accr\n",
    "    break\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619298970114\n"
     ]
    }
   ],
   "source": [
    "basic_features.extend(['hcc_manager_id_high','hcc_manager_id_medium'])\n",
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)\n",
    "\n",
    "#run the logistic algorithm to do \n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "    models.append(lr)   \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    #print(cv_scores)\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coff = np.abs(models[0].coef_) \n",
    "for i in [1,2,3,4]:\n",
    "    coff+=np.abs(models[i].coef_) \n",
    "coeff = coff.sum(axis=0)\n",
    "\n",
    "total_weight={}\n",
    "total_weight_list=[]\n",
    "for i in range(len(basic_features)):\n",
    "    total_weight[basic_features[i]]=coeff[i]\n",
    "    total_weight_list.append((basic_features[i],coeff[i]))\n",
    "#total_weight_list=sorted(total_weight_list,key=lambda x:x[1],reverse=True)\n",
    "\n",
    "#trying knn using the weight from the above regression\n",
    "knn_train_df = train_df.copy()\n",
    "knn_test_df = test_df.copy()\n",
    "\n",
    "for feature_value in total_weight_list:\n",
    "    knn_train_df[feature_value[0]]*=np.sqrt(feature_value[1])\n",
    "    knn_test_df[feature_value[0]]*=np.sqrt(feature_value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.690507547361\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "#features=list(processing_features)+with_feat\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = knn_train_df.iloc[dev_index,:] , knn_train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = KN()\n",
    "    kn.fit(dev_X,dev_y)\n",
    "    #preds = kn.predict_proba(val_X)\n",
    "    accr=accuracy_score(kn.predict(val_X),val_y)   \n",
    "    #cv_scores.append(log_loss(val_y, preds))\n",
    "    cv_scores.append(accr)  \n",
    "\n",
    "    #print accr\n",
    "    break\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_features.remove('manager_id_perf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619298352856\n"
     ]
    }
   ],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)\n",
    "\n",
    "#run the logistic algorithm to do \n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "    models.append(lr)   \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    #print(cv_scores)\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coff = np.abs(models[0].coef_) \n",
    "for i in [1,2,3,4]:\n",
    "    coff+=np.abs(models[i].coef_) \n",
    "coeff = coff.sum(axis=0)\n",
    "\n",
    "total_weight={}\n",
    "total_weight_list=[]\n",
    "for i in range(len(basic_features)):\n",
    "    total_weight[basic_features[i]]=coeff[i]\n",
    "    total_weight_list.append((basic_features[i],coeff[i]))\n",
    "#total_weight_list=sorted(total_weight_list,key=lambda x:x[1],reverse=True)\n",
    "\n",
    "#trying knn using the weight from the above regression\n",
    "knn_train_df = train_df.copy()\n",
    "knn_test_df = test_df.copy()\n",
    "\n",
    "for feature_value in total_weight_list:\n",
    "    knn_train_df[feature_value[0]]*=np.sqrt(feature_value[1])\n",
    "    knn_test_df[feature_value[0]]*=np.sqrt(feature_value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692128457097\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "#features=list(processing_features)+with_feat\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = knn_train_df.iloc[dev_index,:] , knn_train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = KN()\n",
    "    kn.fit(dev_X,dev_y)\n",
    "    #preds = kn.predict_proba(val_X)\n",
    "    accr=accuracy_score(kn.predict(val_X),val_y)   \n",
    "    #cv_scores.append(log_loss(val_y, preds))\n",
    "    cv_scores.append(accr)  \n",
    "\n",
    "    #print accr\n",
    "    break\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_features.extend([u'created_day',\n",
    " u'created_hour',\n",
    " u'created_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616317075081\n"
     ]
    }
   ],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)\n",
    "\n",
    "#run the logistic algorithm to do \n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "    models.append(lr)   \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    #print(cv_scores)\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coff = np.abs(models[0].coef_) \n",
    "for i in [1,2,3,4]:\n",
    "    coff+=np.abs(models[i].coef_) \n",
    "coeff = coff.sum(axis=0)\n",
    "\n",
    "total_weight={}\n",
    "total_weight_list=[]\n",
    "for i in range(len(basic_features)):\n",
    "    total_weight[basic_features[i]]=coeff[i]\n",
    "    total_weight_list.append((basic_features[i],coeff[i]))\n",
    "#total_weight_list=sorted(total_weight_list,key=lambda x:x[1],reverse=True)\n",
    "\n",
    "#trying knn using the weight from the above regression\n",
    "knn_train_df = train_df.copy()\n",
    "knn_test_df = test_df.copy()\n",
    "\n",
    "for feature_value in total_weight_list:\n",
    "    knn_train_df[feature_value[0]]*=np.sqrt(feature_value[1])\n",
    "    knn_test_df[feature_value[0]]*=np.sqrt(feature_value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696585958869\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "#features=list(processing_features)+with_feat\n",
    "cv_scores=[]\n",
    "models =[]\n",
    "\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = knn_train_df.iloc[dev_index,:] , knn_train_df.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[basic_features].as_matrix(), val_set[basic_features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    kn = KN()\n",
    "    kn.fit(dev_X,dev_y)\n",
    "    #preds = kn.predict_proba(val_X)\n",
    "    accr=accuracy_score(kn.predict(val_X),val_y)   \n",
    "    #cv_scores.append(log_loss(val_y, preds))\n",
    "    cv_scores.append(accr)  \n",
    "\n",
    "    #print accr\n",
    "    break\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "store = \"/home/raku/kaggleData/2sigma/knn4/\"\n",
    "knn_train_df.to_json('knn-train.json')\n",
    "knn_test_df.to_json('knn-test.json')\n",
    "\n",
    "\n",
    "pickl_file = store+'knn-weigthed-features.pickle'\n",
    "fileObject = open(pickl_file,'wb') \n",
    "pickle.dump(basic_features,fileObject)   \n",
    "fileObject.close()\n",
    "print len(basic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weightFeatures)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
