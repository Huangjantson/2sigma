{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import  preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from mochi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try xgboost\n",
    "#original fucntion from SRK\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, \\\n",
    "     seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1,\\\n",
    "     max_depth = 6,cv_dict = None,verbose_eval=True):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = eta\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.3\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,\\\n",
    "        early_stopping_rounds=early_stop,evals_result = cv_dict,verbose_eval = verbose_eval)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"../../kaggleData/2sigma/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic numerical features\n",
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some transfromed features\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "#some new numerical features related to the price\n",
    "train_df[\"price_per_bath\"] =  (train_df[\"price\"]*1.0/train_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_bed\"] = (train_df[\"price\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"bath_per_bed\"] = (train_df[\"bathrooms\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_room\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"]+train_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "test_df[\"price_per_bath\"] =  (test_df[\"price\"]*1.0/test_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_bed\"] = (test_df[\"price\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"bath_per_bed\"] = (test_df[\"bathrooms\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_room\"] = (test_df[\"price\"]*1.0/(test_df[\"bedrooms\"]+test_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "\n",
    "# adding all these new features to use list # \"listing_id\",\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\\\n",
    "                        \"created_year\",\"listing_id\", \"created_month\", \"created_day\", \"created_hour\"])\n",
    "#price new features\n",
    "features_to_use.extend([\"price_per_bed\",\"bath_per_bed\",\"price_per_room\"])\n",
    "\n",
    "#for latter use\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processMap(train_df)\n",
    "processMap(test_df)\n",
    "\n",
    "#adding the house type\n",
    "train_df['house_type']=map(lambda x,y:(x,y),train_df['bedrooms'],train_df['bathrooms'])\n",
    "train_df['house_type'] = train_df['house_type'].apply(str)\n",
    "#filling outliers with nan\n",
    "test_df['house_type']=map(lambda x,y:(x,y),test_df['bedrooms'],test_df['bathrooms'])\n",
    "test_df['house_type'] = test_df['house_type'].apply(str)\n",
    "\n",
    "\"\"\"\n",
    "new categorical data generated from the old ones\n",
    "\"\"\"\n",
    "#new feature for the street_address, use them instead of the original one\n",
    "train_df[\"street_name\"] = train_df[\"street_address\"].apply(proecessStreet)\n",
    "test_df[\"street_name\"] = test_df[\"street_address\"].apply(proecessStreet)\n",
    "\n",
    "train_df['building0']=map(lambda x:1 if x== '0' else 0,train_df['building_id'])\n",
    "test_df['building0']=map(lambda x:1 if x== '0' else 0,test_df['building_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dealing with features\n",
    "\n",
    "#preprocessing for features\n",
    "train_df[\"features\"] = train_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_') \\\n",
    "                                                            for i in x])\n",
    "test_df[\"features\"] = test_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_')\\\n",
    "                                                          for i in x])\n",
    "#create the accept list\n",
    "accept_list = list(featureList(train_df,test_df,limit = 0.001))\n",
    "\n",
    "#map the feature to dummy slots\n",
    "featureMapping(train_df,test_df,accept_list)\n",
    "features_to_use.extend(map(lambda x : 'with_'+x,accept_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the basic features from preprocessing \n",
    "features = list(features_to_use)\n",
    "\n",
    "#features to be added during cv by cv-manner statistics\n",
    "features.extend(['manager_id_perf','house_type_perf'])\n",
    "features.extend(['m3perf','m7perf','m14perf','m30perf'])\n",
    "features.extend(['manager_id_nrank'])\n",
    "\n",
    "\n",
    "#categorical features to be added\n",
    "categorical = [\"display_address\", \"street_address\",\"street_name\",'building_id',\\\n",
    "'manager_id','building0','house_type']\n",
    "features.extend(categorical)\n",
    "features.extend(['cluster_id_10','cluster_id_30'])\n",
    "\n",
    "#statistical features\n",
    "features.extend(['m_m_distance','mlon','mlat'])\n",
    "\n",
    "main_st_nf = [\"bathrooms\", \"bedrooms\",\"price_per_bed\",\"bath_per_bed\",\\\n",
    "\"price_per_room\",\"num_photos\", \"num_features\", \"num_description_words\",'price']\n",
    "main_statistics =['mean','max','min','median']\n",
    "\n",
    "for st in main_statistics:\n",
    "    features.extend(map(lambda x : 'manager_id_'+x+'_'+st,main_st_nf))\n",
    "    features.extend(map(lambda x : 'house_type_'+x+'_'+st,main_st_nf)) \n",
    "\n",
    "features.extend(map(lambda x : 'cluster_id_10_'+x+'_'+'mean',main_st_nf))\n",
    "features.extend(map(lambda x : 'cluster_id_30_'+x+'_'+'mean',main_st_nf))\n",
    "\n",
    "price_related = ['price_per_bed','price_per_room','price']\n",
    "\n",
    "features.extend(['manager_id_size','house_type_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    #=============================================================        \n",
    "    \"\"\"feature engineerings for the categorical features\"\"\"\n",
    "    #fill substitute the small size values by their mean\n",
    "    for f in categorical:    \n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f])+list(test_df[f]))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "    \n",
    "    for f in categorical:\n",
    "        train_df,test_df  = singleValueConvert(train_df,test_df,f,1)\n",
    "    \n",
    "    #kmeans grouping\n",
    "    getCluster(train_df,test_df,30)\n",
    "    getCluster(train_df,test_df,10)\n",
    "    \n",
    "    #K-FOLD evaluation for the statistic features\n",
    "    skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "    #dev set adding manager skill\n",
    "    for train,test in skf:\n",
    "            performance_eval(train_df.iloc[train,:],train_df.iloc[test,:],feature='manager_id',k=5,g=10,\n",
    "                           update_df = train_df,smoothing=False)\n",
    "            performance_eval(train_df.iloc[train,:],train_df.iloc[test,:],feature='house_type',k=5,g=10,\n",
    "                           update_df = train_df,smoothing=False)\n",
    "            temporalManagerPerf(train_df.iloc[train,:],train_df.iloc[test,:],update_df = train_df)\n",
    "\n",
    "    performance_eval(train_df,test_df,feature='manager_id',k=5,g=10,smoothing=False)\n",
    "    performance_eval(train_df,test_df,feature='house_type',k=5,g=10,smoothing=False)\n",
    "    temporalManagerPerf(train_df,test_df)\n",
    "        \n",
    "        \n",
    "    #statitstic\n",
    "    for f in main_st_nf:\n",
    "        #print f\n",
    "        categorical_statistics(train_df,test_df,'manager_id',f)\n",
    "        categorical_statistics(train_df,test_df,'cluster_id_10',f)\n",
    "        categorical_statistics(train_df,test_df,'cluster_id_30',f)\n",
    "        categorical_statistics(train_df,test_df,'house_type',f)\n",
    "        categorical_size(train_df,test_df,'manager_id')\n",
    "        categorical_size(train_df,test_df,'house_type')\n",
    "    \n",
    "    #for f in price_related:\n",
    "    #    rank_on_categorical(train_df,test_df,'house_type_30',f,random =None)\n",
    "\n",
    "    \n",
    "    #manager main location\n",
    "    manager_lon_lat(train_df,test_df)\n",
    "    \n",
    "    for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f])+list(test_df[f]))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"output the processed dataframes\"\"\"\n",
    "#shorten reprocessing time: save the preprocessed train_df and test_df with some basic features\n",
    "train_df.to_json(data_path+'processed_train_df.json')\n",
    "test_df.to_json(data_path+'processed_test_df.json')\n",
    "#print features_to_use\n",
    "#shorten reprocessing time: load the preprocessed train_df and test_df with some basic features\n",
    "\n",
    "#train_df=pd.read_json(data_path+'processed_train_df.json')\n",
    "#test_df=pd.read_json(data_path+'processed_test_df.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nxgb14featureFile = 'xgb145.feat'\\nfileObject = open(xgb14featureFile,'r')\\nfeatures = pickle.load(fileObject)\\nfileObject.close()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output the feature list\n",
    "\n",
    "xgb14featureFile = 'xgb145.feat'\n",
    "fileObject = open(xgb14featureFile,'wb') \n",
    "pickle.dump(features,fileObject)   \n",
    "fileObject.close()\n",
    "\"\"\"\n",
    "xgb14featureFile = 'xgb145.feat'\n",
    "fileObject = open(xgb14featureFile,'r')\n",
    "features = pickle.load(fileObject)\n",
    "fileObject.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare for validation\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=StratifiedKFold(train_y,5,shuffle=True,random_state = 42)\n",
    "\n",
    "train_df = train_df.fillna(-1)\n",
    "#test_df = test_df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the output\n",
    "train_X, test_X = train_df[features].as_matrix(), test_df[features].as_matrix()\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X,\\\n",
    "feature_names=features,\n",
    "num_rounds = 3500, eta = 0.02,max_depth = 4,verbose_eval=100)\n",
    "\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_beta1point45-0.02-leakage.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_levels = ['low', 'medium', 'high']\n",
    "\n",
    "tau = {\n",
    "    'low': 0.69195995, \n",
    "    'medium': 0.23108864,\n",
    "    'high': 0.07695141, \n",
    "}\n",
    "\n",
    "def correct(df):\n",
    "    y = df[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print a\n",
    "\n",
    "    def f(p):\n",
    "        for k in range(len(interest_levels)):\n",
    "            p[k] *= a[k]\n",
    "        return p / p.sum()\n",
    "\n",
    "    df_correct = df.copy()\n",
    "    df_correct[interest_levels] = df_correct[interest_levels].apply(f, axis=1)\n",
    "\n",
    "    y = df_correct[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print a\n",
    "\n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_df = correct(out_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df.to_csv('xgb_beta1point45-0.02-correct.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the model\n",
    "xgbmodel = 'xgb145.model'\n",
    "modelFileObject = open(xgbmodel,'wb') \n",
    "pickle.dump(model,modelFileObject)   \n",
    "modelFileObject.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
