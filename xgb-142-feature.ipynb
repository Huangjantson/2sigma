{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import  preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mochi import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try xgboost\n",
    "#original fucntion from SRK\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None,      seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1,     max_depth = 6,cv_dict = None,verbose_eval=True):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = eta\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.3\n",
    "    param['seed'] = seed_val\n",
    "    param['nthread'] = 2\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,        early_stopping_rounds=early_stop,evals_result = cv_dict,verbose_eval = verbose_eval)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 16)\n",
      "(74659, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npic_file = data_path + \"listing_image_time.csv\"\\npic_df = pd.read_csv(pic_file).set_index(\\'Listing_Id\\')\\n\\ntrain_df=train_df.join(pic_df,on=\\'listing_id\\',how=\\'left\\')\\ntest_df=test_df.join(pic_df,on=\\'listing_id\\',how=\\'left\\')\\n\\ntrain_df.to_json(data_path + \"train_t.json\")\\ntest_df.to_json(data_path + \"test_t.json\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"/home/raku/kaggleData/2sigma/\"\n",
    "train_file = data_path + \"train_t.json\"\n",
    "test_file = data_path + \"test_t.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\"\"\"\n",
    "pic_file = data_path + \"listing_image_time.csv\"\n",
    "pic_df = pd.read_csv(pic_file).set_index('Listing_Id')\n",
    "\n",
    "train_df=train_df.join(pic_df,on='listing_id',how='left')\n",
    "test_df=test_df.join(pic_df,on='listing_id',how='left')\n",
    "\n",
    "train_df.to_json(data_path + \"train_t.json\")\n",
    "test_df.to_json(data_path + \"test_t.json\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",'time_stamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "\n",
    "#some transfromed features\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "#some new numerical features related to the price\n",
    "train_df[\"price_per_bath\"] =  (train_df[\"price\"]*1.0/train_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_bed\"] = (train_df[\"price\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"bath_per_bed\"] = (train_df[\"bathrooms\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_room\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"]+train_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "test_df[\"price_per_bath\"] =  (test_df[\"price\"]*1.0/test_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_bed\"] = (test_df[\"price\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"bath_per_bed\"] = (test_df[\"bathrooms\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "test_df[\"price_per_room\"] = (test_df[\"price\"]*1.0/(test_df[\"bedrooms\"]+test_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "\n",
    "# adding all these new features to use list # \"listing_id\",\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",                        \"created_year\",\"listing_id\", \"created_month\", \"created_day\", \"created_hour\"])\n",
    "#price new features\n",
    "features_to_use.extend([\"price_per_bed\",\"bath_per_bed\",\"price_per_room\"])\n",
    "\n",
    "#for latter use\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "\n",
    "train_df['house_type']=map(lambda x,y:(x,y),train_df['bedrooms'],train_df['bathrooms'])\n",
    "train_df['house_type'] = train_df['house_type'].apply(str)\n",
    "test_df['house_type']=map(lambda x,y:(x,y),test_df['bedrooms'],test_df['bathrooms'])\n",
    "test_df['house_type'] = test_df['house_type'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "new categorical data generated from the old ones\n",
    "\"\"\"\n",
    "#new feature for the street_address, use them instead of the original one\n",
    "train_df[\"street_name\"] = train_df[\"street_address\"].apply(proecessStreet)\n",
    "test_df[\"street_name\"] = test_df[\"street_address\"].apply(proecessStreet)\n",
    "\n",
    "train_df['building0']=map(lambda x:1 if x== '0' else 0,train_df['building_id'])\n",
    "test_df['building0']=map(lambda x:1 if x== '0' else 0,test_df['building_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "#dealing with features\n",
    "\n",
    "#preprocessing for features\n",
    "train_df[\"features\"] = train_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_')                                                             for i in x])\n",
    "test_df[\"features\"] = test_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_')                                                         for i in x])\n",
    "#create the accept list\n",
    "accept_list = list(featureList(train_df,test_df,limit = 0.001))\n",
    "\n",
    "#map the feature to dummy slots\n",
    "featureMapping(train_df,test_df,accept_list)\n",
    "features_to_use.extend(map(lambda x : 'with_'+x,accept_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "\n",
    "#the basic features from preprocessing \n",
    "features = list(set(features_to_use))\n",
    "\n",
    "#features to be added during cv by cv-manner statistics\n",
    "features.extend(['manager_id_perf'])\n",
    "features.extend(['m3perf','m7perf','m14perf','m30perf'])\n",
    "features.extend(['m3perf_f','m7perf_f','m14perf_f','m30perf_f'])\n",
    "features.extend(['manager_id_nrank'])\n",
    "\n",
    "\n",
    "#categorical features to be added\n",
    "categorical = [\"display_address\", \"street_address\",\"street_name\",'building_id','manager_id','building0','house_type']\n",
    "features.extend(categorical)\n",
    "features.extend(['cluster_id_10','cluster_id_30'])\n",
    "\n",
    "\n",
    "#statistical features\n",
    "features.extend(['m_m_distance','mlon','mlat'])\n",
    "\n",
    "main_st_nf = [\"bathrooms\", \"bedrooms\",\"price_per_bed\",\"bath_per_bed\",\"price_per_room\",\"num_photos\", \"num_features\", \"num_description_words\",'price']\n",
    "main_statistics =['mean','max','min','median']\n",
    "\n",
    "for st in main_statistics:\n",
    "    features.extend(map(lambda x : 'manager_id_'+x+'_'+st,main_st_nf))\n",
    "    features.extend(map(lambda x : 'house_type_'+x+'_'+st,main_st_nf)) \n",
    "\n",
    "features.extend(map(lambda x : 'cluster_id_10_'+x+'_'+'mean',main_st_nf))\n",
    "features.extend(map(lambda x : 'cluster_id_30_'+x+'_'+'mean',main_st_nf))\n",
    "\n",
    "price_related = ['price_per_bed','price_per_room','price']\n",
    "\n",
    "features.extend(['manager_id_size','house_type_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=list(set(features))\n",
    "\n",
    "processMap(train_df)\n",
    "processMap(test_df)\n",
    "train_df = train_df.fillna(-1)\n",
    "test_df=test_df.fillna(-1)\n",
    "getCluster(train_df,test_df,30)\n",
    "getCluster(train_df,test_df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#K-FOLD evaluation for the statistic features\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#dev set adding manager skill\n",
    "for train,test in skf:\n",
    "        performance_eval(train_df.iloc[train,:],train_df.iloc[test,:],feature='manager_id',\n",
    "                       update_df = train_df,smoothing=False)\n",
    "        temporalManagerPerf_f(train_df.iloc[train,:],train_df.iloc[test,:],update_df = train_df)\n",
    "                \n",
    "performance_eval(train_df,test_df,feature='manager_id',smoothing=False)\n",
    "temporalManagerPerf_f(train_df,test_df)\n",
    "    \n",
    "    \n",
    "#statitstic\n",
    "for f in main_st_nf:\n",
    "    #print f\n",
    "    categorical_statistics(train_df,test_df,'manager_id',f)\n",
    "    categorical_statistics(train_df,test_df,'cluster_id_10',f)\n",
    "    categorical_statistics(train_df,test_df,'cluster_id_30',f)\n",
    "    categorical_statistics(train_df,test_df,'house_type',f)\n",
    "    categorical_size(train_df,test_df,'manager_id')\n",
    "    categorical_size(train_df,test_df,'house_type')\n",
    "\n",
    "\n",
    "#manager main location\n",
    "manager_lon_lat(train_df,test_df)\n",
    "\n",
    "for f in categorical:\n",
    "\n",
    "    if train_df[f].dtype=='object':\n",
    "        #print(f)\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train_df[f])+list(test_df[f]))\n",
    "        train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "        test_df[f] = lbl.transform(list(test_df[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(-1)\n",
    "test_df = test_df.fillna(-1)\n",
    "\n",
    "store = '/home/raku/kaggleData/2sigma/xgb142/'\n",
    "\n",
    "train_df.to_json(store+'xgb1.42-train.json')\n",
    "test_df.to_json(store+'xgb1.42-test.json')\n",
    "\n",
    "pickl_file = store+'xgb142features.pickle'\n",
    "fileObject = open(pickl_file,'wb') \n",
    "pickle.dump(features,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.03691\ttest-mlogloss:1.03782\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 64 rounds.\n",
      "[100]\ttrain-mlogloss:0.51431\ttest-mlogloss:0.550616\n",
      "[200]\ttrain-mlogloss:0.470358\ttest-mlogloss:0.530696\n",
      "[300]\ttrain-mlogloss:0.440814\ttest-mlogloss:0.524611\n",
      "[400]\ttrain-mlogloss:0.41679\ttest-mlogloss:0.521665\n",
      "[500]\ttrain-mlogloss:0.396991\ttest-mlogloss:0.520391\n",
      "[600]\ttrain-mlogloss:0.378263\ttest-mlogloss:0.520061\n",
      "Stopping. Best iteration:\n",
      "[553]\ttrain-mlogloss:0.386797\ttest-mlogloss:0.51984\n",
      "\n",
      "loss for the turn 1 is 0.51988604158\n"
     ]
    }
   ],
   "source": [
    "# In[17]:\n",
    "\n",
    "#running and getting the cv from xgboost\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=StratifiedKFold(train_y,5,shuffle=True,random_state = 2333)\n",
    "\n",
    "cv_scores = []\n",
    "cv_result = []\n",
    "models = []\n",
    "\n",
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    result_dict = {}\n",
    "    \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    \"\"\"\n",
    "    #=============================================================        \n",
    "    #feature engineerings for the categorical features\n",
    "    #fill substitute the small size values by their mean\n",
    "    for f in ['display_address','manager_id','building_id','street_name']:\n",
    "        dev_set,val_set  = singleValueConvert(dev_set,val_set,f,1)\n",
    "    \n",
    "    #kmeans grouping\n",
    "    getCluster(dev_set,val_set,30)\n",
    "    getCluster(dev_set,val_set,10)\n",
    "    \n",
    "\n",
    "    #K-FOLD evaluation for the statistic features\n",
    "    skf=KFold(len(dev_set['interest_level']),5,shuffle=True,random_state = 42)\n",
    "    #dev set adding manager skill\n",
    "    for train,test in skf:\n",
    "            performance_eval(dev_set.iloc[train,:],dev_set.iloc[test,:],feature='manager_id',k=5,g=10,\n",
    "                           update_df = dev_set,smoothing=False)\n",
    "            temporalManagerPerf_f(dev_set.iloc[train,:],dev_set.iloc[test,:],update_df = dev_set)\n",
    "\n",
    "\n",
    "    performance_eval(dev_set,val_set,feature='manager_id',k=5,g=10,smoothing=False)\n",
    "    temporalManagerPerf_f(dev_set,val_set)\n",
    "        \n",
    "        \n",
    "    #statitstic\n",
    "    for f in main_st_nf:\n",
    "        #print f\n",
    "        categorical_statistics(dev_set,val_set,'manager_id',f)\n",
    "        categorical_statistics(dev_set,val_set,'cluster_id_10',f)\n",
    "        categorical_statistics(dev_set,val_set,'cluster_id_30',f)\n",
    "        categorical_statistics(dev_set,val_set,'house_type',f)\n",
    "        categorical_size(dev_set,val_set,'manager_id')\n",
    "        categorical_size(dev_set,val_set,'house_type')\n",
    "    \n",
    "    #for f in price_related:\n",
    "    #    rank_on_categorical(dev_set,val_set,'house_type_30',f,random =None)\n",
    "\n",
    "    \n",
    "    #manager main location\n",
    "    manager_lon_lat(dev_set,val_set)\n",
    "    \n",
    "    for f in categorical:\n",
    "    \n",
    "        if dev_set[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(dev_set[f])+list(val_set[f]))\n",
    "            dev_set[f] = lbl.transform(list(dev_set[f].values))\n",
    "            val_set[f] = lbl.transform(list(val_set[f].values))\n",
    "    \n",
    "    #============================================================\n",
    "    #dev_set.to_csv('having_view.csv',index=False,encoding  = 'utf-8')\n",
    "    \n",
    "    \"\"\"\n",
    "    #filter the features\n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    runXGB(dev_X, train_y, val_X, test_y=None, feature_names=None, \\\n",
    "    seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1, max_depth = 6)\n",
    "    \"\"\"        \n",
    "    \n",
    "    preds,model = runXGB(dev_X, dev_y, val_X, val_y,feature_names=features,\\\n",
    "           early_stop = 64,num_rounds=4500,eta = 0.1,max_depth=4,cv_dict = result_dict,verbose_eval=100)\n",
    "\n",
    "    loss = log_loss(val_y, preds)\n",
    "    \n",
    "    \"\"\"\n",
    "    #save the pickles for futures use\n",
    "    pickl_file = store+'xgb142-5fold-out'+str(i)+'.pickle'\n",
    "    fileObject = open(pickl_file,'wb') \n",
    "    pickle.dump(preds,fileObject)   \n",
    "    fileObject.close()\n",
    "    \"\"\"\n",
    "    \n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    models.append(model)\n",
    "    i+=1\n",
    "    print 'loss for the turn '+str(i)+' is '+str(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'The mean of the cv_scores is:'\n",
    "print np.mean(cv_scores)\n",
    "\n",
    "cvResult = CVstatistics(cv_result,'mlogloss')\n",
    "\n",
    "meanTestError = cvResult.result.filter(like='test').mean(axis=1)\n",
    "\n",
    "print meanTestError[meanTestError==np.min(meanTestError)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
