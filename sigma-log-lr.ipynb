{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mochi import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 293)\n",
      "(74659, 292)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"/home/raku/kaggleData/2sigma/xgb142/\"\n",
    "train_file = data_path + \"xgb1.42-train.json\"\n",
    "test_file = data_path + \"xgb1.42-test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericals = [u'bath_per_bed',u'bathrooms',u'bedrooms',u'building0',u'cluster_id_10_d',u'cluster_id_30_d',u'dayofyear',\n",
    " u'latitude',u'listing_id',u'longitude',u'm14perf',u'm14perf_f',u'm30perf',u'm30perf_f',u'm3perf',u'm3perf_f',\n",
    " u'm7perf',u'm7perf_f',u'm_c_distance',u'm_m_distance',u'manager_id_nrank',u'manager_id_perf',u'mlat',\n",
    " u'mlon',u'num_description_words',u'num_features', u'num_photos',\n",
    " u'price',u'price_per_bath',u'price_per_bed',u'price_per_room',]\n",
    "\n",
    "numerical_may_processed = [ u'created_day',u'created_hour',u'created_month',u'time_stamp'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_hcc_categoricals=[ u'building_id',u'cluster_id_10',u'cluster_id_30',u'street_name']\n",
    "hcc_categoricals = ['manager_id','house_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#statiscals\n",
    "statistical = []\n",
    "for feature in test_df.columns:\n",
    "    if re.match('((manager_id)|(house_type))\\S+((mean)|(median)|(min)|(max))',feature) !=None:\n",
    "        statistical.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features\n",
    "with_feat = []\n",
    "for feature in test_df.columns:\n",
    "    if re.match('with_\\S+',feature) !=None:\n",
    "        with_feat.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_hcc_feature = []\n",
    "new_new_hcc_features = []\n",
    "for feature in hcc_categoricals:    \n",
    "    new_hcc_feature.append('hcc_'+feature+'_high')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_medium')\n",
    "\n",
    "for feature in new_hcc_categoricals:\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_high')\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_medium')\n",
    "    new_new_hcc_features.append(feature+'_nrank_s_r')\n",
    "    new_new_hcc_features.append(feature+'_perf_s_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_hcc_feature = []\n",
    "\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#hcc encoding for the old hcc features\n",
    "for feature in hcc_categoricals:    \n",
    "    for train,test in skf:\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'high',\\\n",
    "                   update_df = train_df)\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'medium',\\\n",
    "                   update_df = train_df)\n",
    "\n",
    "    hcc_scoring(train_df,test_df,feature,'high')\n",
    "    hcc_scoring(train_df,test_df,feature,'medium')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_high')\n",
    "    new_hcc_feature.append('hcc_'+feature+'_medium')\n",
    "\n",
    "transferred=['another_day','another_hour']\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"],unit='ms')\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"],unit='ms')\n",
    "\n",
    "train_df['another_day']=(train_df['created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "train_df['another_hour']=(train_df['created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "test_df['another_day']=(test_df['created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "test_df['another_hour']=(test_df['created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "\n",
    "train_df['pic_created']=train_df['time_stamp'].apply(datetime.datetime.fromtimestamp)\n",
    "test_df['pic_created']=test_df['time_stamp'].apply(datetime.datetime.fromtimestamp)\n",
    "\n",
    "train_df[\"pic_month\"] = train_df[\"pic_created\"].dt.month\n",
    "test_df[\"pic_month\"] = test_df[\"pic_created\"].dt.month\n",
    "train_df[\"pic_day\"] = train_df[\"pic_created\"].dt.day\n",
    "test_df[\"pic_day\"] = test_df[\"pic_created\"].dt.day\n",
    "train_df[\"pic_hour\"] = train_df[\"pic_created\"].dt.hour\n",
    "test_df[\"pic_hour\"] = test_df[\"pic_created\"].dt.hour\n",
    "\n",
    "train_df['another_pic_day']=(train_df['pic_created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "train_df['another_pic_hour']=(train_df['pic_created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "test_df['another_pic_day']=(test_df['pic_created']+pd.tseries.offsets.DateOffset(days=15)).dt.day\n",
    "test_df['another_pic_hour']=(test_df['pic_created']+pd.tseries.offsets.DateOffset(hours=12)).dt.hour\n",
    "\n",
    "transferred.extend(['pic_month','pic_day','pic_hour','another_pic_day','another_pic_hour'])\n",
    "\n",
    "\n",
    "new_new_hcc_features=[]\n",
    "#new cat_gen_features\n",
    "skf=KFold(len(train_df['interest_level']),5,shuffle=True,random_state = 42)\n",
    "#hcc encoding for the old hcc features\n",
    "for feature in new_hcc_categoricals:\n",
    "    for train,test in skf:\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'high',\\\n",
    "                   update_df = train_df)\n",
    "        hcc_scoring(train_df.iloc[train,:],train_df.iloc[test,:],feature,'medium',\\\n",
    "                   update_df = train_df)\n",
    "        performance_eval(train_df.iloc[train,:],train_df.iloc[test,:],feature,\\\n",
    "                   update_df = train_df,random=0.01)\n",
    "\n",
    "    hcc_scoring(train_df,test_df,feature,'high')\n",
    "    hcc_scoring(train_df,test_df,feature,'medium')\n",
    "    performance_eval(train_df,test_df,feature,random=0.01)\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_high')\n",
    "    new_new_hcc_features.append('hcc_'+feature+'_medium')\n",
    "    new_new_hcc_features.append(feature+'_nrank_s_r')\n",
    "    new_new_hcc_features.append(feature+'_perf_s_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transferred=['another_day','another_hour']\n",
    "transferred.extend(['pic_month','pic_day','pic_hour','another_pic_day','another_pic_hour'])\n",
    "\n",
    "processing_features = numericals+numerical_may_processed+new_hcc_feature+transferred+statistical+new_new_hcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_features =['num_description_words','num_features','num_photos','price',\\\n",
    "               'price_per_bath','price_per_bed','price_per_room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in processing_features:\n",
    "    train_df.loc[train_df[f]==-1,f]=np.nan\n",
    "    test_df.loc[test_df[f]==-1,f]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_train = train_df.copy()\n",
    "normalized_test = test_df.copy()\n",
    "\n",
    "train_test = pd.concat([train_df.drop('interest_level',axis=1),test_df])\n",
    "\n",
    "for f in processing_features:\n",
    "    normalized_train[f]=normalized_train[f].fillna(train_test[f].median())\n",
    "    normalized_test[f]=normalized_test[f].fillna(train_test[f].median())\n",
    "\n",
    "for f in log_features:\n",
    "    normalized_train[f]=np.log(normalized_train[f]+1)\n",
    "    normalized_test[f]=np.log(normalized_test[f]+1)\n",
    "    \n",
    "for f in processing_features:\n",
    "    normalized_train[f]=(normalized_train[f]-train_test[f].mean())/train_test[f].std()\n",
    "    normalized_test[f]=(normalized_test[f]-train_test[f].mean())/train_test[f].std()\n",
    "\n",
    "#store the basic transformed train and test\n",
    "#normalized_train.to_json(data_path+'normal_train_df.json')\n",
    "#normalized_test.to_json(data_path+'normal_test_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_train.to_json(store+'loglr-n-train.json')\n",
    "normalized_test.to_json(store+'loglr-n-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare for training\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=KFold(len(train_df),5,shuffle=True,random_state = 2333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=list(processing_features)+with_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60390196827772025]\n",
      "0.603901968278\n"
     ]
    }
   ],
   "source": [
    "#first edition:\n",
    "#numericals from xgb142 + some new hcc encoding + with_feat from xgb142\n",
    "cv_scores=[]\n",
    "for dev_index, val_index in KF:\n",
    "    dev_set, val_set = normalized_train.iloc[dev_index,:] , normalized_train.iloc[val_index,:] \n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "            #random forest us\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(dev_X,dev_y)\n",
    "    preds = lr.predict_proba(val_X)\n",
    "        \n",
    "    cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "\n",
    "    print(cv_scores)\n",
    "    break\n",
    "print np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.fillna(-1)\n",
    "test_df = test_df.fillna(-1)\n",
    "\n",
    "store = '/home/raku/kaggleData/2sigma/loglr/'\n",
    "\n",
    "train_df.to_json(store+'loglr-train.json')\n",
    "test_df.to_json(store+'loglr-test.json')\n",
    "\n",
    "pickl_file = store+'loglrfeatures.pickle'\n",
    "fileObject = open(pickl_file,'wb') \n",
    "pickle.dump(features,fileObject)   \n",
    "fileObject.close()\n",
    "print len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "store = '/home/raku/kaggleData/2sigma/loglr/'\n",
    "\n",
    "train_df=pd.read_json(store+'loglr-train.json')\n",
    "test_df=pd.read_json(store+'loglr-test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = test_df['listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         7142618\n",
       "1         7210040\n",
       "100       7103890\n",
       "1000      7143442\n",
       "100000    6860601\n",
       "100001    6840081\n",
       "100002    6922337\n",
       "100003    6913616\n",
       "100005    6937820\n",
       "100006    6893933\n",
       "100008    6832604\n",
       "100009    6915282\n",
       "10001     7127565\n",
       "100010    6827899\n",
       "100011    6934855\n",
       "100012    6861826\n",
       "100015    6871643\n",
       "100017    6842542\n",
       "100018    6934145\n",
       "100019    6829365\n",
       "10002     7167858\n",
       "100021    6859483\n",
       "100022    6861377\n",
       "100023    6848960\n",
       "100024    6918850\n",
       "100025    6916867\n",
       "100028    6895840\n",
       "100029    6813539\n",
       "10003     7116900\n",
       "100031    6890328\n",
       "           ...   \n",
       "99957     6855560\n",
       "99958     6816731\n",
       "99959     6925764\n",
       "9996      7139280\n",
       "99962     6913068\n",
       "99963     6828445\n",
       "99967     6867865\n",
       "99968     6820397\n",
       "99969     6852197\n",
       "9997      7122934\n",
       "99970     6907838\n",
       "99971     6865896\n",
       "99972     6840250\n",
       "99973     6926011\n",
       "99974     6893100\n",
       "99975     6867538\n",
       "99976     6884360\n",
       "99977     6903964\n",
       "99978     6907851\n",
       "9998      7211166\n",
       "99981     6844290\n",
       "99983     6947597\n",
       "99985     6895423\n",
       "99989     6812077\n",
       "99990     6903956\n",
       "99995     6881005\n",
       "99996     6835379\n",
       "99997     6882352\n",
       "99998     6884758\n",
       "99999     6924212\n",
       "Name: listing_id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
