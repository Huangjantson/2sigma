{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import lightgbm as lgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mochi import CVstatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try xgboost    \n",
    "\n",
    "def runLGBM(train_X, train_y, test_X, test_y=None, feature_names=None,\n",
    "           seed_val=0, num_rounds=10000,watch_dict = None,max_bin=50000,\n",
    "           max_depth=4,early_stop=64,verbose=True,eta=0.1):\n",
    "    param = {'learning_rate':eta, \n",
    "             'max_depth':max_depth,\n",
    "             'application':'multiclass',\n",
    "             'num_class':3,\n",
    "             'metric':'multi_logloss',\n",
    "              'num_threads':4}\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    #plst = list(param.items())\n",
    "    lgbtrain = lgb.Dataset(train_X, label=train_y,max_bin=max_bin,feature_name=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgbtest = lgb.Dataset(test_X, label=test_y,max_bin=max_bin,feature_name=feature_names)\n",
    "        watchlist = [lgbtrain,lgbtest]\n",
    "        watchlist_name=['train','test']\n",
    "        model = lgb.train(param, lgbtrain, num_rounds, watchlist,watchlist_name, early_stopping_rounds=early_stop,\\\n",
    "                         evals_result = watch_dict,verbose_eval=verbose)\n",
    "    else:\n",
    "        #lgbtest = lgb.Dataset(test_X,feature_name=feature_names)\n",
    "        model = lgb.train(param, lgbtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 293)\n",
      "(74659, 292)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"/home/raku/kaggleData/2sigma/xgb142/\"\n",
    "store = \"/home/raku/kaggleData/2sigma/lgbm/\"\n",
    "train_file = data_path + \"xgb1.42-train.json\"\n",
    "test_file = data_path + \"xgb1.42-test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "feature_file = data_path+'xgb142features.pickle'\n",
    "fileObject = open(feature_file,'r') \n",
    "features = pickle.load(fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=StratifiedKFold(train_y,5,shuffle=True,random_state = 2333)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's multi_logloss: 0.636503\ttest's multi_logloss: 0.649845\n",
      "[200]\ttrain's multi_logloss: 0.56796\ttest's multi_logloss: 0.58866\n",
      "[300]\ttrain's multi_logloss: 0.540214\ttest's multi_logloss: 0.567894\n",
      "[400]\ttrain's multi_logloss: 0.52217\ttest's multi_logloss: 0.555502\n",
      "[500]\ttrain's multi_logloss: 0.508789\ttest's multi_logloss: 0.547945\n",
      "[600]\ttrain's multi_logloss: 0.497688\ttest's multi_logloss: 0.542587\n",
      "[700]\ttrain's multi_logloss: 0.488546\ttest's multi_logloss: 0.538502\n",
      "[800]\ttrain's multi_logloss: 0.480775\ttest's multi_logloss: 0.53532\n",
      "[900]\ttrain's multi_logloss: 0.473766\ttest's multi_logloss: 0.532694\n",
      "[1000]\ttrain's multi_logloss: 0.46725\ttest's multi_logloss: 0.530681\n",
      "[1100]\ttrain's multi_logloss: 0.461307\ttest's multi_logloss: 0.529177\n",
      "[1200]\ttrain's multi_logloss: 0.455817\ttest's multi_logloss: 0.528073\n",
      "[1300]\ttrain's multi_logloss: 0.450436\ttest's multi_logloss: 0.527115\n",
      "[1400]\ttrain's multi_logloss: 0.445314\ttest's multi_logloss: 0.526373\n",
      "[1500]\ttrain's multi_logloss: 0.440168\ttest's multi_logloss: 0.525744\n",
      "[1600]\ttrain's multi_logloss: 0.435314\ttest's multi_logloss: 0.525216\n",
      "[1700]\ttrain's multi_logloss: 0.430591\ttest's multi_logloss: 0.524634\n",
      "[1800]\ttrain's multi_logloss: 0.425755\ttest's multi_logloss: 0.524242\n",
      "[1900]\ttrain's multi_logloss: 0.421216\ttest's multi_logloss: 0.524034\n",
      "[2000]\ttrain's multi_logloss: 0.416845\ttest's multi_logloss: 0.523792\n",
      "[2100]\ttrain's multi_logloss: 0.412557\ttest's multi_logloss: 0.523517\n",
      "[2200]\ttrain's multi_logloss: 0.408409\ttest's multi_logloss: 0.52336\n",
      "[2300]\ttrain's multi_logloss: 0.404307\ttest's multi_logloss: 0.523148\n",
      "[2400]\ttrain's multi_logloss: 0.400356\ttest's multi_logloss: 0.523044\n",
      "[2500]\ttrain's multi_logloss: 0.396398\ttest's multi_logloss: 0.523037\n",
      "[2600]\ttrain's multi_logloss: 0.392562\ttest's multi_logloss: 0.522988\n",
      "[2700]\ttrain's multi_logloss: 0.388769\ttest's multi_logloss: 0.522965\n",
      "[2800]\ttrain's multi_logloss: 0.38525\ttest's multi_logloss: 0.52305\n",
      "[2900]\ttrain's multi_logloss: 0.381564\ttest's multi_logloss: 0.5232\n",
      "[3000]\ttrain's multi_logloss: 0.377865\ttest's multi_logloss: 0.523162\n",
      "loss for the turn 1 is 0.521870999838\n",
      "[100]\ttrain's multi_logloss: 0.637757\ttest's multi_logloss: 0.644076\n",
      "[200]\ttrain's multi_logloss: 0.56987\ttest's multi_logloss: 0.581929\n",
      "[300]\ttrain's multi_logloss: 0.542933\ttest's multi_logloss: 0.560486\n",
      "[400]\ttrain's multi_logloss: 0.525371\ttest's multi_logloss: 0.548188\n",
      "[500]\ttrain's multi_logloss: 0.512251\ttest's multi_logloss: 0.539837\n",
      "[600]\ttrain's multi_logloss: 0.501511\ttest's multi_logloss: 0.533931\n",
      "[700]\ttrain's multi_logloss: 0.49246\ttest's multi_logloss: 0.529423\n",
      "[800]\ttrain's multi_logloss: 0.484791\ttest's multi_logloss: 0.525833\n",
      "[900]\ttrain's multi_logloss: 0.477751\ttest's multi_logloss: 0.522967\n",
      "[1000]\ttrain's multi_logloss: 0.471129\ttest's multi_logloss: 0.520561\n",
      "[1100]\ttrain's multi_logloss: 0.464992\ttest's multi_logloss: 0.518461\n",
      "[1200]\ttrain's multi_logloss: 0.459184\ttest's multi_logloss: 0.516664\n",
      "[1300]\ttrain's multi_logloss: 0.453377\ttest's multi_logloss: 0.515242\n",
      "[1400]\ttrain's multi_logloss: 0.448129\ttest's multi_logloss: 0.514194\n",
      "[1500]\ttrain's multi_logloss: 0.442915\ttest's multi_logloss: 0.513146\n",
      "[1600]\ttrain's multi_logloss: 0.437898\ttest's multi_logloss: 0.512205\n",
      "[1700]\ttrain's multi_logloss: 0.433011\ttest's multi_logloss: 0.511298\n",
      "[1800]\ttrain's multi_logloss: 0.428167\ttest's multi_logloss: 0.510622\n",
      "[1900]\ttrain's multi_logloss: 0.423421\ttest's multi_logloss: 0.510192\n",
      "[2000]\ttrain's multi_logloss: 0.418956\ttest's multi_logloss: 0.509728\n",
      "[2100]\ttrain's multi_logloss: 0.414657\ttest's multi_logloss: 0.509486\n",
      "[2200]\ttrain's multi_logloss: 0.410385\ttest's multi_logloss: 0.509274\n",
      "[2300]\ttrain's multi_logloss: 0.406152\ttest's multi_logloss: 0.509091\n",
      "[2400]\ttrain's multi_logloss: 0.402238\ttest's multi_logloss: 0.508927\n",
      "[2500]\ttrain's multi_logloss: 0.398352\ttest's multi_logloss: 0.508861\n",
      "[2600]\ttrain's multi_logloss: 0.394718\ttest's multi_logloss: 0.508804\n",
      "[2700]\ttrain's multi_logloss: 0.391033\ttest's multi_logloss: 0.50876\n",
      "[2800]\ttrain's multi_logloss: 0.387425\ttest's multi_logloss: 0.508693\n",
      "[2900]\ttrain's multi_logloss: 0.383879\ttest's multi_logloss: 0.508723\n",
      "[3000]\ttrain's multi_logloss: 0.380437\ttest's multi_logloss: 0.508755\n",
      "loss for the turn 2 is 0.50891160849\n",
      "[100]\ttrain's multi_logloss: 0.637399\ttest's multi_logloss: 0.645888\n",
      "[200]\ttrain's multi_logloss: 0.569218\ttest's multi_logloss: 0.585092\n",
      "[300]\ttrain's multi_logloss: 0.541401\ttest's multi_logloss: 0.564269\n",
      "[400]\ttrain's multi_logloss: 0.52346\ttest's multi_logloss: 0.552269\n",
      "[500]\ttrain's multi_logloss: 0.509748\ttest's multi_logloss: 0.544479\n",
      "[600]\ttrain's multi_logloss: 0.498905\ttest's multi_logloss: 0.539215\n",
      "[700]\ttrain's multi_logloss: 0.489752\ttest's multi_logloss: 0.535079\n",
      "[800]\ttrain's multi_logloss: 0.48175\ttest's multi_logloss: 0.531871\n",
      "[900]\ttrain's multi_logloss: 0.474665\ttest's multi_logloss: 0.529591\n",
      "[1000]\ttrain's multi_logloss: 0.467931\ttest's multi_logloss: 0.527505\n",
      "[1100]\ttrain's multi_logloss: 0.461824\ttest's multi_logloss: 0.525899\n",
      "[1200]\ttrain's multi_logloss: 0.456299\ttest's multi_logloss: 0.524693\n",
      "[1300]\ttrain's multi_logloss: 0.451005\ttest's multi_logloss: 0.523794\n",
      "[1400]\ttrain's multi_logloss: 0.445941\ttest's multi_logloss: 0.522948\n",
      "[1500]\ttrain's multi_logloss: 0.440698\ttest's multi_logloss: 0.522007\n",
      "[1600]\ttrain's multi_logloss: 0.435748\ttest's multi_logloss: 0.521381\n",
      "[1700]\ttrain's multi_logloss: 0.430993\ttest's multi_logloss: 0.520804\n",
      "[1800]\ttrain's multi_logloss: 0.426373\ttest's multi_logloss: 0.520347\n",
      "[1900]\ttrain's multi_logloss: 0.422012\ttest's multi_logloss: 0.519981\n",
      "[2000]\ttrain's multi_logloss: 0.417576\ttest's multi_logloss: 0.519559\n",
      "[2100]\ttrain's multi_logloss: 0.413145\ttest's multi_logloss: 0.519399\n",
      "[2200]\ttrain's multi_logloss: 0.408996\ttest's multi_logloss: 0.519098\n",
      "[2300]\ttrain's multi_logloss: 0.40495\ttest's multi_logloss: 0.518949\n",
      "[2400]\ttrain's multi_logloss: 0.401072\ttest's multi_logloss: 0.518837\n",
      "[2500]\ttrain's multi_logloss: 0.397248\ttest's multi_logloss: 0.518798\n",
      "[2600]\ttrain's multi_logloss: 0.393603\ttest's multi_logloss: 0.518712\n",
      "[2700]\ttrain's multi_logloss: 0.389936\ttest's multi_logloss: 0.51866\n",
      "[2800]\ttrain's multi_logloss: 0.386286\ttest's multi_logloss: 0.518627\n",
      "[2900]\ttrain's multi_logloss: 0.382914\ttest's multi_logloss: 0.518795\n",
      "[3000]\ttrain's multi_logloss: 0.379609\ttest's multi_logloss: 0.518865\n",
      "loss for the turn 3 is 0.518469703149\n",
      "[100]\ttrain's multi_logloss: 0.637759\ttest's multi_logloss: 0.644047\n",
      "[200]\ttrain's multi_logloss: 0.569296\ttest's multi_logloss: 0.58307\n",
      "[300]\ttrain's multi_logloss: 0.542116\ttest's multi_logloss: 0.56212\n",
      "[400]\ttrain's multi_logloss: 0.524221\ttest's multi_logloss: 0.550587\n",
      "[500]\ttrain's multi_logloss: 0.510382\ttest's multi_logloss: 0.542972\n",
      "[600]\ttrain's multi_logloss: 0.499577\ttest's multi_logloss: 0.53789\n",
      "[700]\ttrain's multi_logloss: 0.490178\ttest's multi_logloss: 0.534181\n",
      "[800]\ttrain's multi_logloss: 0.48225\ttest's multi_logloss: 0.531245\n",
      "[900]\ttrain's multi_logloss: 0.475103\ttest's multi_logloss: 0.529005\n",
      "[1000]\ttrain's multi_logloss: 0.468712\ttest's multi_logloss: 0.527339\n",
      "[1100]\ttrain's multi_logloss: 0.462867\ttest's multi_logloss: 0.526008\n",
      "[1200]\ttrain's multi_logloss: 0.457172\ttest's multi_logloss: 0.524886\n",
      "[1300]\ttrain's multi_logloss: 0.451742\ttest's multi_logloss: 0.523973\n",
      "[1400]\ttrain's multi_logloss: 0.446568\ttest's multi_logloss: 0.523101\n",
      "[1500]\ttrain's multi_logloss: 0.441455\ttest's multi_logloss: 0.522418\n",
      "[1600]\ttrain's multi_logloss: 0.436655\ttest's multi_logloss: 0.521868\n",
      "[1700]\ttrain's multi_logloss: 0.4318\ttest's multi_logloss: 0.521375\n",
      "[1800]\ttrain's multi_logloss: 0.427288\ttest's multi_logloss: 0.521032\n",
      "[1900]\ttrain's multi_logloss: 0.422789\ttest's multi_logloss: 0.520623\n",
      "[2000]\ttrain's multi_logloss: 0.418437\ttest's multi_logloss: 0.520264\n",
      "[2100]\ttrain's multi_logloss: 0.414233\ttest's multi_logloss: 0.520072\n",
      "[2200]\ttrain's multi_logloss: 0.410126\ttest's multi_logloss: 0.520123\n",
      "[2300]\ttrain's multi_logloss: 0.406118\ttest's multi_logloss: 0.520112\n",
      "[2400]\ttrain's multi_logloss: 0.401926\ttest's multi_logloss: 0.520018\n",
      "[2500]\ttrain's multi_logloss: 0.398104\ttest's multi_logloss: 0.519942\n",
      "[2600]\ttrain's multi_logloss: 0.394348\ttest's multi_logloss: 0.519789\n",
      "[2700]\ttrain's multi_logloss: 0.390604\ttest's multi_logloss: 0.519754\n",
      "[2800]\ttrain's multi_logloss: 0.386884\ttest's multi_logloss: 0.519682\n",
      "[2900]\ttrain's multi_logloss: 0.383349\ttest's multi_logloss: 0.519684\n",
      "[3000]\ttrain's multi_logloss: 0.37964\ttest's multi_logloss: 0.51968\n",
      "loss for the turn 4 is 0.519509801479\n",
      "[100]\ttrain's multi_logloss: 0.638292\ttest's multi_logloss: 0.644525\n",
      "[200]\ttrain's multi_logloss: 0.570621\ttest's multi_logloss: 0.58047\n",
      "[300]\ttrain's multi_logloss: 0.543372\ttest's multi_logloss: 0.558103\n",
      "[400]\ttrain's multi_logloss: 0.525728\ttest's multi_logloss: 0.545792\n",
      "[500]\ttrain's multi_logloss: 0.512607\ttest's multi_logloss: 0.537367\n",
      "[600]\ttrain's multi_logloss: 0.501785\ttest's multi_logloss: 0.531684\n",
      "[700]\ttrain's multi_logloss: 0.492555\ttest's multi_logloss: 0.527459\n",
      "[800]\ttrain's multi_logloss: 0.48465\ttest's multi_logloss: 0.524127\n",
      "[900]\ttrain's multi_logloss: 0.477703\ttest's multi_logloss: 0.521727\n",
      "[1000]\ttrain's multi_logloss: 0.471464\ttest's multi_logloss: 0.519689\n",
      "[1100]\ttrain's multi_logloss: 0.465546\ttest's multi_logloss: 0.518094\n",
      "[1200]\ttrain's multi_logloss: 0.459664\ttest's multi_logloss: 0.516656\n",
      "[1300]\ttrain's multi_logloss: 0.454258\ttest's multi_logloss: 0.515588\n",
      "[1400]\ttrain's multi_logloss: 0.449107\ttest's multi_logloss: 0.514636\n",
      "[1500]\ttrain's multi_logloss: 0.443985\ttest's multi_logloss: 0.513948\n",
      "[1600]\ttrain's multi_logloss: 0.439059\ttest's multi_logloss: 0.51323\n",
      "[1700]\ttrain's multi_logloss: 0.434181\ttest's multi_logloss: 0.512651\n",
      "[1800]\ttrain's multi_logloss: 0.429502\ttest's multi_logloss: 0.512205\n",
      "[1900]\ttrain's multi_logloss: 0.42507\ttest's multi_logloss: 0.511751\n",
      "[2000]\ttrain's multi_logloss: 0.420447\ttest's multi_logloss: 0.511394\n",
      "[2100]\ttrain's multi_logloss: 0.416121\ttest's multi_logloss: 0.511189\n",
      "[2200]\ttrain's multi_logloss: 0.411705\ttest's multi_logloss: 0.510964\n",
      "[2300]\ttrain's multi_logloss: 0.407605\ttest's multi_logloss: 0.510712\n",
      "[2400]\ttrain's multi_logloss: 0.403607\ttest's multi_logloss: 0.510564\n",
      "[2500]\ttrain's multi_logloss: 0.399655\ttest's multi_logloss: 0.510516\n",
      "[2600]\ttrain's multi_logloss: 0.395661\ttest's multi_logloss: 0.510393\n",
      "[2700]\ttrain's multi_logloss: 0.391752\ttest's multi_logloss: 0.510343\n",
      "[2800]\ttrain's multi_logloss: 0.388043\ttest's multi_logloss: 0.510375\n",
      "[2900]\ttrain's multi_logloss: 0.384587\ttest's multi_logloss: 0.510292\n",
      "[3000]\ttrain's multi_logloss: 0.381126\ttest's multi_logloss: 0.510438\n",
      "loss for the turn 5 is 0.510470750864\n",
      "The mean of the cv_scores is:\n",
      "0.515846572764\n"
     ]
    }
   ],
   "source": [
    "cv_scores=[]\n",
    "cv_result=[]\n",
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    result_dict = {}\n",
    "    \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "       #filter the features\n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "    \n",
    "    preds, model = runLGBM(dev_X, dev_y, val_X, val_y,feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=3000,watch_dict=result_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    #save the pickles for futures use\n",
    "    pickl_file = store+'rf2000-5fold-out-'+str(i)+'.pickle'\n",
    "    fileObject = open(pickl_file,'wb') \n",
    "    pickle.dump(preds,fileObject)   \n",
    "    fileObject.close()\n",
    "    \"\"\"\n",
    "    loss = log_loss(val_y, preds)\n",
    "    \n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    i+=1\n",
    "    print'loss for the turn '+str(i)+' is '+str(loss)\n",
    "\n",
    "print 'The mean of the cv_scores is:'\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2805    0.516073\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvResult = CVstatistics(cv_result,'multi_logloss')\n",
    "meanTestError = cvResult.result.filter(like='test').mean(axis=1)\n",
    "meanTestError[meanTestError==np.min(meanTestError)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's multi_logloss: 0.507801\ttest's multi_logloss: 0.547764\n",
      "[200]\ttrain's multi_logloss: 0.466074\ttest's multi_logloss: 0.530546\n",
      "[300]\ttrain's multi_logloss: 0.437966\ttest's multi_logloss: 0.525532\n",
      "[400]\ttrain's multi_logloss: 0.413967\ttest's multi_logloss: 0.523429\n",
      "[500]\ttrain's multi_logloss: 0.393274\ttest's multi_logloss: 0.522885\n",
      "[600]\ttrain's multi_logloss: 0.374306\ttest's multi_logloss: 0.523549\n",
      "loss for the turn 1 is 0.522707556516\n",
      "[100]\ttrain's multi_logloss: 0.510451\ttest's multi_logloss: 0.539206\n",
      "[200]\ttrain's multi_logloss: 0.468151\ttest's multi_logloss: 0.52004\n",
      "[300]\ttrain's multi_logloss: 0.439215\ttest's multi_logloss: 0.513042\n",
      "[400]\ttrain's multi_logloss: 0.414608\ttest's multi_logloss: 0.510413\n",
      "[500]\ttrain's multi_logloss: 0.393972\ttest's multi_logloss: 0.509464\n",
      "[600]\ttrain's multi_logloss: 0.375713\ttest's multi_logloss: 0.509674\n",
      "loss for the turn 2 is 0.509744063282\n",
      "[100]\ttrain's multi_logloss: 0.508182\ttest's multi_logloss: 0.544172\n",
      "[200]\ttrain's multi_logloss: 0.46635\ttest's multi_logloss: 0.527834\n",
      "[300]\ttrain's multi_logloss: 0.438022\ttest's multi_logloss: 0.522411\n",
      "[400]\ttrain's multi_logloss: 0.414959\ttest's multi_logloss: 0.520036\n",
      "[500]\ttrain's multi_logloss: 0.394207\ttest's multi_logloss: 0.519647\n",
      "[600]\ttrain's multi_logloss: 0.375929\ttest's multi_logloss: 0.51931\n",
      "loss for the turn 3 is 0.518782492848\n",
      "[100]\ttrain's multi_logloss: 0.509008\ttest's multi_logloss: 0.542959\n",
      "[200]\ttrain's multi_logloss: 0.466737\ttest's multi_logloss: 0.526931\n",
      "[300]\ttrain's multi_logloss: 0.438537\ttest's multi_logloss: 0.521801\n",
      "[400]\ttrain's multi_logloss: 0.414691\ttest's multi_logloss: 0.520077\n",
      "[500]\ttrain's multi_logloss: 0.394571\ttest's multi_logloss: 0.519674\n",
      "[600]\ttrain's multi_logloss: 0.3755\ttest's multi_logloss: 0.51997\n",
      "loss for the turn 4 is 0.519785163436\n",
      "[100]\ttrain's multi_logloss: 0.511325\ttest's multi_logloss: 0.537342\n",
      "[200]\ttrain's multi_logloss: 0.469887\ttest's multi_logloss: 0.520244\n",
      "[300]\ttrain's multi_logloss: 0.441282\ttest's multi_logloss: 0.514412\n",
      "[400]\ttrain's multi_logloss: 0.417618\ttest's multi_logloss: 0.512079\n",
      "[500]\ttrain's multi_logloss: 0.395876\ttest's multi_logloss: 0.511177\n",
      "[600]\ttrain's multi_logloss: 0.37689\ttest's multi_logloss: 0.51135\n",
      "loss for the turn 5 is 0.5114209649\n",
      "The mean of the cv_scores is:\n",
      "0.516488048197\n"
     ]
    }
   ],
   "source": [
    "cv_scores=[]\n",
    "cv_result=[]\n",
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    result_dict = {}\n",
    "    \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "       #filter the features\n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "    \n",
    "    preds, model = runLGBM(dev_X, dev_y, val_X, val_y,feature_names=features,verbose=100,eta=0.1,\n",
    "                          early_stop=None,num_rounds=600,watch_dict=result_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    #save the pickles for futures use\n",
    "    pickl_file = store+'rf2000-5fold-out-'+str(i)+'.pickle'\n",
    "    fileObject = open(pickl_file,'wb') \n",
    "    pickle.dump(preds,fileObject)   \n",
    "    fileObject.close()\n",
    "    \"\"\"\n",
    "    loss = log_loss(val_y, preds)\n",
    "    \n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    i+=1\n",
    "    print'loss for the turn '+str(i)+' is '+str(loss)\n",
    "\n",
    "print 'The mean of the cv_scores is:'\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520    0.516471\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvResult = CVstatistics(cv_result,'multi_logloss')\n",
    "meanTestError = cvResult.result.filter(like='test').mean(axis=1)\n",
    "meanTestError[meanTestError==np.min(meanTestError)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's multi_logloss: 0.636503\ttest's multi_logloss: 0.649845\n",
      "[200]\ttrain's multi_logloss: 0.56796\ttest's multi_logloss: 0.58866\n",
      "[300]\ttrain's multi_logloss: 0.540214\ttest's multi_logloss: 0.567894\n",
      "[400]\ttrain's multi_logloss: 0.52217\ttest's multi_logloss: 0.555502\n",
      "[500]\ttrain's multi_logloss: 0.508789\ttest's multi_logloss: 0.547945\n",
      "[600]\ttrain's multi_logloss: 0.497688\ttest's multi_logloss: 0.542587\n",
      "[700]\ttrain's multi_logloss: 0.488546\ttest's multi_logloss: 0.538502\n",
      "[800]\ttrain's multi_logloss: 0.480775\ttest's multi_logloss: 0.53532\n",
      "[900]\ttrain's multi_logloss: 0.473766\ttest's multi_logloss: 0.532694\n",
      "[1000]\ttrain's multi_logloss: 0.46725\ttest's multi_logloss: 0.530681\n",
      "[1100]\ttrain's multi_logloss: 0.461307\ttest's multi_logloss: 0.529177\n",
      "[1200]\ttrain's multi_logloss: 0.455817\ttest's multi_logloss: 0.528073\n",
      "[1300]\ttrain's multi_logloss: 0.450436\ttest's multi_logloss: 0.527115\n",
      "[1400]\ttrain's multi_logloss: 0.445314\ttest's multi_logloss: 0.526373\n",
      "[1500]\ttrain's multi_logloss: 0.440168\ttest's multi_logloss: 0.525744\n",
      "[1600]\ttrain's multi_logloss: 0.435314\ttest's multi_logloss: 0.525216\n",
      "[1700]\ttrain's multi_logloss: 0.430591\ttest's multi_logloss: 0.524634\n",
      "[1800]\ttrain's multi_logloss: 0.425755\ttest's multi_logloss: 0.524242\n",
      "[1900]\ttrain's multi_logloss: 0.421216\ttest's multi_logloss: 0.524034\n",
      "[2000]\ttrain's multi_logloss: 0.416845\ttest's multi_logloss: 0.523792\n",
      "[2100]\ttrain's multi_logloss: 0.412557\ttest's multi_logloss: 0.523517\n",
      "[2200]\ttrain's multi_logloss: 0.408409\ttest's multi_logloss: 0.52336\n",
      "[2300]\ttrain's multi_logloss: 0.404307\ttest's multi_logloss: 0.523148\n",
      "[2400]\ttrain's multi_logloss: 0.400356\ttest's multi_logloss: 0.523044\n",
      "[2500]\ttrain's multi_logloss: 0.396398\ttest's multi_logloss: 0.523037\n",
      "[2600]\ttrain's multi_logloss: 0.392562\ttest's multi_logloss: 0.522988\n",
      "[2700]\ttrain's multi_logloss: 0.388769\ttest's multi_logloss: 0.522965\n",
      "[2800]\ttrain's multi_logloss: 0.38525\ttest's multi_logloss: 0.52305\n",
      "loss for the turn 1 is 0.521846013309\n",
      "[100]\ttrain's multi_logloss: 0.637757\ttest's multi_logloss: 0.644076\n",
      "[200]\ttrain's multi_logloss: 0.56987\ttest's multi_logloss: 0.581929\n",
      "[300]\ttrain's multi_logloss: 0.542933\ttest's multi_logloss: 0.560486\n",
      "[400]\ttrain's multi_logloss: 0.525371\ttest's multi_logloss: 0.548188\n",
      "[500]\ttrain's multi_logloss: 0.512251\ttest's multi_logloss: 0.539837\n",
      "[600]\ttrain's multi_logloss: 0.501511\ttest's multi_logloss: 0.533931\n",
      "[700]\ttrain's multi_logloss: 0.49246\ttest's multi_logloss: 0.529423\n",
      "[800]\ttrain's multi_logloss: 0.484791\ttest's multi_logloss: 0.525833\n",
      "[900]\ttrain's multi_logloss: 0.477751\ttest's multi_logloss: 0.522967\n",
      "[1000]\ttrain's multi_logloss: 0.471129\ttest's multi_logloss: 0.520561\n",
      "[1100]\ttrain's multi_logloss: 0.464992\ttest's multi_logloss: 0.518461\n",
      "[1200]\ttrain's multi_logloss: 0.459184\ttest's multi_logloss: 0.516664\n",
      "[1300]\ttrain's multi_logloss: 0.453377\ttest's multi_logloss: 0.515242\n",
      "[1400]\ttrain's multi_logloss: 0.448129\ttest's multi_logloss: 0.514194\n",
      "[1500]\ttrain's multi_logloss: 0.442915\ttest's multi_logloss: 0.513146\n",
      "[1600]\ttrain's multi_logloss: 0.437898\ttest's multi_logloss: 0.512205\n",
      "[1700]\ttrain's multi_logloss: 0.433011\ttest's multi_logloss: 0.511298\n",
      "[1800]\ttrain's multi_logloss: 0.428167\ttest's multi_logloss: 0.510622\n",
      "[1900]\ttrain's multi_logloss: 0.423421\ttest's multi_logloss: 0.510192\n",
      "[2000]\ttrain's multi_logloss: 0.418956\ttest's multi_logloss: 0.509728\n",
      "[2100]\ttrain's multi_logloss: 0.414657\ttest's multi_logloss: 0.509486\n",
      "[2200]\ttrain's multi_logloss: 0.410385\ttest's multi_logloss: 0.509274\n",
      "[2300]\ttrain's multi_logloss: 0.406152\ttest's multi_logloss: 0.509091\n",
      "[2400]\ttrain's multi_logloss: 0.402238\ttest's multi_logloss: 0.508927\n",
      "[2500]\ttrain's multi_logloss: 0.398352\ttest's multi_logloss: 0.508861\n",
      "[2600]\ttrain's multi_logloss: 0.394718\ttest's multi_logloss: 0.508804\n",
      "[2700]\ttrain's multi_logloss: 0.391033\ttest's multi_logloss: 0.50876\n",
      "[2800]\ttrain's multi_logloss: 0.387425\ttest's multi_logloss: 0.508693\n",
      "loss for the turn 2 is 0.508830756468\n",
      "[100]\ttrain's multi_logloss: 0.637399\ttest's multi_logloss: 0.645888\n",
      "[200]\ttrain's multi_logloss: 0.569218\ttest's multi_logloss: 0.585092\n",
      "[300]\ttrain's multi_logloss: 0.541401\ttest's multi_logloss: 0.564269\n",
      "[400]\ttrain's multi_logloss: 0.52346\ttest's multi_logloss: 0.552269\n",
      "[500]\ttrain's multi_logloss: 0.509748\ttest's multi_logloss: 0.544479\n",
      "[600]\ttrain's multi_logloss: 0.498905\ttest's multi_logloss: 0.539215\n",
      "[700]\ttrain's multi_logloss: 0.489752\ttest's multi_logloss: 0.535079\n",
      "[800]\ttrain's multi_logloss: 0.48175\ttest's multi_logloss: 0.531871\n",
      "[900]\ttrain's multi_logloss: 0.474665\ttest's multi_logloss: 0.529591\n",
      "[1000]\ttrain's multi_logloss: 0.467931\ttest's multi_logloss: 0.527505\n",
      "[1100]\ttrain's multi_logloss: 0.461824\ttest's multi_logloss: 0.525899\n",
      "[1200]\ttrain's multi_logloss: 0.456299\ttest's multi_logloss: 0.524693\n",
      "[1300]\ttrain's multi_logloss: 0.451005\ttest's multi_logloss: 0.523794\n",
      "[1400]\ttrain's multi_logloss: 0.445941\ttest's multi_logloss: 0.522948\n",
      "[1500]\ttrain's multi_logloss: 0.440698\ttest's multi_logloss: 0.522007\n",
      "[1600]\ttrain's multi_logloss: 0.435748\ttest's multi_logloss: 0.521381\n",
      "[1700]\ttrain's multi_logloss: 0.430993\ttest's multi_logloss: 0.520804\n",
      "[1800]\ttrain's multi_logloss: 0.426373\ttest's multi_logloss: 0.520347\n",
      "[1900]\ttrain's multi_logloss: 0.422012\ttest's multi_logloss: 0.519981\n",
      "[2000]\ttrain's multi_logloss: 0.417576\ttest's multi_logloss: 0.519559\n",
      "[2100]\ttrain's multi_logloss: 0.413145\ttest's multi_logloss: 0.519399\n",
      "[2200]\ttrain's multi_logloss: 0.408996\ttest's multi_logloss: 0.519098\n",
      "[2300]\ttrain's multi_logloss: 0.40495\ttest's multi_logloss: 0.518949\n",
      "[2400]\ttrain's multi_logloss: 0.401072\ttest's multi_logloss: 0.518837\n",
      "[2500]\ttrain's multi_logloss: 0.397248\ttest's multi_logloss: 0.518798\n",
      "[2600]\ttrain's multi_logloss: 0.393603\ttest's multi_logloss: 0.518712\n",
      "[2700]\ttrain's multi_logloss: 0.389936\ttest's multi_logloss: 0.51866\n",
      "[2800]\ttrain's multi_logloss: 0.386286\ttest's multi_logloss: 0.518627\n",
      "loss for the turn 3 is 0.518229563135\n",
      "[100]\ttrain's multi_logloss: 0.637759\ttest's multi_logloss: 0.644047\n",
      "[200]\ttrain's multi_logloss: 0.569296\ttest's multi_logloss: 0.58307\n",
      "[300]\ttrain's multi_logloss: 0.542116\ttest's multi_logloss: 0.56212\n",
      "[400]\ttrain's multi_logloss: 0.524221\ttest's multi_logloss: 0.550587\n",
      "[500]\ttrain's multi_logloss: 0.510382\ttest's multi_logloss: 0.542972\n",
      "[600]\ttrain's multi_logloss: 0.499577\ttest's multi_logloss: 0.53789\n",
      "[700]\ttrain's multi_logloss: 0.490178\ttest's multi_logloss: 0.534181\n",
      "[800]\ttrain's multi_logloss: 0.48225\ttest's multi_logloss: 0.531245\n",
      "[900]\ttrain's multi_logloss: 0.475103\ttest's multi_logloss: 0.529005\n",
      "[1000]\ttrain's multi_logloss: 0.468712\ttest's multi_logloss: 0.527339\n",
      "[1100]\ttrain's multi_logloss: 0.462867\ttest's multi_logloss: 0.526008\n",
      "[1200]\ttrain's multi_logloss: 0.457172\ttest's multi_logloss: 0.524886\n",
      "[1300]\ttrain's multi_logloss: 0.451742\ttest's multi_logloss: 0.523973\n",
      "[1400]\ttrain's multi_logloss: 0.446568\ttest's multi_logloss: 0.523101\n",
      "[1500]\ttrain's multi_logloss: 0.441455\ttest's multi_logloss: 0.522418\n",
      "[1600]\ttrain's multi_logloss: 0.436655\ttest's multi_logloss: 0.521868\n",
      "[1700]\ttrain's multi_logloss: 0.4318\ttest's multi_logloss: 0.521375\n",
      "[1800]\ttrain's multi_logloss: 0.427288\ttest's multi_logloss: 0.521032\n",
      "[1900]\ttrain's multi_logloss: 0.422789\ttest's multi_logloss: 0.520623\n",
      "[2000]\ttrain's multi_logloss: 0.418437\ttest's multi_logloss: 0.520264\n",
      "[2100]\ttrain's multi_logloss: 0.414233\ttest's multi_logloss: 0.520072\n",
      "[2200]\ttrain's multi_logloss: 0.410126\ttest's multi_logloss: 0.520123\n",
      "[2300]\ttrain's multi_logloss: 0.406118\ttest's multi_logloss: 0.520112\n",
      "[2400]\ttrain's multi_logloss: 0.401926\ttest's multi_logloss: 0.520018\n",
      "[2500]\ttrain's multi_logloss: 0.398104\ttest's multi_logloss: 0.519942\n",
      "[2600]\ttrain's multi_logloss: 0.394348\ttest's multi_logloss: 0.519789\n",
      "[2700]\ttrain's multi_logloss: 0.390604\ttest's multi_logloss: 0.519754\n",
      "[2800]\ttrain's multi_logloss: 0.386884\ttest's multi_logloss: 0.519682\n",
      "loss for the turn 4 is 0.519505620859\n",
      "[100]\ttrain's multi_logloss: 0.638292\ttest's multi_logloss: 0.644525\n",
      "[200]\ttrain's multi_logloss: 0.570621\ttest's multi_logloss: 0.58047\n",
      "[300]\ttrain's multi_logloss: 0.543372\ttest's multi_logloss: 0.558103\n",
      "[400]\ttrain's multi_logloss: 0.525728\ttest's multi_logloss: 0.545792\n",
      "[500]\ttrain's multi_logloss: 0.512607\ttest's multi_logloss: 0.537367\n",
      "[600]\ttrain's multi_logloss: 0.501785\ttest's multi_logloss: 0.531684\n",
      "[700]\ttrain's multi_logloss: 0.492555\ttest's multi_logloss: 0.527459\n",
      "[800]\ttrain's multi_logloss: 0.48465\ttest's multi_logloss: 0.524127\n",
      "[900]\ttrain's multi_logloss: 0.477703\ttest's multi_logloss: 0.521727\n",
      "[1000]\ttrain's multi_logloss: 0.471464\ttest's multi_logloss: 0.519689\n",
      "[1100]\ttrain's multi_logloss: 0.465546\ttest's multi_logloss: 0.518094\n",
      "[1200]\ttrain's multi_logloss: 0.459664\ttest's multi_logloss: 0.516656\n",
      "[1300]\ttrain's multi_logloss: 0.454258\ttest's multi_logloss: 0.515588\n",
      "[1400]\ttrain's multi_logloss: 0.449107\ttest's multi_logloss: 0.514636\n",
      "[1500]\ttrain's multi_logloss: 0.443985\ttest's multi_logloss: 0.513948\n",
      "[1600]\ttrain's multi_logloss: 0.439059\ttest's multi_logloss: 0.51323\n",
      "[1700]\ttrain's multi_logloss: 0.434181\ttest's multi_logloss: 0.512651\n",
      "[1800]\ttrain's multi_logloss: 0.429502\ttest's multi_logloss: 0.512205\n",
      "[1900]\ttrain's multi_logloss: 0.42507\ttest's multi_logloss: 0.511751\n",
      "[2000]\ttrain's multi_logloss: 0.420447\ttest's multi_logloss: 0.511394\n",
      "[2100]\ttrain's multi_logloss: 0.416121\ttest's multi_logloss: 0.511189\n",
      "[2200]\ttrain's multi_logloss: 0.411705\ttest's multi_logloss: 0.510964\n",
      "[2300]\ttrain's multi_logloss: 0.407605\ttest's multi_logloss: 0.510712\n",
      "[2400]\ttrain's multi_logloss: 0.403607\ttest's multi_logloss: 0.510564\n",
      "[2500]\ttrain's multi_logloss: 0.399655\ttest's multi_logloss: 0.510516\n",
      "[2600]\ttrain's multi_logloss: 0.395661\ttest's multi_logloss: 0.510393\n",
      "[2700]\ttrain's multi_logloss: 0.391752\ttest's multi_logloss: 0.510343\n",
      "[2800]\ttrain's multi_logloss: 0.388043\ttest's multi_logloss: 0.510375\n",
      "loss for the turn 5 is 0.510410762542\n",
      "The mean of the cv_scores is:\n",
      "0.515764543263\n"
     ]
    }
   ],
   "source": [
    "cv_scores=[]\n",
    "cv_result=[]\n",
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    result_dict = {}\n",
    "    \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "       #filter the features\n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "    \n",
    "    preds, model = runLGBM(dev_X, dev_y, val_X, val_y,feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=2800,watch_dict=result_dict,max_depth=4)\n",
    "    \n",
    "\n",
    "    #save the pickles for futures use\n",
    "    pickl_file = store+'lgbm-5fold-out-'+str(i)+'.pickle'\n",
    "    fileObject = open(pickl_file,'wb') \n",
    "    pickle.dump(preds,fileObject)   \n",
    "    fileObject.close()\n",
    "    \n",
    "    loss = log_loss(val_y, preds)\n",
    "    \n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    i+=1\n",
    "    print'loss for the turn '+str(i)+' is '+str(loss)\n",
    "\n",
    "print 'The mean of the cv_scores is:'\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, test_X = train_df[features].as_matrix(), test_df[features].as_matrix()\n",
    "\n",
    "preds, model = runLGBM(train_X, train_y, test_X,\\\n",
    "feature_names=features,\n",
    "num_rounds = 2800, eta = 0.02,max_depth = 4,verbose=100)\n",
    "\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df.to_json(store+'lgbm142-bulk-out.json')\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "#out_df.to_csv(\"xgb_beta1point42-0.02.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lgbm using 145\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
