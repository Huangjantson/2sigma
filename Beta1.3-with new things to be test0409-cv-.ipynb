{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import  preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new modules to be added\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try xgboost\n",
    "#original fucntion from SRK\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, \\\n",
    "     seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1,\\\n",
    "     max_depth = 6,cv_dict = None,verbose_eval=True):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = eta\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y,feature_names=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y,feature_names=feature_names)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,\\\n",
    "        early_stopping_rounds=early_stop,evals_result = cv_dict,verbose_eval = verbose_eval)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X,feature_names=feature_names)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "class CVstatistics(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    self.result : the result dataframe storing the cv results\n",
    "    self.endpoint : the first ending point for the validations\n",
    "    self.turns: the turns for each validation\n",
    "    \n",
    "    validCurve : plot the validation curve,stop at the first endpoint\n",
    "    errorsAt: return the average errors at a certain turn\n",
    "    \"\"\"\n",
    "    def __init__(self,result_dict,metric,k=5):\n",
    "        self.metric = metric\n",
    "        if type(result_dict) == pd.DataFrame:\n",
    "            self.result = result_dict\n",
    "        else:\n",
    "            temp_dict = {}\n",
    "            for phase in ['train','test']:\n",
    "                for turn in range(k):\n",
    "                    temp_dict[phase+str(turn)]=cv_result[turn][phase][metric]\n",
    "                    self.result=pd.DataFrame(dict([ (key,pd.Series(v)) for key,v in temp_dict.iteritems()]))    \n",
    "        \n",
    "        self.endpoint =len(self.result.filter(like = 'train').dropna())\n",
    "        \n",
    "        self.turns = self.result.filter(like = 'test').\\\n",
    "            apply(lambda x : ~np.isnan(x)).cumsum(axis=0).iloc[len(self.result)-1,:]\n",
    "\n",
    "    def validCurve(self,start = 0, stop_at_first = True):\n",
    "        if stop_at_first:\n",
    "            eout = self.result.iloc[start:,:].filter(like = 'test').dropna().mean(axis=1)\n",
    "            ein =  self.result.iloc[start:,:].filter(like = 'train').dropna().mean(axis=1)\n",
    "        else:\n",
    "            eout = self.result.iloc[start:,:].filter(like = 'test').mean(axis=1)\n",
    "            ein =  self.result.iloc[start:,:].filter(like = 'train').mean(axis=1)\n",
    "        plt.plot(map(lambda x :x+start,range(len(eout))), eout,\n",
    "        map(lambda x :x+start,range(len(ein))), ein)\n",
    "        plt.xlabel(\"turn\")\n",
    "        plt.ylabel(self.metric)\n",
    "        plt.title('Validation Curve')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def errorsAt(self,turn):\n",
    "        eout = self.result.filter(like = 'test').loc[turn].mean()\n",
    "        ein = self.result.filter(like = 'train').loc[turn].mean()\n",
    "        return eout,ein\n",
    "    \n",
    "def xgbImportance(model,factor_name):\n",
    "    factors = model.get_score(importance_type=factor_name)\n",
    "    factor_list = []\n",
    "    total = sum(factors.values())\n",
    "    for key in factors:\n",
    "        factors[key] = factors[key]*1.0/total\n",
    "        factor_list.append((key,factors[key]))\n",
    "    return sorted(factor_list,key=lambda x : x[1],reverse=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature processing functions\n",
    "#define punctutaion filter\n",
    "def removePunctuation(x):\n",
    "    #filter the head or tail blanks\n",
    "    x = re.sub(r'^\\s+',r' ',x)\n",
    "    x = re.sub(r'\\s+$',r' ',x)\n",
    "    \n",
    "    # Lowercasing all words\n",
    "    x = x.lower()\n",
    "    # Removing non ASCII chars, warning if you are dealing with other languages!!!!!!!!!!!!!!!\n",
    "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n",
    "    #change all the blank to space\n",
    "    x = re.sub(r'\\s',r' ',x)\n",
    "    # Removing (replacing with empty spaces actually) all the punctuations\n",
    "    removing = string.punctuation#.replace('-','')# except '-'\n",
    "    removed = re.sub(\"[\"+removing+\"]\", \"\", x)\n",
    "    #removing the line-changing\n",
    "    #removed = re.sub('\\\\n',\" \",removed)    \n",
    "    return removed\n",
    "\n",
    "#feature processing functions\n",
    "def proecessStreet(address):\n",
    "    #remove the building number\n",
    "    pattern = re.compile('^[\\d-]*[\\s]+')\n",
    "    street = removePunctuation(pattern.sub('',address))\n",
    "    \n",
    "    #sub the st to street\n",
    "    pattern = re.compile('( st)$')\n",
    "    street = pattern.sub(' street',street)\n",
    "    \n",
    "    #sub the ave to avenue\n",
    "    pattern = re.compile('( ave)$')\n",
    "    street = pattern.sub(' avenue',street)\n",
    "    \n",
    "    pattern = re.compile('(\\d+)((th)|(st)|(rd)|(nd))')\n",
    "    street = pattern.sub('\\g<1>',street)\n",
    "    \n",
    "    #deal with the w 14 street => west 14 street\n",
    "    pattern = re.compile('(w)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('west \\g<3>',street)\n",
    "    \n",
    "    #deal with the e....\n",
    "    pattern = re.compile('(e)(\\s+)(\\d+)')    \n",
    "    street = pattern.sub('east \\g<3>',street)\n",
    "    \n",
    "    return street\n",
    "    \n",
    "#from \"this is a lit\"s python version by rakhlin\n",
    "def singleValueConvert(df1,df2,column,minimum_size=5):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= minimum_size, column] = -1\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= minimum_size, column] = -1\n",
    "    return df1, df2\n",
    "\n",
    "def performance_eval(train_df,test_df,feature,k,g=1,f=1,update_df =None,random = None):\n",
    "    target_num_map = {'High':2, 'Medium':1, 'Low':0}\n",
    "    temp=pd.concat([train_df[feature],pd.get_dummies(train_df.interest_level)], axis = 1)\\\n",
    "         .groupby(feature).mean()\n",
    "     \n",
    "    new_feature = feature+'_perf'\n",
    "    \n",
    "    temp.columns = ['tempHigh','tempLow', 'tempMed']\n",
    "    \n",
    "    temp['count'] = train_df.groupby(feature).count().iloc[:,1]\n",
    "    temp[\"lambda\"] = g / (g + np.exp((k - temp[\"count\"] )/f))\n",
    "    temp[feature+'_origin'] = temp['tempHigh']*2 + temp['tempMed']\n",
    "    mean_values = temp.loc[:, feature+'_origin'].mean()\n",
    "    \n",
    "    temp[new_feature] = temp[\"lambda\"]*temp[feature+'_origin']+(1-temp[\"lambda\"])*mean_values    \n",
    "    \n",
    "    # Add uniform noise. Not mentioned in original paper.adding to each manager\n",
    "    if random:\n",
    "        temp[new_feature] *= np.random.uniform(1 - random, 1 + random, len(temp))     \n",
    "\n",
    "    value = test_df[[feature]].join(temp, on=feature, how=\"left\")[new_feature].fillna(mean_values)\n",
    "    \n",
    "    if update_df is None: update_df = test_df\n",
    "    if new_feature not in update_df.columns: update_df[new_feature] = np.nan\n",
    "    update_df.update(value)\n",
    "    \n",
    "#functions for features\n",
    "def featureList(train_df,test_df,limit = 0.001):\n",
    "    #acquiring the feature lists\n",
    "    features_in_train = train_df[\"features\"].apply(pd.Series).unstack().reset_index(drop = True).dropna().value_counts()\n",
    "    features_in_test = test_df[\"features\"].apply(pd.Series).unstack().reset_index(drop = True).dropna().value_counts()\n",
    "    \n",
    "    filtered_features_in_train = features_in_train[features_in_train > limit*len(train_df)]\n",
    "    filtered_features_in_test = features_in_test[features_in_test > limit*len(test_df)]\n",
    "    accept_list = set(filtered_features_in_train.index).union(set(filtered_features_in_test.index))\n",
    "    return accept_list\n",
    "\n",
    "def featureMapping(train_df,test_df,feature_list):\n",
    "    for feature in feature_list:\n",
    "        #add the feature column for both\n",
    "        #if feature in the row, then set the value for (row,feature) to 1\n",
    "        train_df['with_'+feature]=train_df['features'].apply(lambda x : 1 if feature in x else 0)\n",
    "        test_df['with_'+feature]=test_df['features'].apply(lambda x : 1 if feature in x else 0)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new function for clustering\n",
    "def getCluster(train_df,test_df,k):\n",
    "    cluster = KMeans(k,random_state = 2333)\n",
    "    cluster.fit(train_df[['latitude', 'longitude']].dropna())\n",
    "    train_df['cluster_id_'+str(k)]=map(lambda x,y: cluster.predict(np.array([x,y]).reshape(1,-1)) \\\n",
    "                           if ~(np.isnan(x)|np.isnan(y)) else -1,\\\n",
    "                           train_df['latitude'],train_df['longitude'])\n",
    "    test_df['cluster_id_'+str(k)]=map(lambda x,y: cluster.predict(np.array([x,y]).reshape(1,-1)) \\\n",
    "                           if ~(np.isnan(x)|np.isnan(y)) else -1,\\\n",
    "                           test_df['latitude'],test_df['longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting the outliers to be nan. to be test\n",
    "def processMap(df):\n",
    "    for i in ['latitude', 'longitude']:\n",
    "        Q1 = df[i].quantile(0.005)\n",
    "        Q3 = df[i].quantile(0.995)\n",
    "        IQR = Q3 - Q1\n",
    "        upper = Q3\n",
    "        lower = Q1\n",
    "        df.ix[(df[i]>upper)|(df[i]<lower),i] = np.nan\n",
    "        #df.ix[:,i] =  df[i].round(3) \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-845ad5162935>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#test_df = pd.read_json(test_file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"../../kaggleData/2sigma/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "#test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "#test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "#print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic numerical features\n",
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some transfromed features\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "#test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "#test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "#test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "#test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "#test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "#test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "#test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "#test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "#some new numerical features related to the price\n",
    "train_df[\"price_per_bath\"] =  (train_df[\"price\"]*1.0/train_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_bed\"] = (train_df[\"price\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"bath_per_bed\"] = (train_df[\"bathrooms\"]*1.0/train_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "train_df[\"price_per_room\"] = (train_df[\"price\"]*1.0/(train_df[\"bedrooms\"]+train_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "#test_df[\"price_per_bath\"] =  (test_df[\"price\"]*1.0/test_df[\"bathrooms\"]).replace(np.Inf,-1)\n",
    "#test_df[\"price_per_bed\"] = (test_df[\"price\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "#test_df[\"bath_per_bed\"] = (test_df[\"bathrooms\"]*1.0/test_df[\"bedrooms\"]).replace(np.Inf,-1)\n",
    "#test_df[\"price_per_room\"] = (test_df[\"price\"]*1.0/(test_df[\"bedrooms\"]+test_df[\"bathrooms\"])).replace(np.Inf,-1)\n",
    "\n",
    "\n",
    "# adding all these new features to use list # \"listing_id\",\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\\\n",
    "                        \"created_year\",\"listing_id\", \"created_month\", \"created_day\", \"created_hour\"])\n",
    "#price new features\n",
    "features_to_use.extend([\"price_per_bed\",\"bath_per_bed\",\"price_per_room\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "new categorical data generated from the old ones\n",
    "\"\"\"\n",
    "#new feature for the street_address, use them instead of the original one\n",
    "train_df[\"street_name\"] = train_df[\"street_address\"].apply(proecessStreet)\n",
    "#test_df[\"street_name\"] = test_df[\"street_address\"].apply(proecessStreet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dealing with features\n",
    "\n",
    "#preprocessing for features\n",
    "train_df[\"features\"] = train_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_') \\\n",
    "                                                            for i in x])\n",
    "#test_df[\"features\"] = test_df[\"features\"].apply(lambda x:[\"_\".join(i.split(\" \")).lower().strip().replace('-','_')\\\n",
    "#                                                          for i in x])\n",
    "#create the accept list\n",
    "accept_list = list(featureList(train_df,test_df,limit = 0.001))\n",
    "\n",
    "#map the feature to dummy slots\n",
    "featureMapping(train_df,test_df,accept_list)\n",
    "features_to_use.extend(map(lambda x : 'with_'+x,accept_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'num_photos', 'num_features', 'num_description_words', 'created_year', 'listing_id', 'created_month', 'created_day', 'created_hour', 'price_per_bed', 'bath_per_bed', 'price_per_room', u'with_exclusive', u'with_furnished', u'with_lowrise', u'with_common_parking/garage', u'with_pets_on_approval', u'with_terrace', u'with_live_in_superintendent', u'with_newly_renovated', u'with_full_time_doorman', u'with_duplex', u'with_dryer_in_unit', u'with_multi_level', u'with_garden', u'with_hardwood_floors', u'with_on_site_garage', u'with_fireplace', u'with_eat_in_kitchen', u'with_wifi_access', u'with_garage', u'with_subway', u'with_dining_room', u'with_view', u'with_publicoutdoor', u'with_hardwood', u'with_fitness_center', u'with_high_speed_internet', u'with_laundry_in_building', u'with_parking', u'with_garden/patio', u'with_prewar', u'with_on_site_laundry', u'with_valet', u'with_green_building', u'with_short_term_allowed', u'with_new_construction', u'with_reduced_fee', u'with_roofdeck', u'with_stainless_steel_appliances', u'with_simplex', u'with_dishwasher', u'with_washer_in_unit', u'with_cats_allowed', u'with_exposed_brick', u'with_roof_deck', u'with_common_outdoor_space', u'with_outdoor_areas', u'with_common_roof_deck', u'with_no_pets', u'with_childrens_playroom', u'with_central_a/c', u'with_wheelchair_access', u'with_post_war', u'with_renovated', u'with_elevator', u'with_highrise', u'with_loft', u'with_gym', u'with_luxury_building', u'with_outdoor_space', u'with_pre_war', u'with_residents_lounge', u'with_laundry_room', u'with_marble_bath', u'with_laundry_in_unit', u'with_parking_space', u'with_private_outdoor_space', u'with_high_ceiling', u'with_concierge', u'with_walk_in_closet(s)', u'with_doorman', u'with_balcony', u'with_dogs_allowed', u'with_gym/fitness', u'with_storage', u'with_live_in_super', u'with_lounge', u'with_granite_kitchen', u'with_private_balcony', u'with_laundry', u'with_actual_apt._photos', u'with_residents_garden', u'with_pool', u'with_washer/dryer', u'with_light', u'with_swimming_pool', u'with_high_ceilings', u'with_patio', u'with_no_fee', u'with_bike_room']\n"
     ]
    }
   ],
   "source": [
    "#shorten reprocessing time: save the preprocessed train_df and test_df with some basic features\n",
    "#train_df.to_json('train1.3std.json')\n",
    "#test_df.to_json('test1.3std.json')\n",
    "#print features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shorten reprocessing time: load the preprocessed train_df and test_df with some basic features\n",
    "train_df=pd.read_json('train1.3std.json')\n",
    "#test_df=pd.read_json('test1.3std.json')\n",
    "features_to_use = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'num_photos', 'num_features', 'num_description_words', 'created_year', 'listing_id', 'created_month', 'created_day', 'created_hour', 'price_per_bed', 'bath_per_bed', 'price_per_room', u'with_exclusive', u'with_furnished', u'with_lowrise', u'with_common_parking/garage', u'with_pets_on_approval', u'with_terrace', u'with_live_in_superintendent', u'with_newly_renovated', u'with_full_time_doorman', u'with_duplex', u'with_dryer_in_unit', u'with_multi_level', u'with_garden', u'with_hardwood_floors', u'with_on_site_garage', u'with_fireplace', u'with_eat_in_kitchen', u'with_wifi_access', u'with_garage', u'with_subway', u'with_dining_room', u'with_view', u'with_publicoutdoor', u'with_hardwood', u'with_fitness_center', u'with_high_speed_internet', u'with_laundry_in_building', u'with_parking', u'with_garden/patio', u'with_prewar', u'with_on_site_laundry', u'with_valet', u'with_green_building', u'with_short_term_allowed', u'with_new_construction', u'with_reduced_fee', u'with_roofdeck', u'with_stainless_steel_appliances', u'with_simplex', u'with_dishwasher', u'with_washer_in_unit', u'with_cats_allowed', u'with_exposed_brick', u'with_roof_deck', u'with_common_outdoor_space', u'with_outdoor_areas', u'with_common_roof_deck', u'with_no_pets', u'with_childrens_playroom', u'with_central_a/c', u'with_wheelchair_access', u'with_post_war', u'with_renovated', u'with_elevator', u'with_highrise', u'with_loft', u'with_gym', u'with_luxury_building', u'with_outdoor_space', u'with_pre_war', u'with_residents_lounge', u'with_laundry_room', u'with_marble_bath', u'with_laundry_in_unit', u'with_parking_space', u'with_private_outdoor_space', u'with_high_ceiling', u'with_concierge', u'with_walk_in_closet(s)', u'with_doorman', u'with_balcony', u'with_dogs_allowed', u'with_gym/fitness', u'with_storage', u'with_live_in_super', u'with_lounge', u'with_granite_kitchen', u'with_private_balcony', u'with_laundry', u'with_actual_apt._photos', u'with_residents_garden', u'with_pool', u'with_washer/dryer', u'with_light', u'with_swimming_pool', u'with_high_ceilings', u'with_patio', u'with_no_fee', u'with_bike_room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first TODO :filling outliers with nan\n",
    "processMap(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare for validation\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=StratifiedKFold(train_y,5,shuffle=True,random_state = 42)\n",
    "\n",
    "train_df = train_df.fillna(-1)\n",
    "#test_df = test_df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the basic features from preprocessing \n",
    "features = list(features_to_use)\n",
    "\n",
    "#features to be added during cv by cv-manner statistics\n",
    "features.extend(['manager_id_perf'])\n",
    "#categorical features to be added\n",
    "categorical = [\"display_address\", \"street_address\",\"street_name\",'building_id','manager_id']\n",
    "features.extend(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#second TODO\n",
    "features_to_use.extend(['cluster_id_10','cluster_id_30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:3770: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  raise_on_error=True)\n",
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\dell\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#running and getting the cv from xgboost\n",
    "cv_scores = []\n",
    "cv_result = []\n",
    "\n",
    "for dev_index, val_index in KF: \n",
    "    result_dict = {}\n",
    "    \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "    \n",
    "    #=============================================================        \n",
    "    \"\"\"feature engineerings for the categorical features\"\"\"\n",
    "    #fill substitute the small size values by their mean\n",
    "    for f in ['display_address','manager_id','building_id','street_name']:\n",
    "        dev_set,val_set  = singleValueConvert(dev_set,val_set,f,1)\n",
    "    \n",
    "    \n",
    "    #K-FOLD evaluation for the statistic features\n",
    "    \n",
    "    skf=StratifiedKFold(dev_set['interest_level'],5,shuffle=True,random_state = 42)\n",
    "    #dev set adding manager skill\n",
    "    for feature in ['manager_id']:\n",
    "        for train,test in skf:\n",
    "            performance_eval(dev_set.iloc[train,:],dev_set.iloc[test,:],feature=feature,k=5,g=10,\n",
    "                           update_df = dev_set)\n",
    "        \n",
    "        performance_eval(dev_set,val_set,feature=feature,k=5,g=10)\n",
    "    \n",
    "    \n",
    "    #second TODO\n",
    "    #getCluster(dev_set,val_set,30)\n",
    "    #getCluster(dev_set,val_set,10)\n",
    "    \n",
    "    for f in categorical:\n",
    "    \n",
    "        if dev_set[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(dev_set[f])+list(val_set[f]))\n",
    "            dev_set[f] = lbl.transform(list(dev_set[f].values))\n",
    "            val_set[f] = lbl.transform(list(val_set[f].values))\n",
    "    \n",
    "    #============================================================\n",
    "            \n",
    "    #filter the features\n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "    \"\"\"\n",
    "    runXGB(dev_X, train_y, val_X, test_y=None, feature_names=None, \\\n",
    "    seed_val=0, early_stop = 20,num_rounds=10000, eta = 0.1, max_depth = 6)\n",
    "    \"\"\"        \n",
    "    \n",
    "    preds,model = runXGB(dev_X, dev_y, val_X, val_y,feature_names=features,\\\n",
    "           early_stop = 64,eta = 0.1,max_depth=4,cv_dict = result_dict,verbose_eval=False)\n",
    "\n",
    "    loss = log_loss(val_y, preds)\n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    print 'loss for the turn '+str(i)+' is '+str(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test0    777\n",
       "test1    600\n",
       "test2    655\n",
       "test3    594\n",
       "test4    732\n",
       "Name: 776, dtype: int32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot the validation curv\n",
    "cvResult = CVstatistics(cv_result,'mlogloss')\n",
    "cvResult.turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvResult.validCurve(start =500,stop_at_first = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53173314728001397"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5320038, 0.42075579999999996)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvResult.errorsAt(550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('listing_id', 0.07671085177903215),\n",
       " ('price', 0.0743324665440477),\n",
       " ('manager_id_perf', 0.07262002917485888),\n",
       " ('latitude', 0.07090759180567006),\n",
       " ('price_per_room', 0.06567514428870426),\n",
       " ('longitude', 0.06275765840045665),\n",
       " ('street_address', 0.0606329675905372),\n",
       " ('manager_id', 0.055939620726834526),\n",
       " ('num_description_words', 0.05549565548297076),\n",
       " ('building_id', 0.05067546140673559),\n",
       " ('price_per_bed', 0.04445994799264286),\n",
       " ('street_name', 0.042430392592122786),\n",
       " ('display_address', 0.04068624341980085),\n",
       " ('created_day', 0.02980909494513858),\n",
       " ('num_photos', 0.026701338238092218),\n",
       " ('created_hour', 0.025940254962897192),\n",
       " ('num_features', 0.023022769074649584),\n",
       " ('with_no_fee', 0.008720745861609692),\n",
       " ('bedrooms', 0.008498763239677808),\n",
       " ('bathrooms', 0.008467051436544682),\n",
       " ('bath_per_bed', 0.007071732098687131),\n",
       " ('with_furnished', 0.004312805226105157),\n",
       " ('with_laundry_in_unit', 0.003773704572842012),\n",
       " ('with_hardwood_floors', 0.003678569163442633),\n",
       " ('with_laundry_in_building', 0.003615145557176381),\n",
       " ('with_pre_war', 0.0030126212976469844),\n",
       " ('created_month', 0.002949197691380732),\n",
       " ('with_cats_allowed', 0.0026637914631825964),\n",
       " ('with_dishwasher', 0.0026320796600494702),\n",
       " ('with_exclusive', 0.002536944250650092),\n",
       " ('with_dogs_allowed', 0.002505232447516966),\n",
       " ('with_dining_room', 0.002346673431851335),\n",
       " ('with_fitness_center', 0.0022198262193188303),\n",
       " ('with_outdoor_space', 0.0021564026130525783),\n",
       " ('with_swimming_pool', 0.002124690809919452),\n",
       " ('with_elevator', 0.002092979006786326),\n",
       " ('with_doorman', 0.0020295554005200734),\n",
       " ('with_loft', 0.0020295554005200734),\n",
       " ('with_terrace', 0.001997843597386947),\n",
       " ('with_balcony', 0.0019344199911206952),\n",
       " ('with_actual_apt._photos', 0.0018709963848544428),\n",
       " ('with_reduced_fee', 0.0018392845817213166),\n",
       " ('with_common_outdoor_space', 0.0018392845817213166),\n",
       " ('with_high_speed_internet', 0.0017124373691888121),\n",
       " ('with_garden/patio', 0.001680725566055686),\n",
       " ('with_roof_deck', 0.0016490137629225597),\n",
       " ('with_private_outdoor_space', 0.0016490137629225597),\n",
       " ('with_parking_space', 0.0015538783535231813),\n",
       " ('with_new_construction', 0.0013318957315912982),\n",
       " ('with_hardwood', 0.0013318957315912982),\n",
       " ('with_wheelchair_access', 0.0013001839284581722),\n",
       " ('with_storage', 0.0013001839284581722),\n",
       " ('with_multi_level', 0.0012050485190587938),\n",
       " ('with_simplex', 0.0011733367159256676),\n",
       " ('with_garage', 0.0009196422908606583),\n",
       " ('with_fireplace', 0.0008562186845944061),\n",
       " ('with_high_ceilings', 0.0007927950783281538),\n",
       " ('with_high_ceiling', 0.0007293714720619014),\n",
       " ('with_outdoor_areas', 0.0006976596689287753),\n",
       " ('with_dryer_in_unit', 0.0006976596689287753),\n",
       " ('with_live_in_super', 0.0006659478657956491),\n",
       " ('with_short_term_allowed', 0.000634236062662523),\n",
       " ('with_walk_in_closet(s)', 0.0005391006532631446),\n",
       " ('with_pets_on_approval', 0.0005073888501300184),\n",
       " ('with_prewar', 0.00047567704699689225),\n",
       " ('with_renovated', 0.00047567704699689225),\n",
       " ('with_green_building', 0.00047567704699689225),\n",
       " ('with_washer/dryer', 0.00041225344073063993),\n",
       " ('with_publicoutdoor', 0.00038054163759751377),\n",
       " ('with_newly_renovated', 0.00038054163759751377),\n",
       " ('with_granite_kitchen', 0.00038054163759751377),\n",
       " ('with_light', 0.00034882983446438766),\n",
       " ('with_stainless_steel_appliances', 0.00034882983446438766),\n",
       " ('with_concierge', 0.0003171180313312615),\n",
       " ('with_live_in_superintendent', 0.0003171180313312615),\n",
       " ('with_duplex', 0.0003171180313312615),\n",
       " ('with_laundry', 0.00028540622819813534),\n",
       " ('with_washer_in_unit', 0.00028540622819813534),\n",
       " ('with_on_site_laundry', 0.0002536944250650092),\n",
       " ('with_view', 0.0002536944250650092),\n",
       " ('with_on_site_garage', 0.0002536944250650092),\n",
       " ('with_parking', 0.0002536944250650092),\n",
       " ('with_residents_lounge', 0.0002536944250650092),\n",
       " ('with_pool', 0.0002536944250650092),\n",
       " ('with_lowrise', 0.00022198262193188305),\n",
       " ('with_luxury_building', 0.00022198262193188305),\n",
       " ('with_no_pets', 0.00019027081879875688),\n",
       " ('with_lounge', 0.00019027081879875688),\n",
       " ('with_marble_bath', 0.00015855901566563075),\n",
       " ('with_subway', 0.00015855901566563075),\n",
       " ('with_central_a/c', 0.00015855901566563075),\n",
       " ('with_private_balcony', 0.00015855901566563075),\n",
       " ('with_garden', 0.0001268472125325046),\n",
       " ('with_valet', 0.0001268472125325046),\n",
       " ('with_bike_room', 0.0001268472125325046),\n",
       " ('with_patio', 0.0001268472125325046),\n",
       " ('with_post_war', 0.0001268472125325046),\n",
       " ('with_common_parking/garage', 9.513540939937844e-05),\n",
       " ('with_eat_in_kitchen', 9.513540939937844e-05),\n",
       " ('with_exposed_brick', 6.34236062662523e-05),\n",
       " ('with_roofdeck', 6.34236062662523e-05),\n",
       " ('with_common_roof_deck', 6.34236062662523e-05),\n",
       " ('with_gym', 3.171180313312615e-05),\n",
       " ('with_childrens_playroom', 3.171180313312615e-05),\n",
       " ('with_gym/fitness', 3.171180313312615e-05),\n",
       " ('with_laundry_room', 3.171180313312615e-05),\n",
       " ('with_highrise', 3.171180313312615e-05)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the importance of the features\n",
    "xgbImportance(model,'weight')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
