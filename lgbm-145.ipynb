{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import lightgbm as lgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mochi import CVstatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try xgboost    \n",
    "\n",
    "def runLGBM(train_X, train_y, test_X, test_y=None, feature_names=None,\n",
    "           seed_val=0, num_rounds=10000,watch_dict = None,max_bin=50000,\n",
    "           max_depth=4,early_stop=64,verbose=True,eta=0.1):\n",
    "    param = {'learning_rate':eta, \n",
    "             'max_depth':max_depth,\n",
    "             'application':'multiclass',\n",
    "             'num_class':3,\n",
    "             'metric':'multi_logloss',\n",
    "              'num_threads':4}\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    #plst = list(param.items())\n",
    "    lgbtrain = lgb.Dataset(train_X, label=train_y,max_bin=max_bin,feature_name=feature_names)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgbtest = lgb.Dataset(test_X, label=test_y,max_bin=max_bin,feature_name=feature_names)\n",
    "        watchlist = [lgbtrain,lgbtest]\n",
    "        watchlist_name=['train','test']\n",
    "        model = lgb.train(param, lgbtrain, num_rounds, watchlist,watchlist_name, early_stopping_rounds=early_stop,\\\n",
    "                         evals_result = watch_dict,verbose_eval=verbose)\n",
    "    else:\n",
    "        #lgbtest = lgb.Dataset(test_X,feature_name=feature_names)\n",
    "        model = lgb.train(param, lgbtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 297)\n",
      "(74659, 296)\n"
     ]
    }
   ],
   "source": [
    "#lodaing data\n",
    "data_path = \"/home/raku/kaggleData/2sigma/xgb145/\"\n",
    "store = \"/home/raku/kaggleData/2sigma/lgb145/\"\n",
    "train_file = data_path + \"xgb1.45-train.json\"\n",
    "test_file = data_path + \"xgb1.45-test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "feature_file = data_path+'xgb145features.pickle'\n",
    "fileObject = open(feature_file,'r') \n",
    "features = pickle.load(fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "KF=StratifiedKFold(train_y,5,shuffle=True,random_state = 2333)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's multi_logloss: 0.636527\ttest's multi_logloss: 0.649748\n",
      "[200]\ttrain's multi_logloss: 0.567904\ttest's multi_logloss: 0.588765\n",
      "[300]\ttrain's multi_logloss: 0.539923\ttest's multi_logloss: 0.567937\n",
      "[400]\ttrain's multi_logloss: 0.521996\ttest's multi_logloss: 0.555618\n",
      "[500]\ttrain's multi_logloss: 0.508262\ttest's multi_logloss: 0.54795\n",
      "[600]\ttrain's multi_logloss: 0.497234\ttest's multi_logloss: 0.542569\n",
      "[700]\ttrain's multi_logloss: 0.488024\ttest's multi_logloss: 0.538455\n",
      "[800]\ttrain's multi_logloss: 0.480249\ttest's multi_logloss: 0.535246\n",
      "[900]\ttrain's multi_logloss: 0.473323\ttest's multi_logloss: 0.532662\n",
      "[1000]\ttrain's multi_logloss: 0.46691\ttest's multi_logloss: 0.530731\n",
      "[1100]\ttrain's multi_logloss: 0.461034\ttest's multi_logloss: 0.529325\n",
      "[1200]\ttrain's multi_logloss: 0.455492\ttest's multi_logloss: 0.528104\n",
      "[1300]\ttrain's multi_logloss: 0.450171\ttest's multi_logloss: 0.527149\n",
      "[1400]\ttrain's multi_logloss: 0.444904\ttest's multi_logloss: 0.526491\n",
      "[1500]\ttrain's multi_logloss: 0.439969\ttest's multi_logloss: 0.525786\n",
      "[1600]\ttrain's multi_logloss: 0.434817\ttest's multi_logloss: 0.525251\n",
      "[1700]\ttrain's multi_logloss: 0.429898\ttest's multi_logloss: 0.524886\n",
      "[1800]\ttrain's multi_logloss: 0.42512\ttest's multi_logloss: 0.52449\n",
      "[1900]\ttrain's multi_logloss: 0.420642\ttest's multi_logloss: 0.524254\n",
      "[2000]\ttrain's multi_logloss: 0.416213\ttest's multi_logloss: 0.523922\n",
      "[2100]\ttrain's multi_logloss: 0.411749\ttest's multi_logloss: 0.523621\n",
      "[2200]\ttrain's multi_logloss: 0.407444\ttest's multi_logloss: 0.523333\n",
      "[2300]\ttrain's multi_logloss: 0.403333\ttest's multi_logloss: 0.523105\n",
      "[2400]\ttrain's multi_logloss: 0.399276\ttest's multi_logloss: 0.522922\n",
      "[2500]\ttrain's multi_logloss: 0.395296\ttest's multi_logloss: 0.522684\n",
      "[2600]\ttrain's multi_logloss: 0.391375\ttest's multi_logloss: 0.52265\n",
      "[2700]\ttrain's multi_logloss: 0.387607\ttest's multi_logloss: 0.52272\n",
      "[2800]\ttrain's multi_logloss: 0.383886\ttest's multi_logloss: 0.522809\n",
      "[2900]\ttrain's multi_logloss: 0.380189\ttest's multi_logloss: 0.523\n",
      "[3000]\ttrain's multi_logloss: 0.37657\ttest's multi_logloss: 0.523096\n",
      "[3100]\ttrain's multi_logloss: 0.373197\ttest's multi_logloss: 0.523287\n",
      "[3200]\ttrain's multi_logloss: 0.369879\ttest's multi_logloss: 0.523382\n",
      "[3300]\ttrain's multi_logloss: 0.366599\ttest's multi_logloss: 0.52345\n",
      "[3400]\ttrain's multi_logloss: 0.363319\ttest's multi_logloss: 0.523501\n",
      "[3500]\ttrain's multi_logloss: 0.360004\ttest's multi_logloss: 0.523669\n",
      "loss for the turn 1 is 0.522437750746\n",
      "[100]\ttrain's multi_logloss: 0.637623\ttest's multi_logloss: 0.643966\n",
      "[200]\ttrain's multi_logloss: 0.56973\ttest's multi_logloss: 0.581827\n",
      "[300]\ttrain's multi_logloss: 0.542812\ttest's multi_logloss: 0.560484\n",
      "[400]\ttrain's multi_logloss: 0.525067\ttest's multi_logloss: 0.547985\n",
      "[500]\ttrain's multi_logloss: 0.511901\ttest's multi_logloss: 0.539731\n",
      "[600]\ttrain's multi_logloss: 0.50126\ttest's multi_logloss: 0.533862\n",
      "[700]\ttrain's multi_logloss: 0.491999\ttest's multi_logloss: 0.529325\n",
      "[800]\ttrain's multi_logloss: 0.484219\ttest's multi_logloss: 0.525976\n",
      "[900]\ttrain's multi_logloss: 0.477146\ttest's multi_logloss: 0.523163\n",
      "[1000]\ttrain's multi_logloss: 0.470508\ttest's multi_logloss: 0.520703\n",
      "[1100]\ttrain's multi_logloss: 0.464461\ttest's multi_logloss: 0.518835\n",
      "[1200]\ttrain's multi_logloss: 0.45847\ttest's multi_logloss: 0.517005\n",
      "[1300]\ttrain's multi_logloss: 0.452778\ttest's multi_logloss: 0.515686\n",
      "[1400]\ttrain's multi_logloss: 0.447286\ttest's multi_logloss: 0.514595\n",
      "[1500]\ttrain's multi_logloss: 0.442135\ttest's multi_logloss: 0.513539\n",
      "[1600]\ttrain's multi_logloss: 0.437029\ttest's multi_logloss: 0.512735\n",
      "[1700]\ttrain's multi_logloss: 0.432095\ttest's multi_logloss: 0.511958\n",
      "[1800]\ttrain's multi_logloss: 0.427287\ttest's multi_logloss: 0.511349\n",
      "[1900]\ttrain's multi_logloss: 0.422657\ttest's multi_logloss: 0.510816\n",
      "[2000]\ttrain's multi_logloss: 0.418201\ttest's multi_logloss: 0.510348\n",
      "[2100]\ttrain's multi_logloss: 0.413698\ttest's multi_logloss: 0.510016\n",
      "[2200]\ttrain's multi_logloss: 0.409595\ttest's multi_logloss: 0.509849\n",
      "[2300]\ttrain's multi_logloss: 0.405604\ttest's multi_logloss: 0.509652\n",
      "[2400]\ttrain's multi_logloss: 0.401656\ttest's multi_logloss: 0.509537\n",
      "[2500]\ttrain's multi_logloss: 0.397737\ttest's multi_logloss: 0.509458\n",
      "[2600]\ttrain's multi_logloss: 0.393914\ttest's multi_logloss: 0.509353\n",
      "[2700]\ttrain's multi_logloss: 0.390143\ttest's multi_logloss: 0.509318\n",
      "[2800]\ttrain's multi_logloss: 0.386466\ttest's multi_logloss: 0.509399\n",
      "[2900]\ttrain's multi_logloss: 0.382968\ttest's multi_logloss: 0.509412\n",
      "[3000]\ttrain's multi_logloss: 0.379428\ttest's multi_logloss: 0.509502\n",
      "[3100]\ttrain's multi_logloss: 0.375845\ttest's multi_logloss: 0.509571\n",
      "[3200]\ttrain's multi_logloss: 0.37242\ttest's multi_logloss: 0.509618\n",
      "[3300]\ttrain's multi_logloss: 0.369108\ttest's multi_logloss: 0.509591\n",
      "[3400]\ttrain's multi_logloss: 0.365762\ttest's multi_logloss: 0.509758\n",
      "[3500]\ttrain's multi_logloss: 0.36247\ttest's multi_logloss: 0.509838\n",
      "loss for the turn 2 is 0.509931814769\n",
      "[100]\ttrain's multi_logloss: 0.637428\ttest's multi_logloss: 0.645783\n",
      "[200]\ttrain's multi_logloss: 0.56922\ttest's multi_logloss: 0.585062\n",
      "[300]\ttrain's multi_logloss: 0.541426\ttest's multi_logloss: 0.564204\n",
      "[400]\ttrain's multi_logloss: 0.523317\ttest's multi_logloss: 0.552162\n",
      "[500]\ttrain's multi_logloss: 0.509679\ttest's multi_logloss: 0.54441\n",
      "[600]\ttrain's multi_logloss: 0.498971\ttest's multi_logloss: 0.53928\n",
      "[700]\ttrain's multi_logloss: 0.489673\ttest's multi_logloss: 0.535364\n",
      "[800]\ttrain's multi_logloss: 0.481638\ttest's multi_logloss: 0.532249\n",
      "[900]\ttrain's multi_logloss: 0.474476\ttest's multi_logloss: 0.529791\n",
      "[1000]\ttrain's multi_logloss: 0.467904\ttest's multi_logloss: 0.527924\n",
      "[1100]\ttrain's multi_logloss: 0.461791\ttest's multi_logloss: 0.526289\n",
      "[1200]\ttrain's multi_logloss: 0.456238\ttest's multi_logloss: 0.525139\n",
      "[1300]\ttrain's multi_logloss: 0.450633\ttest's multi_logloss: 0.52406\n",
      "[1400]\ttrain's multi_logloss: 0.445301\ttest's multi_logloss: 0.523236\n",
      "[1500]\ttrain's multi_logloss: 0.440176\ttest's multi_logloss: 0.522501\n",
      "[1600]\ttrain's multi_logloss: 0.435244\ttest's multi_logloss: 0.521883\n",
      "[1700]\ttrain's multi_logloss: 0.430452\ttest's multi_logloss: 0.521332\n",
      "[1800]\ttrain's multi_logloss: 0.425801\ttest's multi_logloss: 0.520924\n",
      "[1900]\ttrain's multi_logloss: 0.421324\ttest's multi_logloss: 0.520445\n",
      "[2000]\ttrain's multi_logloss: 0.417056\ttest's multi_logloss: 0.520033\n",
      "[2100]\ttrain's multi_logloss: 0.412785\ttest's multi_logloss: 0.519771\n",
      "[2200]\ttrain's multi_logloss: 0.408786\ttest's multi_logloss: 0.51962\n",
      "[2300]\ttrain's multi_logloss: 0.404576\ttest's multi_logloss: 0.519366\n",
      "[2400]\ttrain's multi_logloss: 0.400608\ttest's multi_logloss: 0.519137\n",
      "[2500]\ttrain's multi_logloss: 0.396751\ttest's multi_logloss: 0.518968\n",
      "[2600]\ttrain's multi_logloss: 0.392892\ttest's multi_logloss: 0.518823\n",
      "[2700]\ttrain's multi_logloss: 0.389285\ttest's multi_logloss: 0.518733\n",
      "[2800]\ttrain's multi_logloss: 0.385612\ttest's multi_logloss: 0.518647\n",
      "[2900]\ttrain's multi_logloss: 0.382021\ttest's multi_logloss: 0.51869\n",
      "[3000]\ttrain's multi_logloss: 0.378402\ttest's multi_logloss: 0.518594\n",
      "[3100]\ttrain's multi_logloss: 0.374902\ttest's multi_logloss: 0.518635\n",
      "[3200]\ttrain's multi_logloss: 0.371352\ttest's multi_logloss: 0.51869\n",
      "[3300]\ttrain's multi_logloss: 0.368062\ttest's multi_logloss: 0.518821\n",
      "[3400]\ttrain's multi_logloss: 0.364824\ttest's multi_logloss: 0.518911\n",
      "[3500]\ttrain's multi_logloss: 0.361629\ttest's multi_logloss: 0.51907\n",
      "loss for the turn 3 is 0.518701170309\n",
      "[100]\ttrain's multi_logloss: 0.637749\ttest's multi_logloss: 0.644183\n",
      "[200]\ttrain's multi_logloss: 0.569182\ttest's multi_logloss: 0.58315\n",
      "[300]\ttrain's multi_logloss: 0.542104\ttest's multi_logloss: 0.562207\n",
      "[400]\ttrain's multi_logloss: 0.524144\ttest's multi_logloss: 0.550566\n",
      "[500]\ttrain's multi_logloss: 0.510305\ttest's multi_logloss: 0.543115\n",
      "[600]\ttrain's multi_logloss: 0.49939\ttest's multi_logloss: 0.537927\n",
      "[700]\ttrain's multi_logloss: 0.490205\ttest's multi_logloss: 0.534317\n",
      "[800]\ttrain's multi_logloss: 0.482197\ttest's multi_logloss: 0.531472\n",
      "[900]\ttrain's multi_logloss: 0.474905\ttest's multi_logloss: 0.529136\n",
      "[1000]\ttrain's multi_logloss: 0.468364\ttest's multi_logloss: 0.527404\n",
      "[1100]\ttrain's multi_logloss: 0.46241\ttest's multi_logloss: 0.526009\n",
      "[1200]\ttrain's multi_logloss: 0.456745\ttest's multi_logloss: 0.524878\n",
      "[1300]\ttrain's multi_logloss: 0.451171\ttest's multi_logloss: 0.524066\n",
      "[1400]\ttrain's multi_logloss: 0.445935\ttest's multi_logloss: 0.523171\n",
      "[1500]\ttrain's multi_logloss: 0.440864\ttest's multi_logloss: 0.52237\n",
      "[1600]\ttrain's multi_logloss: 0.436119\ttest's multi_logloss: 0.521748\n",
      "[1700]\ttrain's multi_logloss: 0.431306\ttest's multi_logloss: 0.521235\n",
      "[1800]\ttrain's multi_logloss: 0.426701\ttest's multi_logloss: 0.520769\n",
      "[1900]\ttrain's multi_logloss: 0.422078\ttest's multi_logloss: 0.520408\n",
      "[2000]\ttrain's multi_logloss: 0.417613\ttest's multi_logloss: 0.520214\n",
      "[2100]\ttrain's multi_logloss: 0.413369\ttest's multi_logloss: 0.520181\n",
      "[2200]\ttrain's multi_logloss: 0.409212\ttest's multi_logloss: 0.520058\n",
      "[2300]\ttrain's multi_logloss: 0.405211\ttest's multi_logloss: 0.519876\n",
      "[2400]\ttrain's multi_logloss: 0.401108\ttest's multi_logloss: 0.51976\n",
      "[2500]\ttrain's multi_logloss: 0.397162\ttest's multi_logloss: 0.519727\n",
      "[2600]\ttrain's multi_logloss: 0.393126\ttest's multi_logloss: 0.51968\n",
      "[2700]\ttrain's multi_logloss: 0.389409\ttest's multi_logloss: 0.519791\n",
      "[2800]\ttrain's multi_logloss: 0.385645\ttest's multi_logloss: 0.519718\n",
      "[2900]\ttrain's multi_logloss: 0.381996\ttest's multi_logloss: 0.519842\n",
      "[3000]\ttrain's multi_logloss: 0.378318\ttest's multi_logloss: 0.519959\n",
      "[3100]\ttrain's multi_logloss: 0.374732\ttest's multi_logloss: 0.520139\n",
      "[3200]\ttrain's multi_logloss: 0.371329\ttest's multi_logloss: 0.520235\n",
      "[3300]\ttrain's multi_logloss: 0.367839\ttest's multi_logloss: 0.520503\n",
      "[3400]\ttrain's multi_logloss: 0.364502\ttest's multi_logloss: 0.520716\n",
      "[3500]\ttrain's multi_logloss: 0.361253\ttest's multi_logloss: 0.520819\n",
      "loss for the turn 4 is 0.520735922403\n",
      "[100]\ttrain's multi_logloss: 0.638248\ttest's multi_logloss: 0.644509\n",
      "[200]\ttrain's multi_logloss: 0.570493\ttest's multi_logloss: 0.580541\n",
      "[300]\ttrain's multi_logloss: 0.543219\ttest's multi_logloss: 0.558164\n",
      "[400]\ttrain's multi_logloss: 0.525501\ttest's multi_logloss: 0.545725\n",
      "[500]\ttrain's multi_logloss: 0.512281\ttest's multi_logloss: 0.537374\n",
      "[600]\ttrain's multi_logloss: 0.501611\ttest's multi_logloss: 0.531717\n",
      "[700]\ttrain's multi_logloss: 0.492427\ttest's multi_logloss: 0.527614\n",
      "[800]\ttrain's multi_logloss: 0.484516\ttest's multi_logloss: 0.524259\n",
      "[900]\ttrain's multi_logloss: 0.477695\ttest's multi_logloss: 0.521901\n",
      "[1000]\ttrain's multi_logloss: 0.471375\ttest's multi_logloss: 0.519884\n",
      "[1100]\ttrain's multi_logloss: 0.465478\ttest's multi_logloss: 0.518294\n",
      "[1200]\ttrain's multi_logloss: 0.459723\ttest's multi_logloss: 0.516972\n",
      "[1300]\ttrain's multi_logloss: 0.454124\ttest's multi_logloss: 0.515661\n",
      "[1400]\ttrain's multi_logloss: 0.44883\ttest's multi_logloss: 0.514641\n",
      "[1500]\ttrain's multi_logloss: 0.443516\ttest's multi_logloss: 0.513823\n",
      "[1600]\ttrain's multi_logloss: 0.438609\ttest's multi_logloss: 0.512999\n",
      "[1700]\ttrain's multi_logloss: 0.433689\ttest's multi_logloss: 0.512279\n",
      "[1800]\ttrain's multi_logloss: 0.429028\ttest's multi_logloss: 0.511745\n",
      "[1900]\ttrain's multi_logloss: 0.424401\ttest's multi_logloss: 0.511255\n",
      "[2000]\ttrain's multi_logloss: 0.41979\ttest's multi_logloss: 0.510869\n",
      "[2100]\ttrain's multi_logloss: 0.415281\ttest's multi_logloss: 0.510659\n",
      "[2200]\ttrain's multi_logloss: 0.410931\ttest's multi_logloss: 0.510454\n",
      "[2300]\ttrain's multi_logloss: 0.406616\ttest's multi_logloss: 0.510266\n",
      "[2400]\ttrain's multi_logloss: 0.402602\ttest's multi_logloss: 0.510255\n",
      "[2500]\ttrain's multi_logloss: 0.398612\ttest's multi_logloss: 0.510112\n",
      "[2600]\ttrain's multi_logloss: 0.394477\ttest's multi_logloss: 0.510233\n",
      "[2700]\ttrain's multi_logloss: 0.390701\ttest's multi_logloss: 0.510256\n",
      "[2800]\ttrain's multi_logloss: 0.387002\ttest's multi_logloss: 0.51027\n",
      "[2900]\ttrain's multi_logloss: 0.383336\ttest's multi_logloss: 0.51031\n",
      "[3000]\ttrain's multi_logloss: 0.379654\ttest's multi_logloss: 0.510428\n",
      "[3100]\ttrain's multi_logloss: 0.375937\ttest's multi_logloss: 0.510436\n",
      "[3200]\ttrain's multi_logloss: 0.372476\ttest's multi_logloss: 0.510525\n",
      "[3300]\ttrain's multi_logloss: 0.368963\ttest's multi_logloss: 0.510614\n",
      "[3400]\ttrain's multi_logloss: 0.365641\ttest's multi_logloss: 0.510658\n",
      "[3500]\ttrain's multi_logloss: 0.362233\ttest's multi_logloss: 0.510787\n",
      "loss for the turn 5 is 0.51071120025\n",
      "The mean of the cv_scores is:\n",
      "0.516503571695\n"
     ]
    }
   ],
   "source": [
    "cv_scores=[]\n",
    "cv_result=[]\n",
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    result_dict = {}\n",
    "    \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "       #filter the features\n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "    \n",
    "    preds, model = runLGBM(dev_X, dev_y, val_X, val_y,feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=3500,watch_dict=result_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    #save the pickles for futures use\n",
    "    pickl_file = store+'rf2000-5fold-out-'+str(i)+'.pickle'\n",
    "    fileObject = open(pickl_file,'wb') \n",
    "    pickle.dump(preds,fileObject)   \n",
    "    fileObject.close()\n",
    "    \"\"\"\n",
    "    loss = log_loss(val_y, preds)\n",
    "    \n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    i+=1\n",
    "    print'loss for the turn '+str(i)+' is '+str(loss)\n",
    "\n",
    "print 'The mean of the cv_scores is:'\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2576    0.516129\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvResult = CVstatistics(cv_result,'multi_logloss')\n",
    "meanTestError = cvResult.result.filter(like='test').mean(axis=1)\n",
    "meanTestError[meanTestError==np.min(meanTestError)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's multi_logloss: 0.636527\ttest's multi_logloss: 0.649748\n",
      "[200]\ttrain's multi_logloss: 0.567904\ttest's multi_logloss: 0.588765\n",
      "[300]\ttrain's multi_logloss: 0.539923\ttest's multi_logloss: 0.567937\n",
      "[400]\ttrain's multi_logloss: 0.521996\ttest's multi_logloss: 0.555618\n",
      "[500]\ttrain's multi_logloss: 0.508262\ttest's multi_logloss: 0.54795\n",
      "[600]\ttrain's multi_logloss: 0.497234\ttest's multi_logloss: 0.542569\n",
      "[700]\ttrain's multi_logloss: 0.488024\ttest's multi_logloss: 0.538455\n",
      "[800]\ttrain's multi_logloss: 0.480249\ttest's multi_logloss: 0.535246\n",
      "[900]\ttrain's multi_logloss: 0.473323\ttest's multi_logloss: 0.532662\n",
      "[1000]\ttrain's multi_logloss: 0.46691\ttest's multi_logloss: 0.530731\n",
      "[1100]\ttrain's multi_logloss: 0.461034\ttest's multi_logloss: 0.529325\n",
      "[1200]\ttrain's multi_logloss: 0.455492\ttest's multi_logloss: 0.528104\n",
      "[1300]\ttrain's multi_logloss: 0.450171\ttest's multi_logloss: 0.527149\n",
      "[1400]\ttrain's multi_logloss: 0.444904\ttest's multi_logloss: 0.526491\n",
      "[1500]\ttrain's multi_logloss: 0.439969\ttest's multi_logloss: 0.525786\n",
      "[1600]\ttrain's multi_logloss: 0.434817\ttest's multi_logloss: 0.525251\n",
      "[1700]\ttrain's multi_logloss: 0.429898\ttest's multi_logloss: 0.524886\n",
      "[1800]\ttrain's multi_logloss: 0.42512\ttest's multi_logloss: 0.52449\n",
      "[1900]\ttrain's multi_logloss: 0.420642\ttest's multi_logloss: 0.524254\n",
      "[2000]\ttrain's multi_logloss: 0.416213\ttest's multi_logloss: 0.523922\n",
      "[2100]\ttrain's multi_logloss: 0.411749\ttest's multi_logloss: 0.523621\n",
      "[2200]\ttrain's multi_logloss: 0.407444\ttest's multi_logloss: 0.523333\n",
      "[2300]\ttrain's multi_logloss: 0.403333\ttest's multi_logloss: 0.523105\n",
      "[2400]\ttrain's multi_logloss: 0.399276\ttest's multi_logloss: 0.522922\n",
      "[2500]\ttrain's multi_logloss: 0.395296\ttest's multi_logloss: 0.522684\n",
      "loss for the turn 1 is 0.521754054967\n",
      "[100]\ttrain's multi_logloss: 0.637623\ttest's multi_logloss: 0.643966\n",
      "[200]\ttrain's multi_logloss: 0.56973\ttest's multi_logloss: 0.581827\n",
      "[300]\ttrain's multi_logloss: 0.542812\ttest's multi_logloss: 0.560484\n",
      "[400]\ttrain's multi_logloss: 0.525067\ttest's multi_logloss: 0.547985\n",
      "[500]\ttrain's multi_logloss: 0.511901\ttest's multi_logloss: 0.539731\n",
      "[600]\ttrain's multi_logloss: 0.50126\ttest's multi_logloss: 0.533862\n",
      "[700]\ttrain's multi_logloss: 0.491999\ttest's multi_logloss: 0.529325\n",
      "[800]\ttrain's multi_logloss: 0.484219\ttest's multi_logloss: 0.525976\n",
      "[900]\ttrain's multi_logloss: 0.477146\ttest's multi_logloss: 0.523163\n",
      "[1000]\ttrain's multi_logloss: 0.470508\ttest's multi_logloss: 0.520703\n",
      "[1100]\ttrain's multi_logloss: 0.464461\ttest's multi_logloss: 0.518835\n",
      "[1200]\ttrain's multi_logloss: 0.45847\ttest's multi_logloss: 0.517005\n",
      "[1300]\ttrain's multi_logloss: 0.452778\ttest's multi_logloss: 0.515686\n",
      "[1400]\ttrain's multi_logloss: 0.447286\ttest's multi_logloss: 0.514595\n",
      "[1500]\ttrain's multi_logloss: 0.442135\ttest's multi_logloss: 0.513539\n",
      "[1600]\ttrain's multi_logloss: 0.437029\ttest's multi_logloss: 0.512735\n",
      "[1700]\ttrain's multi_logloss: 0.432095\ttest's multi_logloss: 0.511958\n",
      "[1800]\ttrain's multi_logloss: 0.427287\ttest's multi_logloss: 0.511349\n",
      "[1900]\ttrain's multi_logloss: 0.422657\ttest's multi_logloss: 0.510816\n",
      "[2000]\ttrain's multi_logloss: 0.418201\ttest's multi_logloss: 0.510348\n",
      "[2100]\ttrain's multi_logloss: 0.413698\ttest's multi_logloss: 0.510016\n",
      "[2200]\ttrain's multi_logloss: 0.409595\ttest's multi_logloss: 0.509849\n",
      "[2300]\ttrain's multi_logloss: 0.405604\ttest's multi_logloss: 0.509652\n",
      "[2400]\ttrain's multi_logloss: 0.401656\ttest's multi_logloss: 0.509537\n",
      "[2500]\ttrain's multi_logloss: 0.397737\ttest's multi_logloss: 0.509458\n",
      "loss for the turn 2 is 0.509522461249\n",
      "[100]\ttrain's multi_logloss: 0.637428\ttest's multi_logloss: 0.645783\n",
      "[200]\ttrain's multi_logloss: 0.56922\ttest's multi_logloss: 0.585062\n",
      "[300]\ttrain's multi_logloss: 0.541426\ttest's multi_logloss: 0.564204\n",
      "[400]\ttrain's multi_logloss: 0.523317\ttest's multi_logloss: 0.552162\n",
      "[500]\ttrain's multi_logloss: 0.509679\ttest's multi_logloss: 0.54441\n",
      "[600]\ttrain's multi_logloss: 0.498971\ttest's multi_logloss: 0.53928\n",
      "[700]\ttrain's multi_logloss: 0.489673\ttest's multi_logloss: 0.535364\n",
      "[800]\ttrain's multi_logloss: 0.481638\ttest's multi_logloss: 0.532249\n",
      "[900]\ttrain's multi_logloss: 0.474476\ttest's multi_logloss: 0.529791\n",
      "[1000]\ttrain's multi_logloss: 0.467904\ttest's multi_logloss: 0.527924\n",
      "[1100]\ttrain's multi_logloss: 0.461791\ttest's multi_logloss: 0.526289\n",
      "[1200]\ttrain's multi_logloss: 0.456238\ttest's multi_logloss: 0.525139\n",
      "[1300]\ttrain's multi_logloss: 0.450633\ttest's multi_logloss: 0.52406\n",
      "[1400]\ttrain's multi_logloss: 0.445301\ttest's multi_logloss: 0.523236\n",
      "[1500]\ttrain's multi_logloss: 0.440176\ttest's multi_logloss: 0.522501\n",
      "[1600]\ttrain's multi_logloss: 0.435244\ttest's multi_logloss: 0.521883\n",
      "[1700]\ttrain's multi_logloss: 0.430452\ttest's multi_logloss: 0.521332\n",
      "[1800]\ttrain's multi_logloss: 0.425801\ttest's multi_logloss: 0.520924\n",
      "[1900]\ttrain's multi_logloss: 0.421324\ttest's multi_logloss: 0.520445\n",
      "[2000]\ttrain's multi_logloss: 0.417056\ttest's multi_logloss: 0.520033\n",
      "[2100]\ttrain's multi_logloss: 0.412785\ttest's multi_logloss: 0.519771\n",
      "[2200]\ttrain's multi_logloss: 0.408786\ttest's multi_logloss: 0.51962\n",
      "[2300]\ttrain's multi_logloss: 0.404576\ttest's multi_logloss: 0.519366\n",
      "[2400]\ttrain's multi_logloss: 0.400608\ttest's multi_logloss: 0.519137\n",
      "[2500]\ttrain's multi_logloss: 0.396751\ttest's multi_logloss: 0.518968\n",
      "loss for the turn 3 is 0.51856971348\n",
      "[100]\ttrain's multi_logloss: 0.637749\ttest's multi_logloss: 0.644183\n",
      "[200]\ttrain's multi_logloss: 0.569182\ttest's multi_logloss: 0.58315\n",
      "[300]\ttrain's multi_logloss: 0.542104\ttest's multi_logloss: 0.562207\n",
      "[400]\ttrain's multi_logloss: 0.524144\ttest's multi_logloss: 0.550566\n",
      "[500]\ttrain's multi_logloss: 0.510305\ttest's multi_logloss: 0.543115\n",
      "[600]\ttrain's multi_logloss: 0.49939\ttest's multi_logloss: 0.537927\n",
      "[700]\ttrain's multi_logloss: 0.490205\ttest's multi_logloss: 0.534317\n",
      "[800]\ttrain's multi_logloss: 0.482197\ttest's multi_logloss: 0.531472\n",
      "[900]\ttrain's multi_logloss: 0.474905\ttest's multi_logloss: 0.529136\n",
      "[1000]\ttrain's multi_logloss: 0.468364\ttest's multi_logloss: 0.527404\n",
      "[1100]\ttrain's multi_logloss: 0.46241\ttest's multi_logloss: 0.526009\n",
      "[1200]\ttrain's multi_logloss: 0.456745\ttest's multi_logloss: 0.524878\n",
      "[1300]\ttrain's multi_logloss: 0.451171\ttest's multi_logloss: 0.524066\n",
      "[1400]\ttrain's multi_logloss: 0.445935\ttest's multi_logloss: 0.523171\n",
      "[1500]\ttrain's multi_logloss: 0.440864\ttest's multi_logloss: 0.52237\n",
      "[1600]\ttrain's multi_logloss: 0.436119\ttest's multi_logloss: 0.521748\n",
      "[1700]\ttrain's multi_logloss: 0.431306\ttest's multi_logloss: 0.521235\n",
      "[1800]\ttrain's multi_logloss: 0.426701\ttest's multi_logloss: 0.520769\n",
      "[1900]\ttrain's multi_logloss: 0.422078\ttest's multi_logloss: 0.520408\n",
      "[2000]\ttrain's multi_logloss: 0.417613\ttest's multi_logloss: 0.520214\n",
      "[2100]\ttrain's multi_logloss: 0.413369\ttest's multi_logloss: 0.520181\n",
      "[2200]\ttrain's multi_logloss: 0.409212\ttest's multi_logloss: 0.520058\n",
      "[2300]\ttrain's multi_logloss: 0.405211\ttest's multi_logloss: 0.519876\n",
      "[2400]\ttrain's multi_logloss: 0.401108\ttest's multi_logloss: 0.51976\n",
      "[2500]\ttrain's multi_logloss: 0.397162\ttest's multi_logloss: 0.519727\n",
      "loss for the turn 4 is 0.519522603767\n",
      "[100]\ttrain's multi_logloss: 0.638248\ttest's multi_logloss: 0.644509\n",
      "[200]\ttrain's multi_logloss: 0.570493\ttest's multi_logloss: 0.580541\n",
      "[300]\ttrain's multi_logloss: 0.543219\ttest's multi_logloss: 0.558164\n",
      "[400]\ttrain's multi_logloss: 0.525501\ttest's multi_logloss: 0.545725\n",
      "[500]\ttrain's multi_logloss: 0.512281\ttest's multi_logloss: 0.537374\n",
      "[600]\ttrain's multi_logloss: 0.501611\ttest's multi_logloss: 0.531717\n",
      "[700]\ttrain's multi_logloss: 0.492427\ttest's multi_logloss: 0.527614\n",
      "[800]\ttrain's multi_logloss: 0.484516\ttest's multi_logloss: 0.524259\n",
      "[900]\ttrain's multi_logloss: 0.477695\ttest's multi_logloss: 0.521901\n",
      "[1000]\ttrain's multi_logloss: 0.471375\ttest's multi_logloss: 0.519884\n",
      "[1100]\ttrain's multi_logloss: 0.465478\ttest's multi_logloss: 0.518294\n",
      "[1200]\ttrain's multi_logloss: 0.459723\ttest's multi_logloss: 0.516972\n",
      "[1300]\ttrain's multi_logloss: 0.454124\ttest's multi_logloss: 0.515661\n",
      "[1400]\ttrain's multi_logloss: 0.44883\ttest's multi_logloss: 0.514641\n",
      "[1500]\ttrain's multi_logloss: 0.443516\ttest's multi_logloss: 0.513823\n",
      "[1600]\ttrain's multi_logloss: 0.438609\ttest's multi_logloss: 0.512999\n",
      "[1700]\ttrain's multi_logloss: 0.433689\ttest's multi_logloss: 0.512279\n",
      "[1800]\ttrain's multi_logloss: 0.429028\ttest's multi_logloss: 0.511745\n",
      "[1900]\ttrain's multi_logloss: 0.424401\ttest's multi_logloss: 0.511255\n",
      "[2000]\ttrain's multi_logloss: 0.41979\ttest's multi_logloss: 0.510869\n",
      "[2100]\ttrain's multi_logloss: 0.415281\ttest's multi_logloss: 0.510659\n",
      "[2200]\ttrain's multi_logloss: 0.410931\ttest's multi_logloss: 0.510454\n",
      "[2300]\ttrain's multi_logloss: 0.406616\ttest's multi_logloss: 0.510266\n",
      "[2400]\ttrain's multi_logloss: 0.402602\ttest's multi_logloss: 0.510255\n",
      "[2500]\ttrain's multi_logloss: 0.398612\ttest's multi_logloss: 0.510112\n",
      "loss for the turn 5 is 0.510107816706\n",
      "The mean of the cv_scores is:\n",
      "0.515895330034\n"
     ]
    }
   ],
   "source": [
    "cv_scores=[]\n",
    "cv_result=[]\n",
    "i=0\n",
    "for dev_index, val_index in KF: \n",
    "    result_dict = {}\n",
    "    \n",
    "    dev_set, val_set = train_df.iloc[dev_index,:] , train_df.iloc[val_index,:] \n",
    "       #filter the features\n",
    "    dev_X, val_X = dev_set[features].as_matrix(), val_set[features].as_matrix()\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "    \n",
    "    preds, model = runLGBM(dev_X, dev_y, val_X, val_y,feature_names=features,verbose=100,eta=0.02,\n",
    "                          early_stop=None,num_rounds=2500,watch_dict=result_dict,max_depth=4)\n",
    "    \n",
    "\n",
    "    #save the pickles for futures use\n",
    "    pickl_file = store+'lgbm145-5fold-out-'+str(i)+'.pickle'\n",
    "    fileObject = open(pickl_file,'wb') \n",
    "    pickle.dump(preds,fileObject)   \n",
    "    fileObject.close()\n",
    "    \n",
    "    loss = log_loss(val_y, preds)\n",
    "    \n",
    "    cv_scores.append(loss)\n",
    "    cv_result.append(result_dict)\n",
    "    i+=1\n",
    "    print'loss for the turn '+str(i)+' is '+str(loss)\n",
    "\n",
    "print 'The mean of the cv_scores is:'\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, test_X = train_df[features].as_matrix(), test_df[features].as_matrix()\n",
    "\n",
    "preds, model = runLGBM(train_X, train_y, test_X,\\\n",
    "feature_names=features,\n",
    "num_rounds = 2500, eta = 0.02,max_depth = 4,verbose=100)\n",
    "\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df.to_json(store+'lgbm145-bulk-out.json')\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "#out_df.to_csv(\"xgb_beta1point42-0.02.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lgbm using 145\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
